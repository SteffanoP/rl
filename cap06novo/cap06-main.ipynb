{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeJ3wCaKe2Wl"
      },
      "source": [
        "# Capítulo 6 - SARSA de _n_ passos e Ambientes Contínuos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgUyA2Di5J_n"
      },
      "source": [
        "Você pode rodar este notebook localmente ou no Colab. Para abrir diretamente no Colab, basta clicar no link abaixo.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap06novo/cap06-main.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAHITU7VhsM7"
      },
      "source": [
        "## Configurações Iniciais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS23BU8R1vq-"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # for saving videos\n",
        "    !apt-get install ffmpeg\n",
        "\n",
        "    !pip install gymnasium\n",
        "    !pip install optuna\n",
        "\n",
        "    # clone repository\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "\n",
        "    clear_output()\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unDmWuiH5J_q"
      },
      "outputs": [],
      "source": [
        "'''if IN_COLAB:\n",
        "    # Set up fake display; otherwise rendering will fail\n",
        "    import os\n",
        "    os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "    os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gzf7VhkiHxQ"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import optuna\n",
        "\n",
        "from util.experiments import repeated_exec\n",
        "from util.plot import plot_result, plot_multiple_results\n",
        "from util.wrappers import DiscreteObservationWrapper\n",
        "from util.notebook import display_videos_from_path\n",
        "from util.qtable_helper import evaluate_qtable\n",
        "\n",
        "from cap04.montecarlo_v2 import run_montecarlo2\n",
        "from cap05.qlearning_sarsa import run_qlearning\n",
        "from cap05.expected_sarsa import run_expected_sarsa\n",
        "\n",
        "from nstep_sarsa import run_nstep_sarsa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-xEwtye5J_r"
      },
      "source": [
        "## 1 - SARSA de n Passos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oypatF0t5J_t"
      },
      "source": [
        "É uma extensão do SARSA que usa uma sequência de de *n* passos (ou seja, com *n* ações realizadas no ambiente) como entrada para cada atualização da estimativa do $Q(s,a)$.\n",
        "\n",
        "Com **n=1**, ele estima $Q(s,a)$ igual ao *SARSA*:\n",
        "- usa esta experiência: $s, a, r_1, s_1, a_1$\n",
        "- calcula a nova estimativa de $Q(s,a)$ assim (para estados não-terminais):\n",
        "$$Q_{target} = r_1 + \\gamma . Q(s_1,a_1)$$\n",
        "- note que apenas 1 ação foi realizada ($a$) e apenas 1 recompensa foi recebida ($r_1$)\n",
        "- observe que a última ação ($a_1$) foi escolhida, mas não foi realizada no ambiente\n",
        "\n",
        "Com **n=2**, para estimar $Q(s,a)$, ele:\n",
        "- usa esta experiência: $s, a, r_1, s_1, a_1, r_2, s_2, a_2$\n",
        "- calcula a nova estimativa de $Q(s,a)$ assim:\n",
        "$$Q_{target} = r_1 + \\gamma .r_2 + \\gamma^2 . Q(s_2,a_2)$$\n",
        "- esta é uma estimativa mais precisa, que usa os dados de 2 recompensas reais\n",
        "\n",
        "Para **n qualquer**:\n",
        "- experiência: $s, a, r_1, s_1, a_1, r_2, s_2, a_2, s_3, \\cdots, r_n, s_n, a_n$\n",
        "- nova estimativa:\n",
        "$$Q_{target} = r_1 + \\gamma .r_2 + \\gamma^2 . r_2 + \\cdots + \\gamma^n . Q(s_n,a_n)$$\n",
        "\n",
        "O **n** será um parâmetro do algoritmo.\n",
        "\n",
        "O código é mais complexo do que dos algoritmos de 1 passo. Veja o arquivo `cap05/run_nstep_sarsa.py` para conhecer os detalhes.\n",
        "\n",
        "Abaixo, vamos importar e usar aquela implementação para fazer um experimento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x5E-zx85J_u"
      },
      "outputs": [],
      "source": [
        "#ENV_NAME, r_max = \"Taxi-v3\", 10\n",
        "ENV_NAME, r_max = \"CliffWalking-v0\", 0\n",
        "\n",
        "EPISODES = 7000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.1\n",
        "NSTEPS = 3\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "# Roda o algoritmo \"n-step SARSA\"\n",
        "rewards, qtable = run_nstep_sarsa(env, EPISODES, NSTEPS, LR, GAMMA, EPSILON, render=False)\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jfj6LvPI5J_u"
      },
      "outputs": [],
      "source": [
        "# Mostra um gráfico de episódios x retornos não descontados\n",
        "plot_result(rewards, r_max, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK4dvRUR5J_v"
      },
      "outputs": [],
      "source": [
        "evaluate_qtable(env, qtable, 1, verbose=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkpV4ENC5J_v"
      },
      "outputs": [],
      "source": [
        "RUNS = 5\n",
        "results = []\n",
        "for nstep in [1, 2, 3]:\n",
        "    results.append( repeated_exec(RUNS, f\"{nstep}-step SARSA (LR={LR})\", run_nstep_sarsa, env, EPISODES, nstep, LR) )\n",
        "    clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1Ehr5Eh5J_v"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results, cumulative=False, x_log_scale=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztvJdbKVh20Y"
      },
      "source": [
        "## 2 - Lidando com Ambientes Contínuos\n",
        "\n",
        "Vamos usar os mesmos algoritmos de antes, baseados em Q-Table, para lidar com ambientes de estados contínuos. \n",
        "\n",
        "Para isso, vamos usar um wrapper que discretiza os estados desses ambientes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRoYcm6a5J_w"
      },
      "source": [
        "Abaixo, nós criamos um ambiente e mostramos o \"formato\" do seu estado e um exemplo de estado do ambiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OACm0r-iuh2"
      },
      "outputs": [],
      "source": [
        "# 1. Criar o ambiente contínuo\n",
        "ENV_NAME = \"CartPole-v1\"\n",
        "r_max_plot = 200\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "# vamos ver como é um estado deste ambiente?\n",
        "print(\"Espaço de estados/observações: \", env.observation_space)\n",
        "print(\"  - formato: \", env.observation_space.shape)\n",
        "print(\"  - exemplo: \", env.reset())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vplFS4jKuA4m"
      },
      "outputs": [],
      "source": [
        "# 2. Encapsular o ambiente em nosso wrapper\n",
        "\n",
        "# atenção para o 2o parâmetro: deve ter um valor para cada componente do estado\n",
        "env = DiscreteObservationWrapper(env, [20,50,10,20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le0wNU9PrA8f"
      },
      "outputs": [],
      "source": [
        "# 3. Rodar um algoritmo de treinamento\n",
        "\n",
        "EPISODES = 1000\n",
        "LR = 0.5\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.1\n",
        "NSTEPS = 2\n",
        "\n",
        "#rewards, qtable = run_expected_sarsa(env, EPISODES, LR, GAMMA, EPSILON, render=False)\n",
        "rewards, qtable = run_nstep_sarsa(env, EPISODES, NSTEPS, LR, GAMMA, EPSILON, render=False)\n",
        "\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXlD7B-pshru"
      },
      "outputs": [],
      "source": [
        "# 4. Gerar um gráfico de episódios x retornos (não descontados)\n",
        "\n",
        "filename = f\"results/expected_sarsa-{ENV_NAME.lower()[0:8]}-ep{EPISODES}-lr{LR}.png\"\n",
        "plot_result(rewards, r_max_plot, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bz2Pp1Wsi_P"
      },
      "outputs": [],
      "source": [
        "# 5. Faz alguns testes, usando a tabela de forma greedy\n",
        "\n",
        "test_greedy_Q_policy(env, qtable, 10, render=False, recorded_video_folder='./videos')\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_LkBPgb5iz2"
      },
      "outputs": [],
      "source": [
        "display_videos_from_path('./videos')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8roKzCgsuCl"
      },
      "source": [
        "## 3 - Otimizando Parâmetros\n",
        "\n",
        "Vamos usar a biblioteca *Optuna* para otimizar (hiper-)parâmetros dos algoritmos de treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBsJRMCstj0N"
      },
      "source": [
        "### 3.1 - Ambiente Discreto\n",
        "\n",
        "Este é o caso mais simples, porque não precisamos aplicar nenhum wrapper.\n",
        "\n",
        "Primeiro, você precisa fazer uma função que receber um parâmetro \"Trial\" (objeto do optuna) e retorna uma medida de desempenho.\n",
        "\n",
        "Dentro da função, você usa o trial para pedir \"sugestões\" de valores para os hiper-parâmetros do seu algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU8DNpphvcRa"
      },
      "outputs": [],
      "source": [
        "ENV = gym.make(\"Taxi-v3\")\n",
        "\n",
        "def train(trial : optuna.Trial):\n",
        "    # chama os métodos do \"trial\" (tentativa) para sugerir valores para os parâmetros\n",
        "    lr = trial.suggest_uniform('learning_rate', 0.001, 1.0)\n",
        "    #lr = trial.suggest_loguniform('learning_rate', 0.001, 1.0)\n",
        "    eps = trial.suggest_uniform('epsilon', 0.01, 0.2)\n",
        "\n",
        "    print(f\"\\nTRIAL #{trial.number}: lr={lr}, eps={eps}\")\n",
        "\n",
        "    # roda o algoritmo e recebe os retornos não-descontados\n",
        "    (returns, _) = run_montecarlo2(ENV, 4000, lr=lr, epsilon=eps)\n",
        "\n",
        "    # limpa a saída da célula do notebook\n",
        "    clear_output()\n",
        "\n",
        "    # média dos retornos dos últimos 100 episódios\n",
        "    return sum(returns[-1000:])/1000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUpVCri3v7v9"
      },
      "source": [
        "Depois, você cria um \"study\" do Optuna e manda otimizar sua função, indicando quantas tentativas (trials) ele vai fazer -- ou seja, quantas vezes a sua função vai ser executada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTVBjiirtOUP"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction='maximize',\n",
        "                        storage='sqlite:///resultado_optuna.db',\n",
        "                        study_name='montecarlo2',\n",
        "                        load_if_exists=True)\n",
        "\n",
        "# maximiza o valor de retorno de train_exp_sarsa, rodando \"n_trials\" vezes\n",
        "study.optimize(train, n_trials=10)\n",
        "\n",
        "print(\"MELHORES PARÂMETROS:\")\n",
        "print(study.best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N6eLcX7yfqd"
      },
      "source": [
        "### 3.2 - Ambiente Contínuo\n",
        "\n",
        "Neste cao, nós aplicamos o wrapper, que tem novos parâmetros (quantidade de \"bins\" por valor do estado).\n",
        "\n",
        "O wrapper é considerado parte da solução e, por isso, o ideal é tentar otimizar esses parâmetros. Por isso, o wrapper será criado dentro da função a ser otimizada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7Jbppx7zr8u"
      },
      "outputs": [],
      "source": [
        "# cria apenas a versão contínua\n",
        "ENV = gym.make(\"CartPole-v1\")\n",
        "\n",
        "def train_continuous(trial : optuna.Trial):\n",
        "\n",
        "    # chama os métodos do \"trial\" (tentativa) para sugerir valores para os parâmetros\n",
        "    lr = trial.suggest_uniform('learning_rate', 0.001, 1.0)\n",
        "    eps = trial.suggest_uniform('epsilon', 0.01, 0.2)\n",
        "    bins1 = trial.suggest_int('bins1', 5, 100)\n",
        "    bins2 = trial.suggest_int('bins2', 5, 100)\n",
        "    bins3 = trial.suggest_int('bins3', 5, 100)\n",
        "    bins4 = trial.suggest_int('bins4', 5, 100)\n",
        "\n",
        "    all_bins = [bins1, bins2, bins3, bins4]\n",
        "\n",
        "    print(f\"\\nTRIAL #{trial.number}: lr={lr}, eps={eps}, bins={all_bins}\")\n",
        "\n",
        "    # cria o wrapper e roda o algoritmo, recebendo os retornos não-descontados\n",
        "    env_wrapper = DiscreteObservationWrapper(ENV, all_bins)\n",
        "    (returns, _) = run_expected_sarsa(env_wrapper, 4000, lr=lr, epsilon=eps)\n",
        "\n",
        "    # limpa a saída da célula do notebook\n",
        "    clear_output()\n",
        "\n",
        "    # média dos retornos dos últimos 1000 episódios\n",
        "    return sum(returns[-1000:])/1000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_6xX6d_0lht"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction='maximize',\n",
        "                        storage='sqlite:///resultado_optuna.db',\n",
        "                        study_name='expsarsa_continuo',\n",
        "                        load_if_exists=True)\n",
        "\n",
        "# maximiza o valor de retorno de train_exp_sarsa_continuous, rodando \"n_trials\" vezes\n",
        "study.optimize(train_continuous, n_trials=10)\n",
        "\n",
        "print(\"MELHORES PARÂMETROS:\")\n",
        "print(study.best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF9fggfZzFVV"
      },
      "source": [
        "## 4 - Experimentos Completos\n",
        "\n",
        "Agora que você descobriu bons parâmetros, que tal rodar um treinamento mais longo com o seu algoritmo?\n",
        "\n",
        "Seguem experimentos com os resultados de otimizações obtidas com os alunos do semestre 2021.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdGNkV7T5J_z"
      },
      "source": [
        "### Taxi-v3 (discreto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYSG9xiHxBKe"
      },
      "outputs": [],
      "source": [
        "environment = gym.make(\"Taxi-v3\")\n",
        "NUM_EPISODES = 20000\n",
        "\n",
        "results = []\n",
        "\n",
        "# 'epsilon': 0.012630525410678125, 'learning_rate': 0.9189058377450972\n",
        "results.append( repeated_exec(3, f\"Q-Learning (Maely)\", run_qlearning, environment, NUM_EPISODES, 0.918, 0.95, 0.012) )\n",
        "clear_output()\n",
        "\n",
        "# 'epsilon': 0.01049172026352314, 'learning_rate': 0.07158117429097055\n",
        "results.append( repeated_exec(3, f\"Exp-Sarsa (Bruno)\", run_expected_sarsa, environment, NUM_EPISODES, 0.0715, 0.95, 0.010) )\n",
        "clear_output()\n",
        "\n",
        "# {'epsilon': 0.07291046034184917, 'learning_rate': 0.025046716210814366}\n",
        "results.append( repeated_exec(3, f\"MonteCarlo2 (M.Wei)\", run_montecarlo2, environment, NUM_EPISODES, 0.0250, 0.95, 0.072) )\n",
        "clear_output()\n",
        "\n",
        "plot_multiple_results(results, cumulative=False, x_log_scale=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0z3e7zc5J_z"
      },
      "source": [
        "### FrozenLake-v1 (discreto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDpW0-Nn5J_z"
      },
      "outputs": [],
      "source": [
        "environment = gym.make(\"FrozenLake-v1\")\n",
        "NUM_EPISODES = 20000\n",
        "\n",
        "results = []\n",
        "\n",
        "# {'epsilon': 0.03081133375694635, 'learning_rate': 0.5177593676824266}\n",
        "results.append( repeated_exec(3, f\"Q-Learning (J.Rodrigues)\", run_qlearning, environment, NUM_EPISODES, 0.517, 0.95, 0.030) )\n",
        "clear_output()\n",
        "\n",
        "# 'epsilon': 0.045332160883937794, 'learning_rate': 0.24962062732262033\n",
        "results.append( repeated_exec(3, f\"Exp-Sarsa (Matheus)\", run_expected_sarsa, environment, NUM_EPISODES, 0.249, 0.95, 0.045) )\n",
        "clear_output()\n",
        "\n",
        "# learning_rate': 0.8054242666287916, 'epsilon': 0.07261114965386482, 'gamma': 2.2849204996994357\n",
        "results.append( repeated_exec(3, f\"MonteCarlo2 (L.Fernando)\", run_montecarlo2, environment, NUM_EPISODES, 0.805, 2.28, 0.072) )\n",
        "clear_output()\n",
        "\n",
        "plot_multiple_results(results, cumulative=False, x_log_scale=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtj3R5gp5J_z"
      },
      "source": [
        "### MountainCar (contínuo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q3mTUqO5J_z"
      },
      "outputs": [],
      "source": [
        "ENV = gym.make(\"MountainCar-v0\")\n",
        "NUM_EPISODES = 20000\n",
        "\n",
        "results = []\n",
        "\n",
        "'''\n",
        "Mateus {'bins1': 56, 'bins2': 51, 'epsilon': 0.01186126148733685, 'learning_rate': 0.6912469478373227}\n",
        "Daniel {'bins1': 54, 'bins2': 70, 'epsilon': 0.13703891101977922, 'learning_rate': 0.6792059384074097}\n",
        "Diego {bins1': 65, 'bins2': 46, 'epsilon': 0.09108892703810256, 'learning_rate': 0.27627593841593145}\n",
        "'''\n",
        "\n",
        "wrapped_env = DiscreteObservationWrapper(ENV, [56,51])\n",
        "results.append( repeated_exec(3, f\"Q-Learning (Mateus)\", run_qlearning, wrapped_env, NUM_EPISODES, 0.691, 0.95, 0.011) )\n",
        "clear_output()\n",
        "\n",
        "wrapped_env = DiscreteObservationWrapper(ENV, [54,70])\n",
        "results.append( repeated_exec(3, f\"Exp-Sarsa (Daniel)\", run_expected_sarsa, wrapped_env, NUM_EPISODES, 0.679, 0.95, 0.137) )\n",
        "clear_output()\n",
        "\n",
        "wrapped_env = DiscreteObservationWrapper(ENV, [65,46])\n",
        "results.append( repeated_exec(3, f\"MonteCarlo2 (Diego)\", run_montecarlo2, wrapped_env, NUM_EPISODES, 0.276, 2.28, 0.091) )\n",
        "clear_output()\n",
        "\n",
        "plot_multiple_results(results, cumulative=False, x_log_scale=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCNKUKy15J_z"
      },
      "source": [
        "### CartPole (contínuo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtsW88Z75J_0"
      },
      "outputs": [],
      "source": [
        "ENV = gym.make(\"CartPole-v1\")\n",
        "NUM_EPISODES = 15000\n",
        "\n",
        "results = []\n",
        "\n",
        "#'bin1': 93,\n",
        "#'bin2': 43,\n",
        "#'bin3': 6,\n",
        "#'bin4': 76,\n",
        "#'epsilon': 0.015607854707332426,\n",
        "#'gamma': 0.9134000716662984,\n",
        "#'learning_rate': 0.11056680467861989}\n",
        "wrapped_env = DiscreteObservationWrapper(ENV, [93,43,6,76])\n",
        "results.append( repeated_exec(3, f\"Q-Learning (Lucas)\", run_qlearning, wrapped_env, NUM_EPISODES, 0.110, 0.913, 0.015) )\n",
        "clear_output()\n",
        "\n",
        "# Melhores ????\n",
        "#wrapped_env = DiscreteObservationWrapper(ENV, [54,70])\n",
        "#results.append( repeated_exec(3, f\"Exp-Sarsa (Savio)\", run_expected_sarsa, wrapped_env, NUM_EPISODES, 0.679, 0.95, 0.137) )\n",
        "clear_output()\n",
        "\n",
        "# Melhores parâmetros: {'bins1': 23, 'bins2': 37, 'bins3': 30, 'bins4': 38, 'epsilon': 0.05396213628972839, 'learning_rate': 0.5536537056140982}\n",
        "wrapped_env = DiscreteObservationWrapper(ENV, [93,43,6,76])\n",
        "results.append( repeated_exec(3, f\"MonteCarlo2 (Giulia)\", run_montecarlo2, wrapped_env, NUM_EPISODES, 0.553, 0.95, 0.053) )\n",
        "clear_output()\n",
        "\n",
        "plot_multiple_results(results, cumulative=False, x_log_scale=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgbhKstd8iNB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "cap05-main-2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('rlx')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "27dbc9ce4cc602e4f15257b7b0018d8dff5b9ce9a7d73bc4399cb5afb1e02c4a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
