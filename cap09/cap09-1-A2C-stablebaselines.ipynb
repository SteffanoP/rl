{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# A2C com Stable Baselines3\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap09/cap09-1-A2C-stablebaselines.ipynb)\n",
        "\n",
        "Vamos usar o algoritmo **A2C** (*Advantage Actor Critic*) neste Google Colab.\n",
        "\n",
        "Referências:\n",
        "- Notebook baseado neste tutorial: https://github.com/araffin/rl-tutorial-jnrr19\n",
        "- Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "- Documentação: https://stable-baselines3.readthedocs.io/en/master/index.html\n",
        "- Caso queira usar agentes pré-treinados, veja [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)\n",
        "\n",
        "Alguns textos do tutorial original foram mantidos.\n",
        "\n",
        "A seguir, instalamos as dependências necessárias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from IPython.display import clear_output\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "    !pip install \"stable-baselines3[extra]==2.0.0\"\n",
        "    #clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VwO0bAqmAPw0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "J� existe uma subpasta ou um arquivo log_dir.\n"
          ]
        }
      ],
      "source": [
        "!mkdir log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import tensorboard\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "import stable_baselines3\n",
        "stable_baselines3.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae32CtgzTG3R"
      },
      "source": [
        "Vamos importar especificamente o algoritmo **A2C**. Você pode importar outros algoritmos, instanciar e usar de forma idêntica ou parecida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wgAXxClR0BfH"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import A2C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0_8OQbOTTNT"
      },
      "source": [
        "Você também precisa importar a classe que vai representar a **rede neural da política**. É recomendado importar do submódulo específico do algoritmo desejado. No caso, vamos usar uma rede com camadas totalmente conectadas (ou seja, MLP - Multi-Layer Perceptron)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ROUJr675TT01"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.a2c import MlpPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## 1 - Criar o ambiente e instanciar o agente\n",
        "\n",
        "Estamos criando o ambiente e passando para a classe **A2C**, que permite testar ou treinar o agente.\n",
        "\n",
        "Sobre os parâmetros:\n",
        "- `n_steps`: indica quantos passos vai fazer por atualização\n",
        "- `ent_coef`: indica o peso do \"bônus de exploração\" (coeficiente de entropia)\n",
        "- `tensorboard_log`: diretório para guardar os logs\n",
        "- mais informações: https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pUWGZp3i9wyf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "model = A2C(MlpPolicy, env, n_steps=16, ent_coef=0.01, verbose=1, tensorboard_log=\"log_dir\", device=\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhrgy_iIolTb"
      },
      "source": [
        "Mais alguns detalhes sobre o uso de `MlpPolicy`:\n",
        "- o tamanho da entrada será deduzido do espaço de *estados do ambiente*\n",
        "- o número de nós de saída será deduzido do *espaço de ações*\n",
        "\n",
        "Você também pode passar a string \"MlpPolicy\", sem precisar importar a classe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efFdrQ7MBvl"
      },
      "source": [
        "## 2 - Função auxiliar para rodar e avaliar o agente\n",
        "\n",
        "Na verdade, Stable-Baselines3 já oferece função similar, que poderia ser importada assim:\n",
        "\n",
        "`from stable_baselines3.common.evaluation import evaluate_policy`\n",
        "\n",
        "Mas vamos usar uma versão própria:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "63M8mSKR-6Zt"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, n_eval_episodes=100, deterministic=True):\n",
        "    \"\"\"\n",
        "    Evaluate a RL agent\n",
        "    :param model: (BaseRLModel object) the RL Agent\n",
        "    :param num_episodes: (int) number of episodes to evaluate it\n",
        "    :return: (float) Mean reward for the last num_episodes\n",
        "    \"\"\"\n",
        "    # This function will only work for a single environment\n",
        "    env = model.get_env()\n",
        "    all_episode_rewards = []\n",
        "    for i in range(n_eval_episodes):\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "        while not done:\n",
        "            # _model_state is only useful when using LSTM policies\n",
        "            action, _model_state = model.predict(obs, deterministic=deterministic)\n",
        "            # here, action, rewards and dones are arrays\n",
        "            # because we are using vectorized env\n",
        "            # also note that the step only returns a 4-tuple, as the env that is returned\n",
        "            # by model.get_env() is an sb3 vecenv that wraps the >v0.26 API\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "        all_episode_rewards.append(sum(episode_rewards))\n",
        "\n",
        "    mean_episode_reward = np.mean(all_episode_rewards)\n",
        "    std_episode_rewaard = np.std(all_episode_rewards)\n",
        "    #print(\"Reward: \", mean_episode_reward, \"+/-\", std_episode_rewaard)\n",
        "\n",
        "    return mean_episode_reward, std_episode_rewaard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEVOIY8NVeK"
      },
      "source": [
        "Vamos avaliar o agente não-treinado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xDHLMA6NFk95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retorno médio: 9.29 +/- 0.82\n"
          ]
        }
      ],
      "source": [
        "# Random Agent, before training\n",
        "mean_reward, std_reward = evaluate(model, n_eval_episodes=100)\n",
        "\n",
        "print(f\"Retorno médio: {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## 3 - Treina o agente e avalia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "estawiYlqeFz"
      },
      "source": [
        "Treinar é bem simples. Note que você precisa informar a quantidade de **passos** (e não de *episódios*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "e4cfSXIB-pTF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 166      |\n",
            "|    ep_rew_mean        | 166      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1433     |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 64       |\n",
            "|    total_timesteps    | 92800    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.618   |\n",
            "|    explained_variance | -0.211   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | 0.0014   |\n",
            "|    value_loss         | 9.2e-06  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 173       |\n",
            "|    ep_rew_mean        | 173       |\n",
            "| time/                 |           |\n",
            "|    fps                | 1432      |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 65        |\n",
            "|    total_timesteps    | 94400     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.528    |\n",
            "|    explained_variance | -9.16e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | -26.5     |\n",
            "|    value_loss         | 3.5e+03   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 171      |\n",
            "|    ep_rew_mean        | 171      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1433     |\n",
            "|    iterations         | 6000     |\n",
            "|    time_elapsed       | 66       |\n",
            "|    total_timesteps    | 96000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.597   |\n",
            "|    explained_variance | -0.05    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | 0.00383  |\n",
            "|    value_loss         | 0.000127 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 169      |\n",
            "|    ep_rew_mean        | 169      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1433     |\n",
            "|    iterations         | 6100     |\n",
            "|    time_elapsed       | 68       |\n",
            "|    total_timesteps    | 97600    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.51    |\n",
            "|    explained_variance | 0.0188   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6099     |\n",
            "|    policy_loss        | 0.00368  |\n",
            "|    value_loss         | 4.82e-05 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 165      |\n",
            "|    ep_rew_mean        | 165      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1434     |\n",
            "|    iterations         | 6200     |\n",
            "|    time_elapsed       | 69       |\n",
            "|    total_timesteps    | 99200    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.571   |\n",
            "|    explained_variance | -0.0372  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6199     |\n",
            "|    policy_loss        | 0.000191 |\n",
            "|    value_loss         | 1.71e-07 |\n",
            "------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<stable_baselines3.a2c.a2c.A2C at 0x1997cbe7310>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the agent for 100 thousand steps\n",
        "model.learn(total_timesteps=100_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ygl_gVmV_QP7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retorno médio: 311.07 +/- 14.38\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate(model, n_eval_episodes=100)\n",
        "\n",
        "print(f\"Retorno médio: {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A00W6yY3NkHG"
      },
      "source": [
        "## 4 - Visualizar treinamento no Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2MkjYOgx3itp"
      },
      "outputs": [],
      "source": [
        "#%tensorboard --logdir log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LSG9SuXrjQR"
      },
      "source": [
        "## 5 - Visualizar Agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### 5.1 Definições necessárias\n",
        "\n",
        "Funções e configurações necessárias para salvar um vídeo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    # Set up fake display; otherwise rendering will fail\n",
        "    import os\n",
        "    os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRNUfulOGaF"
      },
      "source": [
        "We will record a video using the [VecVideoRecorder](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "  # Start the video at step=0 and record the given number of steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, done, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOObbeu5MMlR"
      },
      "source": [
        "### 5.2 Grava e exibe vídeo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iATu7AiyMQW2"
      },
      "outputs": [],
      "source": [
        "record_video('CartPole-v1', model, video_length=1000, prefix='a2c-cartpole')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n4i-fW3NojZ"
      },
      "outputs": [],
      "source": [
        "show_videos('videos', prefix='a2c-cartpole')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y8zg4V566qD"
      },
      "source": [
        "## Bônus: Treinar um modelo de RL em UMA linha\n",
        "\n",
        "Vamos treinar o **DQN**, para comparar com o *A2C*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suj-oxBfTgFU"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKkF1rzUVSCW"
      },
      "source": [
        "Usaremos apenas uma linha de código. A classe da política será inferida e o ambiente será criado automaticamente a partir das strings dadas. Veja mais na [documentação](https://stable-baselines.readthedocs.io/en/master/guide/quickstart.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaOPfOrwWEP4"
      },
      "outputs": [],
      "source": [
        "model2 = DQN('MlpPolicy', \"CartPole-v1\", buffer_size=20_000, learning_starts=2_000, target_update_interval=4_000, tensorboard_log=\"log_dir\", verbose=1).learn(100_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# faça o código para testar 'model2' aqui"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FtY8FhliLsGm",
        "xVm9QPNVwKXN"
      ],
      "name": "cap09-1-A2C-stablebaselines.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('rlx')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "27dbc9ce4cc602e4f15257b7b0018d8dff5b9ce9a7d73bc4399cb5afb1e02c4a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
