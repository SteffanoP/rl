{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeJ3wCaKe2Wl"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap07/cap07-main.ipynb)\n",
        "\n",
        "# Capítulo 7 - Algoritmos com Modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAHITU7VhsM7"
      },
      "source": [
        "## Configurações Iniciais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS23BU8R1vq-",
        "outputId": "062d71fc-7094-4293-b8fc-6de9fe682618"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # for saving videos\n",
        "    !apt-get install ffmpeg\n",
        "\n",
        "    !pip install gymnasium moviepy\n",
        "    !pip install optuna\n",
        "\n",
        "    # clone repository\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Gzf7VhkiHxQ"
      },
      "outputs": [],
      "source": [
        "import random as rand\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "import optuna\n",
        "\n",
        "from util.experiments import repeated_exec\n",
        "from util.plot import plot_result, plot_multiple_results\n",
        "from util.notebook import display_videos_from_path\n",
        "\n",
        "from util.qtable_helper import evaluate_qtable, record_video_qtable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nbyZSZ53wCvd"
      },
      "outputs": [],
      "source": [
        "# basta importar o módulo que o ambiente \"RaceTrack-v0\" é registrado no gym\n",
        "import envs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L15104fKzl7_"
      },
      "source": [
        "## 1 - Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSWJnQFw2fvZ"
      },
      "source": [
        "O *Q-Learning* é um algoritmo **livre de modelo**, que vamos comparar com o próximo algoritmo, chamado *Dyna-Q* que é um algoritmo **com modelo**.\n",
        "\n",
        "Recomendamos acessar  o **código** do Q-Learning no `cap05/qlearning_sarsa.py` para relembrar.\n",
        "\n",
        "Também vamos executar o Q-Learning, logo abaixo, para comparar com o desempenho do próximo algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jc5RKqrpzl8E"
      },
      "outputs": [],
      "source": [
        "from cap05.qlearning_sarsa import run_qlearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "x0dKiIRh2fva"
      },
      "outputs": [],
      "source": [
        "# escolha o ambiente descomentando uma das linhas abaixo\n",
        "#ENV_NAME = \"Taxi-v3\"\n",
        "#ENV_NAME = \"WindyGrid-v0\"\n",
        "ENV_NAME = \"RaceTrack-v0\"\n",
        "\n",
        "LR = 0.3\n",
        "GAMMA = 0.90\n",
        "EPSILON = 0.1\n",
        "\n",
        "#VERBOSE = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x5E-zx85J_u",
        "outputId": "7e235c3f-fcc1-4892-90be-df8f62f11ff7"
      },
      "outputs": [],
      "source": [
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "if ENV_NAME in [\"Taxi-v3\", \"WindyGrid-v0\"]:\n",
        "    rmax = 10.0\n",
        "    EPISODES = 700\n",
        "else:\n",
        "    rmax = 0.0\n",
        "    EPISODES = 3_000\n",
        "\n",
        "rewards1, qtable1 = run_qlearning(env, EPISODES, LR, GAMMA, EPSILON)\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards1[-20:]), \", desvio padrao =\", np.std(rewards1[-20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "Jfj6LvPI5J_u",
        "outputId": "2d6989b5-17c5-490f-8efc-a5dea1faafe1"
      },
      "outputs": [],
      "source": [
        "# Mostra um gráfico de passos x retornos não descontados acumulados\n",
        "plot_result(rewards1, rmax, cumulative=False, window=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ7DZk4J2fvi",
        "outputId": "f45e1d09-c4f9-4bbd-f4eb-c542bf4ab088"
      },
      "outputs": [],
      "source": [
        "evaluate_qtable(env, qtable1, 10, verbose=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onLKf4kBwCvf"
      },
      "outputs": [],
      "source": [
        "record_video_qtable(ENV_NAME, qtable1, episodes=3, folder='./videos-qlearn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AfZU97YkwCvf",
        "outputId": "31568682-79fd-430b-b26e-a05c2dbe405c"
      },
      "outputs": [],
      "source": [
        "display_videos_from_path('./videos-qlearn', speed=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEeLJzatzl8H"
      },
      "source": [
        "## 2 - Dyna-Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO52zy36zl8H"
      },
      "source": [
        "O *Dyna-Q* é um algoritmo **com modelo** que é uma extensão do *Q-Learning*. Compare os códigos para entender o Dyna-Q."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NFVuYF92zl8I"
      },
      "outputs": [],
      "source": [
        "def planning(model, planning_steps, Q, lr, gamma):\n",
        "    all_s_a = list(model.keys())\n",
        "    if len(all_s_a) < planning_steps:\n",
        "        samples = rand.choices(all_s_a, k=planning_steps)\n",
        "    else:\n",
        "        samples = rand.sample(all_s_a, k=planning_steps)\n",
        "\n",
        "    for s, a in samples:\n",
        "        r, next_s, is_terminal = model[(s,a)]\n",
        "        if is_terminal:\n",
        "            V_next_s = 0\n",
        "        else:\n",
        "            V_next_s = np.max(Q[next_s])\n",
        "        delta = (r + gamma * V_next_s) - Q[s,a]\n",
        "        Q[s,a] = Q[s,a] + lr * delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AGWxUiuPwCvg"
      },
      "outputs": [],
      "source": [
        "# Esta é a política. Neste caso, escolhe uma ação com base nos valores da tabela Q, usando uma estratégia epsilon-greedy,\n",
        "# dividindo a probabilidade igualmente em caso de empates entre ações de valor máximo.\n",
        "from util.qtable_helper import epsilon_greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pVmOdi4zzl8J"
      },
      "outputs": [],
      "source": [
        "# Algoritmo Dyna Q\n",
        "def run_dyna_q(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1, planning_steps=5, verbose=False):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # inicializa a tabela Q\n",
        "    Q = np.random.uniform(low=-0.01, high=+0.01, size=(env.observation_space.n, num_actions))\n",
        "\n",
        "    # inicializa o modelo do ambiente\n",
        "    model = dict()\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        # executa 1 episódio completo, fazendo atualizações na Q-table\n",
        "        while not done:\n",
        "\n",
        "            # escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if terminated:\n",
        "                # para estados terminais\n",
        "                V_next_state = 0\n",
        "            else:\n",
        "                # para estados não-terminais -- valor máximo (melhor ação)\n",
        "                V_next_state = np.max(Q[next_state])\n",
        "\n",
        "            # atualiza a Q-table / direct RL\n",
        "            delta = (reward + gamma * V_next_state) - Q[state,action]\n",
        "            Q[state,action] = Q[state,action] + lr * delta\n",
        "\n",
        "            # atualiza o modelo\n",
        "            model[state,action] = (reward, next_state, terminated)\n",
        "\n",
        "            # planejamento / indirect RL\n",
        "            planning(model, planning_steps, Q, lr, gamma)\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso\n",
        "        if verbose and ((i+1) % 100 == 0):\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    state = env.reset()\n",
        "    reward = 0\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCdswbdqzl8K",
        "outputId": "372db6a6-6c32-41f8-9e71-a03edbd78dd8"
      },
      "outputs": [],
      "source": [
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "if ENV_NAME == 'Taxi-v3':\n",
        "    rmax = 10.0\n",
        "    EPISODES = 700\n",
        "else:\n",
        "    rmax = 0.0\n",
        "    EPISODES = 3_000\n",
        "\n",
        "rewards2, qtable2 = run_dyna_q(env, EPISODES, LR, GAMMA, EPSILON, planning_steps=10, verbose=True)\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards2[-20:]), \", desvio padrao =\", np.std(rewards2[-20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "wxeaJNc2zl8K",
        "outputId": "4e7ade75-fb2d-4b4b-ffa3-87a5b44f3f8f"
      },
      "outputs": [],
      "source": [
        "# Mostra um gráfico de passos x retornos não descontados acumulados\n",
        "plot_result(rewards2, rmax, cumulative='no', window=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8m0ZzPlwCvg",
        "outputId": "6d21003a-fa63-432d-9c24-ea12b9d7031d"
      },
      "outputs": [],
      "source": [
        "evaluate_qtable(env, qtable2, 10, verbose=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t-0yUapwCvg"
      },
      "outputs": [],
      "source": [
        "record_video_qtable(ENV_NAME, qtable2, episodes=3, folder='./videos-dynaq')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OXbY3zsOwCvg",
        "outputId": "6f6d6909-83db-4a8d-e652-b1cdf7773e7d"
      },
      "outputs": [],
      "source": [
        "display_videos_from_path(\"./videos-dynaq\", speed=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8roKzCgsuCl"
      },
      "source": [
        "## 3 - Experimentos Q-Learning x Dyna-Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDa_GxSu2fvs"
      },
      "source": [
        "Nesta seção, você pode fazer experimentos no `Taxi-v3` ou no `RaceTrack-v0`, como preferir. (Porém, com o `RaceTrack` é mais demorado).\n",
        "\n",
        "Escolha o ambiente na célula de código abaixo.\n",
        "\n",
        "Os experimentos têm duas partes:\n",
        "- Na seção 3.1, vamos otimizar os parâmetros do Q-Learning\n",
        "- Na seção 3.2, vamos rodar o Q-Learning e o Dyna-Q (ambos com os parâmetros do Q-Learning), para comparar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SvFQZWsG2fvt"
      },
      "outputs": [],
      "source": [
        "# Escolha abaixo o ambiente\n",
        "#ENV_NAME_EXPERIMENT_1 = \"Taxi-v3\"\n",
        "#ENV_NAME_EXPERIMENT_1 = \"WindyGrid-v0\"\n",
        "ENV_NAME_EXPERIMENT_1 = \"RaceTrack-v0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDfuPn7azl8L"
      },
      "source": [
        "### 3.1 - Otimizando Parâmetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fWTr0m3wCvh"
      },
      "source": [
        "Veja o script `cap07/optimize_params.py`, que faz uma otimização do parâmetros do Q-Learning puro usando o `optuna`.\n",
        "\n",
        "A linha abaixo executa o script como um comando externo e, depois, carrega os resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# roda o script de otimização\n",
        "if IN_COLAB:\n",
        "    !python rl_facil/cap07/optimize_params.py --env {ENV_NAME_EXPERIMENT_1}\n",
        "else:\n",
        "    !python optimize_params.py --env {ENV_NAME_EXPERIMENT_1}\n",
        "\n",
        "#clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEfipt7pHD0B",
        "outputId": "da92034a-f28e-4b4b-a8c5-f2f26bad7249"
      },
      "outputs": [],
      "source": [
        "# carrega os resultados\n",
        "study = optuna.load_study(storage=f\"sqlite:///optuna_cap07.db\",\n",
        "                          study_name=f\"qlearning_{ENV_NAME_EXPERIMENT_1}\")\n",
        "clear_output()\n",
        "\n",
        "print(f\"MELHORES PARÂMETROS PARA {ENV_NAME_EXPERIMENT_1} (trial #{study.best_trial.number}):\")\n",
        "print(study.best_params)\n",
        "\n",
        "qlearn_params_1 = study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0pFOtarwCvh"
      },
      "source": [
        "Se não quiser rodar a otimização, você pode descomentar uma das linhas abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qjpPgbGewCvh"
      },
      "outputs": [],
      "source": [
        "# para o Racetrack\n",
        "#qlearn_params_1 = {'lr': 0.8167256204339587, 'epsilon': 0.06280337647714501, 'gamma': 0.9619087751023728}\n",
        "\n",
        "# para o Taxi\n",
        "#qlearn_params_1 = {'lr': 0.9616271568166056, 'epsilon': 0.013769122856570889, 'gamma': 0.8467211706577995}\n",
        "\n",
        "# para o WindyGrid-v0\n",
        "#qlearn_params_1 = {'lr': 0.7754141348857424, 'epsilon': 0.010419303097732811, 'gamma': 0.9124779956696675}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ-IKYWszl8M"
      },
      "source": [
        "### 3.2 - Experimentos Comparativos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYSG9xiHxBKe",
        "outputId": "22a82fef-695b-4481-e261-d18fb8346c76"
      },
      "outputs": [],
      "source": [
        "environment = gym.make(ENV_NAME_EXPERIMENT_1)\n",
        "\n",
        "if ENV_NAME_EXPERIMENT_1 in [\"Taxi-v3\", \"WindyGrid-v0\"]:\n",
        "    EPISODES = 1_000\n",
        "else:\n",
        "    EPISODES = 3_000\n",
        "\n",
        "RUNS = 5\n",
        "AUTO_LOAD = True\n",
        "\n",
        "results_1 = []\n",
        "\n",
        "results_1.append( repeated_exec(RUNS, f\"Q-Learning \", run_qlearning, environment, EPISODES, **qlearn_params_1, auto_load=AUTO_LOAD) )\n",
        "clear_output()\n",
        "\n",
        "#plan_steps = 1\n",
        "#results_t.append( repeated_exec(RUNS, f\"Dyna-Q ({plan_steps} passo)\", run_dyna_q, environment, EPISODES, **qlearn_params_taxi, planning_steps=plan_steps, auto_load=AUTO_LOAD) )\n",
        "#clear_output()\n",
        "\n",
        "plan_steps = 10\n",
        "results_1.append( repeated_exec(RUNS, f\"Dyna-Q ({plan_steps} passos)\", run_dyna_q, environment, EPISODES, **qlearn_params_1, planning_steps=plan_steps, auto_load=AUTO_LOAD) )\n",
        "clear_output()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLE-Ehfhzl8N"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results_1, cumulative='no', x_log_scale=False, window=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgbhKstd8iNB"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results_1, cumulative='avg', x_log_scale=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF9fggfZzFVV"
      },
      "source": [
        "## 4 - Experimentos com Ambientes Estocásticos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le9b63TX2fv6"
      },
      "source": [
        "Nesta seção, vamos fazer experimentos similares aos da seção 3, mas focando em ambientes estocásticos como o `FrozenLake`.\n",
        "\n",
        "Este ambiente tem como diferencial o fato de ser **não-determinístico**:\n",
        "- uma ação tem 1/3 de chance de dar o resultado correto\n",
        "- e 1/3 de fazer o agente mover em cada direção perpendicular à desejada.\n",
        "\n",
        "Isso afeta um pouco os resultados, como veremos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 - Otimizando Parâmetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6UaJBtMwCvl"
      },
      "source": [
        "Escolha o ambiente, na célula de código abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ENV_NAME_EXPERIMENT_2 = \"FrozenLake-v1\"\n",
        "ENV_NAME_EXPERIMENT_2 = \"WindyGrid-v2\" # stochastic version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Em seguida, vamos pode novamente usar o script `cap07/optimize_params` para otimizar os parâmetros do Q-Learning puro.\n",
        "\n",
        "Mas, dessa vez, vamos usar valores previamente calculados..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''# roda o script de otimização\n",
        "if IN_COLAB:\n",
        "    !python rl_facil/cap07/optimize_params.py --env {ENV_NAME_EXPERIMENT_2} --episodes_per_trial 300 --trials 120\n",
        "else:\n",
        "    pass #!python optimize_params.py --env {ENV_NAME_EXPERIMENT_2} --episodes_per_trial 300 --trials 120\n",
        "\n",
        "# carrega parâmetros ótimos obtidos com o Optuna\n",
        "study = optuna.load_study(storage=\"sqlite:///optuna_cap07.db\",\n",
        "                          study_name=f\"qlearning_{ENV_NAME_EXPERIMENT_2}\")\n",
        "#clear_output()\n",
        "\n",
        "print(f\"MELHORES PARÂMETROS PARA {ENV_NAME_EXPERIMENT_2} (trial #{study.best_trial.number}):\")\n",
        "print(study.best_params)\n",
        "\n",
        "qlearn_params_2 = study.best_params'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS1l-Rj4wCvl"
      },
      "source": [
        "A célula abaixo carrega bons parâmetros previamente obtidos com o `optuna`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "mVBJ_oPdwCvm"
      },
      "outputs": [],
      "source": [
        "if ENV_NAME_EXPERIMENT_2 == \"FrozenLake-v1\":\n",
        "    qlearn_params_2 = {'lr': 0.3692331683709833, 'epsilon': 0.10280622937354741, 'gamma': 0.9792780782708356}\n",
        "elif ENV_NAME_EXPERIMENT_2 == \"WindyGrid-v2\":\n",
        "    qlearn_params_2 = {'lr': 0.4859475184416657, 'epsilon': 0.0793371367164761, 'gamma': 0.8189890984802477}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbW2NzNTHD0b"
      },
      "source": [
        "### 4.2 - Resultados do Dyna-Q sem alteração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9g5jvhGzl8P"
      },
      "outputs": [],
      "source": [
        "environment = gym.make(ENV_NAME_EXPERIMENT_2)\n",
        "EPISODES = 2_000 if ENV_NAME_EXPERIMENT_2 == \"FrozenLake-v1\" else 1_000\n",
        "RUNS = 15\n",
        "AUTO_LOAD = False\n",
        "\n",
        "results_2 = []\n",
        "\n",
        "results_2.append( repeated_exec(RUNS, f\"Q-Learning \", run_qlearning, environment, EPISODES, **qlearn_params_2, auto_load=AUTO_LOAD) )\n",
        "clear_output()\n",
        "\n",
        "plan_steps = 5\n",
        "results_2.append( repeated_exec(RUNS, f\"Dyna-Q ({plan_steps} passos)\", run_dyna_q, environment, EPISODES, **qlearn_params_2, planning_steps=plan_steps, auto_load=AUTO_LOAD) )\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yzf8o9ea27Tp"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results_2, cumulative='no', x_log_scale=False, window=100, y_min=-300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "iBzYx8VhwCvm"
      },
      "outputs": [],
      "source": [
        "#plot_multiple_results(results_2, cumulative='avg', x_log_scale=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER0PKE9owCvm"
      },
      "source": [
        "Analisando os resultados, o Dyna-Q foi bem nesse ambiente estocástico (não-determinístico)?\n",
        "\n",
        "Veja que, apesar do desempenho variar entre execuções, ele não costuma ir bem com quantidades altas de \"passos de planejamento\" neste ambiente.\n",
        "\n",
        "**O que explica isso?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqpKymDJzl8Q"
      },
      "source": [
        "### 4.3 - Desafio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0LpaQbBzl8Q"
      },
      "source": [
        "Proponha uma modificação simples no **modelo** e no **planejamento** do *Dyna-Q* para melhorar o desempenho dele nos ambiente estocásticos.\n",
        "\n",
        "Depois, refaça os experimentos da seção 4.2 para conferir se deu certo. (O desempenho deve ser melhor do que o do *Dyna-Q* original, pelo menos.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rymv-G0Szl8Q"
      },
      "outputs": [],
      "source": [
        "def planning_new(model, planning_steps, Q, lr, gamma):\n",
        "    pass\n",
        "    # TODO: PROPONHA UMA ALTERAÇÃO AQUI\n",
        "\n",
        "raise NotImplementedError(\"Implemente a função planning_new acima antes de prosseguir\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4B7DBrQzl8W"
      },
      "outputs": [],
      "source": [
        "# Algoritmo Dyna Q\n",
        "def run_dyna_q_new(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1, planning_steps=5):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "   # inicializa a tabela Q\n",
        "    Q = np.random.uniform(low=-0.01, high=+0.01, size=(env.observation_space.n, num_actions))\n",
        "\n",
        "    # inicializa o modelo\n",
        "    model = dict()  # TODO - altere aqui, se precisar\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        # executa 1 episódio completo, fazendo atualizações na Q-table\n",
        "        while not done:\n",
        "\n",
        "            # escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if terminated:\n",
        "                # para estados terminais\n",
        "                V_next_state = 0\n",
        "            else:\n",
        "                # para estados não-terminais -- valor máximo (melhor ação)\n",
        "                V_next_state = np.max(Q[next_state])\n",
        "\n",
        "            # atualiza a Q-table / direct RL\n",
        "            delta = (reward + gamma * V_next_state) - Q[state,action]\n",
        "            Q[state,action] = Q[state,action] + lr * delta\n",
        "\n",
        "            # atualiza o modelo\n",
        "            # TODO: PROPONHA UMA ALTERAÇÃO AQUI\n",
        "\n",
        "            # planejamento / indirect RL\n",
        "            planning_new(model, planning_steps, Q, lr, gamma)\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "    state = env.reset()\n",
        "    reward = 0\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 - Novos Experimentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w8GPbGNzl8Y"
      },
      "outputs": [],
      "source": [
        "environment = gym.make(ENV_NAME_EXPERIMENT_2)\n",
        "results_2b = []\n",
        "\n",
        "results_2b.append( repeated_exec(RUNS, f\"Q-Learning \", run_qlearning, environment, EPISODES, **qlearn_params_2, auto_load=AUTO_LOAD) )\n",
        "clear_output()\n",
        "\n",
        "plan_steps = 5\n",
        "results_2b.append( repeated_exec(RUNS, f\"Dyna-Q ({plan_steps} passos)\", run_dyna_q, environment, EPISODES, **qlearn_params_2, planning_steps=plan_steps, auto_load=AUTO_LOAD) )\n",
        "clear_output()\n",
        "\n",
        "plan_steps = 5\n",
        "results_2b.append( repeated_exec(RUNS, f\"Dyna-Q-new ({plan_steps} passos)\", run_dyna_q_new, environment, EPISODES, **qlearn_params_2, planning_steps=plan_steps, auto_load=False) )\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR6Yn9bgwCvn"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results_2b, cumulative='no', x_log_scale=False, window=100, y_min=-300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n--VB2jGwCvn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "name": "cap07-main.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rl23",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
