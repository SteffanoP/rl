{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeJ3wCaKe2Wl"
      },
      "source": [
        "# Capítulo 7 - DQN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AT7LdIRGf4I"
      },
      "source": [
        "Você pode rodar este notebook no Colab ou localmente. Para abrir diretamente no Colab, basta clicar no link abaixo.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap07/cap07-main.ipynb) \n",
        "\n",
        "Para rodar localmente, primeiro, baixe todo o repositório do github: https://github.com/pablo-sampaio/rl_facil."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAHITU7VhsM7"
      },
      "source": [
        "## 1. Configurações Iniciais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyTpn6lxGf4J"
      },
      "source": [
        "### Configurações Dependentes do Sistema\n",
        "\n",
        "Rode a célula abaixo, mesmo sem estar no Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS23BU8R1vq-",
        "outputId": "7fa33772-d1a8-46e4-b3ab-354d24b041de"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from IPython.display import clear_output\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    !pip install gym\n",
        "    !pip install gym[box2d]\n",
        "    !pip install gym[atari]\n",
        "    !pip install opencv-python\n",
        "    !pip install autorom[accept-rom-license]\n",
        "    !pip install tensorboard\n",
        "\n",
        "    # para salvar videos\n",
        "    !apt-get install -y xvfb x11-utils\n",
        "    !pip install pyvirtualdisplay==0.2.*\n",
        "    !apt-get install ffmpeg\n",
        "\n",
        "    from pyvirtualdisplay import Display\n",
        "    #global display\n",
        "    display = Display(visible=False, size=(1400, 900))\n",
        "    _ = display.start()\n",
        "\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    !mv /content/rl_facil/cap07/* /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "It9SF9Iw2zXq"
      },
      "outputs": [],
      "source": [
        "# Make video\n",
        "# Set up fake display; otherwise rendering will fail\n",
        "#import os\n",
        "#os.system(\"Xvfb :1 -screen 0 0124x768x24 &\")\n",
        "#os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNQSmOCRGf4L"
      },
      "source": [
        "### Configurações para Exibir Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xTUzR6SNGf4M"
      },
      "outputs": [],
      "source": [
        "# ideias adaptadas de : https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google\n",
        "from base64 import b64encode\n",
        "from IPython.display import HTML\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def render_mp4(videopath: str) -> str:\n",
        "  \"\"\"\n",
        "  Gets a string containing a b4-encoded version of the MP4 video\n",
        "  at the specified path.\n",
        "  \"\"\"\n",
        "  mp4 = open(videopath, 'rb').read()\n",
        "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
        "  html_code = f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
        "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'\n",
        "  return HTML(html_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztvJdbKVh20Y"
      },
      "source": [
        "## 2. Imports e Definições Usadas pelo DQN\n",
        "\n",
        "Código adaptado do código explicado no livro de M. Lapan, cap. 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8YL1jIkndhA"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0Gzf7VhkiHxQ"
      },
      "outputs": [],
      "source": [
        "import dqn_models\n",
        "from atari_wrappers import *\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#from tensorboardX import SummaryWriter\n",
        "#from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_79PzMRmndhC"
      },
      "source": [
        "### Classes Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OACm0r-iuh2"
      },
      "outputs": [],
      "source": [
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "\n",
        "class DQNExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, s1, a, r, done, s2):\n",
        "        experience = Experience(s1, a, r, done, s2)\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzwVdZIf3xi6"
      },
      "source": [
        "### Funções Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGMkPSqg3wNx"
      },
      "outputs": [],
      "source": [
        "# Faz uma escolha epsilon-greedy\n",
        "def choose_action(qnet, env, state, epsilon, device):\n",
        "    done_reward = None\n",
        "    if np.random.random() < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        state_a = np.array([state], copy=False)\n",
        "        state_v = torch.tensor(state_a, dtype=torch.float32).to(device)\n",
        "        q_vals_v = qnet(state_v)\n",
        "        _, act_v = torch.max(q_vals_v, dim=1)\n",
        "        action = int(act_v.item())\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFI9Q2h4yfhw"
      },
      "outputs": [],
      "source": [
        "def test_Qpolicy(env, Qpolicy, epsilon=0.0, num_episodes=5, render=False, videorec=None):\n",
        "    episodes_returns = []\n",
        "    total_steps = 0\n",
        "    num_actions = env.action_space.n\n",
        "    #device = Qpolicy.get_device()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    for i in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        if render:\n",
        "            env.render()\n",
        "        if videorec is not None:\n",
        "            videorec.capture_frame()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        episodes_returns.append(0.0)\n",
        "        while not done:\n",
        "            action = choose_action(Qpolicy, env, obs, epsilon, device)\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            if render:\n",
        "                env.render()\n",
        "            if videorec is not None:\n",
        "                videorec.capture_frame()\n",
        "            total_steps += 1\n",
        "            episodes_returns[-1] += reward\n",
        "            steps += 1\n",
        "        print(f\"EPISODE {i+1}\")\n",
        "        print(\"- steps:\", steps)\n",
        "        print(\"- return:\", episodes_returns[-1])\n",
        "    mean_return = round(np.mean(episodes_returns), 1)\n",
        "    print(\"RESULTADO FINAL: média (por episódio):\", mean_return, end=\"\")\n",
        "    print(\", episódios:\", len(episodes_returns), end=\"\")\n",
        "    print(\", total de passos:\", total_steps)\n",
        "    if videorec is not None:\n",
        "        videorec.close()\n",
        "    return mean_return, episodes_returns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHZQuSxmndhD"
      },
      "source": [
        "### Função de Perda (*Loss Function*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PH-mJQ6ndhD"
      },
      "outputs": [],
      "source": [
        "def calc_loss(batch, net, tgt_net, gamma, device=\"cpu\"):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "    states_v = torch.tensor(states, dtype=torch.float32).to(device)\n",
        "    next_states_v = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
        "    actions_v = torch.tensor(actions, dtype=torch.int64).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.tensor(dones, dtype=torch.bool).to(device)\n",
        "\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
        "    next_state_values[done_mask] = 0.0\n",
        "    next_state_values = next_state_values.detach()\n",
        "\n",
        "    expected_state_action_values = next_state_values * gamma + rewards_v\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c241EVuundhE"
      },
      "source": [
        "## 3 - Função Principal do DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsqgk9RNndhE"
      },
      "source": [
        "Esta é a função que faz o aprendizado. (Porém, o DQN é uma solução que incluir também as ideias dos wrappers.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO3eR_sfndhE"
      },
      "outputs": [],
      "source": [
        "def DQN_TRAIN(env, env_name, qnet, qnet_lr, tgt_qnet, target_update_freq, gamma, replay_size, batch_size, epsilon_f, epsilon_decay_last_step, MEAN_REWARD_BOUND):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    qnet = qnet.to(device)\n",
        "    tgt_qnet = tgt_qnet.to(device)\n",
        "\n",
        "    writer = SummaryWriter(comment=\"-\" + env_name)\n",
        "    print(qnet)\n",
        "\n",
        "    buffer = DQNExperienceBuffer(replay_size)\n",
        "    epsilon = 1.0\n",
        "\n",
        "    optimizer = optim.Adam(qnet.parameters(), lr=qnet_lr)\n",
        "    total_rewards = []\n",
        "    frame_idx = 0\n",
        "    ts_frame = 0\n",
        "    ts = time.time()\n",
        "    best_mean_reward = None\n",
        "    start_time_str = datetime.now().strftime(\"%Y-%m-%d,%H-%M-%S\")\n",
        "\n",
        "    state = env.reset()\n",
        "    total_reward = 0.0\n",
        "\n",
        "    while True:\n",
        "        frame_idx += 1\n",
        "        epsilon = max(epsilon_f, 1.0 - frame_idx / epsilon_decay_last_step)\n",
        "\n",
        "        action = choose_action(qnet, env, state, epsilon, device)\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        buffer.append(state, action, reward, is_done, new_state)\n",
        "        state = new_state\n",
        "        \n",
        "        if is_done:\n",
        "            total_rewards.append(total_reward)\n",
        "            state = env.reset()\n",
        "            total_reward = 0.0\n",
        "            if (time.time() - ts) == 0:\n",
        "                speed = float(\"-inf\")\n",
        "            else:\n",
        "                speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "            ts_frame = frame_idx\n",
        "            ts = time.time()\n",
        "            mean_reward = np.mean(total_rewards[-100:])\n",
        "            print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
        "                frame_idx, len(total_rewards), mean_reward, epsilon,\n",
        "                speed\n",
        "            ))\n",
        "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "            writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
        "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "                torch.save(qnet.state_dict(), env_name + \"-\" + start_time_str + \"-best.dat\")\n",
        "                # IDEIA: salvar também a iteração, para poder retomar\n",
        "                if best_mean_reward is not None:\n",
        "                    print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
        "                best_mean_reward = mean_reward\n",
        "            if mean_reward > MEAN_REWARD_BOUND:\n",
        "                print(\"Solved in %d frames!\" % frame_idx)\n",
        "                break\n",
        "\n",
        "        if len(buffer) < replay_size:\n",
        "            continue\n",
        "\n",
        "        if frame_idx % target_update_freq == 0:\n",
        "            tgt_qnet.load_state_dict(qnet.state_dict())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch = buffer.sample(batch_size)\n",
        "        loss_t = calc_loss(batch, qnet, tgt_qnet, gamma, device=device)\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZq9U2FGndhF"
      },
      "source": [
        "## 4 - Rodando em Ambientes Simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SigPdPRcndhF"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"MountainCar-v0\"\n",
        "REWARD_BOUND = -120\n",
        "#ENV_NAME = \"CartPole-v0\"\n",
        "#REWARD_BOUND = 200\n",
        "\n",
        "GAMMA = 0.999\n",
        "REPLAY_SIZE = 2000\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "SYNC_TARGET_FRAMES = 250\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 100000\n",
        "EPSILON_FINAL = 0.02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psuJKKQlndhF"
      },
      "outputs": [],
      "source": [
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "qnet = dqn_models.MLP(env.observation_space.shape[0], [128,256], env.action_space.n)\n",
        "qtarget = dqn_models.MLP(env.observation_space.shape[0], [128,256], env.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iniciando a visualização antes da execução, para acompanhar\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5E2S9WAndhG",
        "outputId": "5e1336ad-6026-4883-e0df-177d732384da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=2, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=256, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "200: done 1 games, mean reward -200.000, eps 1.00, speed 17700.47 f/s\n",
            "400: done 2 games, mean reward -200.000, eps 0.99, speed 15918.87 f/s\n",
            "600: done 3 games, mean reward -200.000, eps 0.99, speed 22539.72 f/s\n",
            "800: done 4 games, mean reward -200.000, eps 0.99, speed 17249.15 f/s\n",
            "1000: done 5 games, mean reward -200.000, eps 0.99, speed 19348.21 f/s\n",
            "1200: done 6 games, mean reward -200.000, eps 0.98, speed 15326.70 f/s\n",
            "1400: done 7 games, mean reward -200.000, eps 0.98, speed 22668.85 f/s\n",
            "1600: done 8 games, mean reward -200.000, eps 0.98, speed 16580.90 f/s\n",
            "1800: done 9 games, mean reward -200.000, eps 0.98, speed 18375.12 f/s\n",
            "2000: done 10 games, mean reward -200.000, eps 0.97, speed 17746.91 f/s\n",
            "2200: done 11 games, mean reward -200.000, eps 0.97, speed 445.21 f/s\n",
            "2400: done 12 games, mean reward -200.000, eps 0.97, speed 448.84 f/s\n",
            "2600: done 13 games, mean reward -200.000, eps 0.97, speed 457.98 f/s\n",
            "2800: done 14 games, mean reward -200.000, eps 0.96, speed 443.30 f/s\n",
            "3000: done 15 games, mean reward -200.000, eps 0.96, speed 451.31 f/s\n",
            "3200: done 16 games, mean reward -200.000, eps 0.96, speed 445.60 f/s\n",
            "3400: done 17 games, mean reward -200.000, eps 0.96, speed 457.19 f/s\n",
            "3600: done 18 games, mean reward -200.000, eps 0.95, speed 452.85 f/s\n",
            "3800: done 19 games, mean reward -200.000, eps 0.95, speed 442.62 f/s\n",
            "4000: done 20 games, mean reward -200.000, eps 0.95, speed 447.60 f/s\n",
            "4200: done 21 games, mean reward -200.000, eps 0.95, speed 448.84 f/s\n",
            "4400: done 22 games, mean reward -200.000, eps 0.94, speed 448.02 f/s\n",
            "4600: done 23 games, mean reward -200.000, eps 0.94, speed 446.09 f/s\n",
            "4800: done 24 games, mean reward -200.000, eps 0.94, speed 460.08 f/s\n",
            "5000: done 25 games, mean reward -200.000, eps 0.94, speed 452.61 f/s\n",
            "5200: done 26 games, mean reward -200.000, eps 0.94, speed 460.19 f/s\n",
            "5400: done 27 games, mean reward -200.000, eps 0.93, speed 444.62 f/s\n",
            "5600: done 28 games, mean reward -200.000, eps 0.93, speed 440.93 f/s\n",
            "5800: done 29 games, mean reward -200.000, eps 0.93, speed 437.94 f/s\n",
            "6000: done 30 games, mean reward -200.000, eps 0.93, speed 451.45 f/s\n",
            "6200: done 31 games, mean reward -200.000, eps 0.92, speed 452.20 f/s\n",
            "6400: done 32 games, mean reward -200.000, eps 0.92, speed 438.13 f/s\n",
            "6600: done 33 games, mean reward -200.000, eps 0.92, speed 442.54 f/s\n",
            "6800: done 34 games, mean reward -200.000, eps 0.92, speed 450.26 f/s\n",
            "7000: done 35 games, mean reward -200.000, eps 0.91, speed 444.37 f/s\n",
            "7200: done 36 games, mean reward -200.000, eps 0.91, speed 451.21 f/s\n",
            "7400: done 37 games, mean reward -200.000, eps 0.91, speed 452.14 f/s\n",
            "7600: done 38 games, mean reward -200.000, eps 0.91, speed 445.15 f/s\n",
            "7800: done 39 games, mean reward -200.000, eps 0.90, speed 436.27 f/s\n",
            "8000: done 40 games, mean reward -200.000, eps 0.90, speed 451.78 f/s\n",
            "8200: done 41 games, mean reward -200.000, eps 0.90, speed 438.61 f/s\n",
            "8400: done 42 games, mean reward -200.000, eps 0.90, speed 445.55 f/s\n",
            "8600: done 43 games, mean reward -200.000, eps 0.89, speed 449.97 f/s\n",
            "8800: done 44 games, mean reward -200.000, eps 0.89, speed 450.30 f/s\n",
            "9000: done 45 games, mean reward -200.000, eps 0.89, speed 444.23 f/s\n",
            "9200: done 46 games, mean reward -200.000, eps 0.89, speed 445.77 f/s\n",
            "9400: done 47 games, mean reward -200.000, eps 0.88, speed 442.42 f/s\n",
            "9600: done 48 games, mean reward -200.000, eps 0.88, speed 443.73 f/s\n",
            "9800: done 49 games, mean reward -200.000, eps 0.88, speed 448.79 f/s\n",
            "10000: done 50 games, mean reward -200.000, eps 0.88, speed 456.39 f/s\n",
            "10200: done 51 games, mean reward -200.000, eps 0.87, speed 444.33 f/s\n",
            "10400: done 52 games, mean reward -200.000, eps 0.87, speed 438.32 f/s\n",
            "10600: done 53 games, mean reward -200.000, eps 0.87, speed 442.64 f/s\n",
            "10800: done 54 games, mean reward -200.000, eps 0.86, speed 450.94 f/s\n",
            "11000: done 55 games, mean reward -200.000, eps 0.86, speed 429.75 f/s\n",
            "11200: done 56 games, mean reward -200.000, eps 0.86, speed 444.39 f/s\n",
            "11400: done 57 games, mean reward -200.000, eps 0.86, speed 448.30 f/s\n",
            "11600: done 58 games, mean reward -200.000, eps 0.85, speed 443.27 f/s\n",
            "11800: done 59 games, mean reward -200.000, eps 0.85, speed 445.34 f/s\n",
            "12000: done 60 games, mean reward -200.000, eps 0.85, speed 441.96 f/s\n",
            "12200: done 61 games, mean reward -200.000, eps 0.85, speed 444.09 f/s\n",
            "12400: done 62 games, mean reward -200.000, eps 0.84, speed 422.85 f/s\n",
            "12600: done 63 games, mean reward -200.000, eps 0.84, speed 445.70 f/s\n",
            "12800: done 64 games, mean reward -200.000, eps 0.84, speed 445.68 f/s\n",
            "13000: done 65 games, mean reward -200.000, eps 0.84, speed 447.60 f/s\n",
            "13200: done 66 games, mean reward -200.000, eps 0.83, speed 442.73 f/s\n",
            "13400: done 67 games, mean reward -200.000, eps 0.83, speed 444.14 f/s\n",
            "13600: done 68 games, mean reward -200.000, eps 0.83, speed 452.34 f/s\n",
            "13800: done 69 games, mean reward -200.000, eps 0.83, speed 437.80 f/s\n",
            "14000: done 70 games, mean reward -200.000, eps 0.82, speed 447.68 f/s\n",
            "14200: done 71 games, mean reward -200.000, eps 0.82, speed 441.52 f/s\n",
            "14400: done 72 games, mean reward -200.000, eps 0.82, speed 445.19 f/s\n",
            "14600: done 73 games, mean reward -200.000, eps 0.82, speed 440.43 f/s\n",
            "14800: done 74 games, mean reward -200.000, eps 0.81, speed 445.53 f/s\n",
            "15000: done 75 games, mean reward -200.000, eps 0.81, speed 439.40 f/s\n",
            "15200: done 76 games, mean reward -200.000, eps 0.81, speed 448.81 f/s\n",
            "15400: done 77 games, mean reward -200.000, eps 0.81, speed 445.02 f/s\n",
            "15600: done 78 games, mean reward -200.000, eps 0.80, speed 433.06 f/s\n",
            "15800: done 79 games, mean reward -200.000, eps 0.80, speed 456.82 f/s\n",
            "16000: done 80 games, mean reward -200.000, eps 0.80, speed 444.11 f/s\n",
            "16200: done 81 games, mean reward -200.000, eps 0.80, speed 432.19 f/s\n",
            "16400: done 82 games, mean reward -200.000, eps 0.80, speed 442.71 f/s\n",
            "16600: done 83 games, mean reward -200.000, eps 0.79, speed 452.18 f/s\n",
            "16800: done 84 games, mean reward -200.000, eps 0.79, speed 446.46 f/s\n",
            "17000: done 85 games, mean reward -200.000, eps 0.79, speed 446.15 f/s\n",
            "17200: done 86 games, mean reward -200.000, eps 0.79, speed 454.42 f/s\n",
            "17400: done 87 games, mean reward -200.000, eps 0.78, speed 427.06 f/s\n",
            "17600: done 88 games, mean reward -200.000, eps 0.78, speed 441.41 f/s\n",
            "17800: done 89 games, mean reward -200.000, eps 0.78, speed 433.85 f/s\n",
            "18000: done 90 games, mean reward -200.000, eps 0.78, speed 444.62 f/s\n",
            "18200: done 91 games, mean reward -200.000, eps 0.77, speed 428.58 f/s\n",
            "18400: done 92 games, mean reward -200.000, eps 0.77, speed 441.38 f/s\n",
            "18600: done 93 games, mean reward -200.000, eps 0.77, speed 436.99 f/s\n",
            "18800: done 94 games, mean reward -200.000, eps 0.77, speed 442.13 f/s\n",
            "19000: done 95 games, mean reward -200.000, eps 0.76, speed 438.67 f/s\n",
            "19200: done 96 games, mean reward -200.000, eps 0.76, speed 440.52 f/s\n",
            "19400: done 97 games, mean reward -200.000, eps 0.76, speed 437.37 f/s\n",
            "19600: done 98 games, mean reward -200.000, eps 0.76, speed 328.60 f/s\n",
            "19800: done 99 games, mean reward -200.000, eps 0.75, speed 346.99 f/s\n",
            "20000: done 100 games, mean reward -200.000, eps 0.75, speed 422.36 f/s\n",
            "20200: done 101 games, mean reward -200.000, eps 0.75, speed 366.90 f/s\n",
            "20400: done 102 games, mean reward -200.000, eps 0.74, speed 233.30 f/s\n",
            "20600: done 103 games, mean reward -200.000, eps 0.74, speed 302.91 f/s\n",
            "20800: done 104 games, mean reward -200.000, eps 0.74, speed 439.78 f/s\n",
            "21000: done 105 games, mean reward -200.000, eps 0.74, speed 428.19 f/s\n",
            "21200: done 106 games, mean reward -200.000, eps 0.73, speed 434.10 f/s\n",
            "21400: done 107 games, mean reward -200.000, eps 0.73, speed 429.62 f/s\n",
            "21600: done 108 games, mean reward -200.000, eps 0.73, speed 432.14 f/s\n",
            "21800: done 109 games, mean reward -200.000, eps 0.73, speed 430.15 f/s\n",
            "22000: done 110 games, mean reward -200.000, eps 0.72, speed 441.25 f/s\n",
            "22200: done 111 games, mean reward -200.000, eps 0.72, speed 440.90 f/s\n",
            "22400: done 112 games, mean reward -200.000, eps 0.72, speed 436.06 f/s\n",
            "22600: done 113 games, mean reward -200.000, eps 0.72, speed 430.61 f/s\n",
            "22800: done 114 games, mean reward -200.000, eps 0.72, speed 434.33 f/s\n",
            "23000: done 115 games, mean reward -200.000, eps 0.71, speed 434.25 f/s\n",
            "23200: done 116 games, mean reward -200.000, eps 0.71, speed 429.55 f/s\n",
            "23400: done 117 games, mean reward -200.000, eps 0.71, speed 432.00 f/s\n",
            "23600: done 118 games, mean reward -200.000, eps 0.71, speed 423.58 f/s\n",
            "23800: done 119 games, mean reward -200.000, eps 0.70, speed 425.32 f/s\n",
            "24000: done 120 games, mean reward -200.000, eps 0.70, speed 435.25 f/s\n",
            "24200: done 121 games, mean reward -200.000, eps 0.70, speed 411.68 f/s\n",
            "24400: done 122 games, mean reward -200.000, eps 0.70, speed 426.79 f/s\n",
            "24600: done 123 games, mean reward -200.000, eps 0.69, speed 436.43 f/s\n",
            "24800: done 124 games, mean reward -200.000, eps 0.69, speed 433.04 f/s\n",
            "25000: done 125 games, mean reward -200.000, eps 0.69, speed 419.18 f/s\n",
            "25200: done 126 games, mean reward -200.000, eps 0.69, speed 418.51 f/s\n",
            "25400: done 127 games, mean reward -200.000, eps 0.68, speed 446.64 f/s\n",
            "25600: done 128 games, mean reward -200.000, eps 0.68, speed 433.67 f/s\n",
            "25800: done 129 games, mean reward -200.000, eps 0.68, speed 432.74 f/s\n",
            "26000: done 130 games, mean reward -200.000, eps 0.68, speed 438.30 f/s\n",
            "26200: done 131 games, mean reward -200.000, eps 0.67, speed 431.32 f/s\n",
            "26400: done 132 games, mean reward -200.000, eps 0.67, speed 364.64 f/s\n",
            "26600: done 133 games, mean reward -200.000, eps 0.67, speed 428.89 f/s\n",
            "26800: done 134 games, mean reward -200.000, eps 0.67, speed 436.45 f/s\n",
            "27000: done 135 games, mean reward -200.000, eps 0.66, speed 421.05 f/s\n",
            "27200: done 136 games, mean reward -200.000, eps 0.66, speed 441.64 f/s\n",
            "27400: done 137 games, mean reward -200.000, eps 0.66, speed 419.51 f/s\n",
            "27600: done 138 games, mean reward -200.000, eps 0.66, speed 431.96 f/s\n",
            "27800: done 139 games, mean reward -200.000, eps 0.65, speed 431.51 f/s\n",
            "28000: done 140 games, mean reward -200.000, eps 0.65, speed 446.29 f/s\n",
            "28200: done 141 games, mean reward -200.000, eps 0.65, speed 439.21 f/s\n",
            "28400: done 142 games, mean reward -200.000, eps 0.65, speed 436.85 f/s\n",
            "28600: done 143 games, mean reward -200.000, eps 0.64, speed 419.83 f/s\n",
            "28800: done 144 games, mean reward -200.000, eps 0.64, speed 427.80 f/s\n",
            "29000: done 145 games, mean reward -200.000, eps 0.64, speed 428.38 f/s\n",
            "29200: done 146 games, mean reward -200.000, eps 0.64, speed 421.30 f/s\n",
            "29400: done 147 games, mean reward -200.000, eps 0.63, speed 402.86 f/s\n",
            "29600: done 148 games, mean reward -200.000, eps 0.63, speed 346.50 f/s\n",
            "29800: done 149 games, mean reward -200.000, eps 0.63, speed 435.80 f/s\n",
            "30000: done 150 games, mean reward -200.000, eps 0.62, speed 423.63 f/s\n",
            "30200: done 151 games, mean reward -200.000, eps 0.62, speed 420.31 f/s\n",
            "30400: done 152 games, mean reward -200.000, eps 0.62, speed 247.41 f/s\n",
            "30600: done 153 games, mean reward -200.000, eps 0.62, speed 263.00 f/s\n",
            "30800: done 154 games, mean reward -200.000, eps 0.61, speed 432.33 f/s\n",
            "31000: done 155 games, mean reward -200.000, eps 0.61, speed 428.74 f/s\n",
            "31200: done 156 games, mean reward -200.000, eps 0.61, speed 431.74 f/s\n",
            "31400: done 157 games, mean reward -200.000, eps 0.61, speed 437.01 f/s\n",
            "31600: done 158 games, mean reward -200.000, eps 0.60, speed 438.82 f/s\n",
            "31800: done 159 games, mean reward -200.000, eps 0.60, speed 431.12 f/s\n",
            "32000: done 160 games, mean reward -200.000, eps 0.60, speed 427.10 f/s\n",
            "32200: done 161 games, mean reward -200.000, eps 0.60, speed 436.79 f/s\n",
            "32400: done 162 games, mean reward -200.000, eps 0.59, speed 430.03 f/s\n",
            "32600: done 163 games, mean reward -200.000, eps 0.59, speed 434.48 f/s\n",
            "32800: done 164 games, mean reward -200.000, eps 0.59, speed 416.87 f/s\n",
            "33000: done 165 games, mean reward -200.000, eps 0.59, speed 438.72 f/s\n",
            "33200: done 166 games, mean reward -200.000, eps 0.58, speed 427.36 f/s\n",
            "33400: done 167 games, mean reward -200.000, eps 0.58, speed 430.19 f/s\n",
            "33600: done 168 games, mean reward -200.000, eps 0.58, speed 438.19 f/s\n",
            "33800: done 169 games, mean reward -200.000, eps 0.58, speed 433.40 f/s\n",
            "34000: done 170 games, mean reward -200.000, eps 0.57, speed 433.73 f/s\n",
            "34200: done 171 games, mean reward -200.000, eps 0.57, speed 426.43 f/s\n",
            "34400: done 172 games, mean reward -200.000, eps 0.57, speed 429.47 f/s\n",
            "34600: done 173 games, mean reward -200.000, eps 0.57, speed 433.20 f/s\n",
            "34800: done 174 games, mean reward -200.000, eps 0.56, speed 434.16 f/s\n",
            "35000: done 175 games, mean reward -200.000, eps 0.56, speed 433.13 f/s\n",
            "35200: done 176 games, mean reward -200.000, eps 0.56, speed 429.92 f/s\n",
            "35400: done 177 games, mean reward -200.000, eps 0.56, speed 435.53 f/s\n",
            "35600: done 178 games, mean reward -200.000, eps 0.55, speed 425.52 f/s\n",
            "35800: done 179 games, mean reward -200.000, eps 0.55, speed 421.17 f/s\n",
            "36000: done 180 games, mean reward -200.000, eps 0.55, speed 440.48 f/s\n",
            "36200: done 181 games, mean reward -200.000, eps 0.55, speed 442.39 f/s\n",
            "36400: done 182 games, mean reward -200.000, eps 0.54, speed 423.53 f/s\n",
            "36600: done 183 games, mean reward -200.000, eps 0.54, speed 429.93 f/s\n",
            "36800: done 184 games, mean reward -200.000, eps 0.54, speed 421.66 f/s\n",
            "37000: done 185 games, mean reward -200.000, eps 0.54, speed 429.48 f/s\n",
            "37200: done 186 games, mean reward -200.000, eps 0.53, speed 411.98 f/s\n",
            "37400: done 187 games, mean reward -200.000, eps 0.53, speed 430.83 f/s\n",
            "37600: done 188 games, mean reward -200.000, eps 0.53, speed 431.15 f/s\n",
            "37800: done 189 games, mean reward -200.000, eps 0.53, speed 415.28 f/s\n",
            "38000: done 190 games, mean reward -200.000, eps 0.53, speed 404.17 f/s\n",
            "38200: done 191 games, mean reward -200.000, eps 0.52, speed 409.50 f/s\n",
            "38400: done 192 games, mean reward -200.000, eps 0.52, speed 424.34 f/s\n",
            "38600: done 193 games, mean reward -200.000, eps 0.52, speed 422.96 f/s\n",
            "38800: done 194 games, mean reward -200.000, eps 0.52, speed 427.65 f/s\n",
            "39000: done 195 games, mean reward -200.000, eps 0.51, speed 424.04 f/s\n",
            "39200: done 196 games, mean reward -200.000, eps 0.51, speed 432.99 f/s\n",
            "39400: done 197 games, mean reward -200.000, eps 0.51, speed 434.46 f/s\n",
            "39600: done 198 games, mean reward -200.000, eps 0.51, speed 435.73 f/s\n",
            "39800: done 199 games, mean reward -200.000, eps 0.50, speed 420.09 f/s\n",
            "40000: done 200 games, mean reward -200.000, eps 0.50, speed 430.79 f/s\n",
            "40200: done 201 games, mean reward -200.000, eps 0.50, speed 424.19 f/s\n",
            "40400: done 202 games, mean reward -200.000, eps 0.49, speed 438.46 f/s\n",
            "40600: done 203 games, mean reward -200.000, eps 0.49, speed 425.60 f/s\n",
            "40800: done 204 games, mean reward -200.000, eps 0.49, speed 430.14 f/s\n",
            "41000: done 205 games, mean reward -200.000, eps 0.49, speed 423.88 f/s\n",
            "41200: done 206 games, mean reward -200.000, eps 0.48, speed 434.57 f/s\n",
            "41400: done 207 games, mean reward -200.000, eps 0.48, speed 436.34 f/s\n",
            "41600: done 208 games, mean reward -200.000, eps 0.48, speed 401.51 f/s\n",
            "41800: done 209 games, mean reward -200.000, eps 0.48, speed 426.48 f/s\n",
            "42000: done 210 games, mean reward -200.000, eps 0.47, speed 415.27 f/s\n",
            "42200: done 211 games, mean reward -200.000, eps 0.47, speed 428.02 f/s\n",
            "42400: done 212 games, mean reward -200.000, eps 0.47, speed 414.57 f/s\n",
            "42600: done 213 games, mean reward -200.000, eps 0.47, speed 430.70 f/s\n",
            "42800: done 214 games, mean reward -200.000, eps 0.46, speed 412.27 f/s\n",
            "43000: done 215 games, mean reward -200.000, eps 0.46, speed 417.46 f/s\n",
            "43200: done 216 games, mean reward -200.000, eps 0.46, speed 416.40 f/s\n",
            "43400: done 217 games, mean reward -200.000, eps 0.46, speed 422.02 f/s\n",
            "43600: done 218 games, mean reward -200.000, eps 0.45, speed 418.94 f/s\n",
            "43800: done 219 games, mean reward -200.000, eps 0.45, speed 425.50 f/s\n",
            "44000: done 220 games, mean reward -200.000, eps 0.45, speed 419.03 f/s\n",
            "44200: done 221 games, mean reward -200.000, eps 0.45, speed 421.22 f/s\n",
            "44400: done 222 games, mean reward -200.000, eps 0.44, speed 411.42 f/s\n",
            "44600: done 223 games, mean reward -200.000, eps 0.44, speed 429.36 f/s\n",
            "44800: done 224 games, mean reward -200.000, eps 0.44, speed 419.88 f/s\n",
            "45000: done 225 games, mean reward -200.000, eps 0.44, speed 416.97 f/s\n",
            "45200: done 226 games, mean reward -200.000, eps 0.44, speed 419.11 f/s\n",
            "45400: done 227 games, mean reward -200.000, eps 0.43, speed 415.38 f/s\n",
            "45600: done 228 games, mean reward -200.000, eps 0.43, speed 431.45 f/s\n",
            "45800: done 229 games, mean reward -200.000, eps 0.43, speed 419.55 f/s\n",
            "46000: done 230 games, mean reward -200.000, eps 0.43, speed 413.70 f/s\n",
            "46200: done 231 games, mean reward -200.000, eps 0.42, speed 415.65 f/s\n",
            "46400: done 232 games, mean reward -200.000, eps 0.42, speed 427.64 f/s\n",
            "46600: done 233 games, mean reward -200.000, eps 0.42, speed 411.01 f/s\n",
            "46800: done 234 games, mean reward -200.000, eps 0.42, speed 422.82 f/s\n",
            "47000: done 235 games, mean reward -200.000, eps 0.41, speed 420.89 f/s\n",
            "47200: done 236 games, mean reward -200.000, eps 0.41, speed 422.59 f/s\n",
            "47400: done 237 games, mean reward -200.000, eps 0.41, speed 420.37 f/s\n",
            "47600: done 238 games, mean reward -200.000, eps 0.41, speed 423.93 f/s\n",
            "47800: done 239 games, mean reward -200.000, eps 0.40, speed 429.72 f/s\n",
            "48000: done 240 games, mean reward -200.000, eps 0.40, speed 416.97 f/s\n",
            "48200: done 241 games, mean reward -200.000, eps 0.40, speed 424.53 f/s\n",
            "48400: done 242 games, mean reward -200.000, eps 0.40, speed 416.13 f/s\n",
            "48600: done 243 games, mean reward -200.000, eps 0.39, speed 416.30 f/s\n",
            "48800: done 244 games, mean reward -200.000, eps 0.39, speed 419.97 f/s\n",
            "49000: done 245 games, mean reward -200.000, eps 0.39, speed 426.14 f/s\n",
            "49200: done 246 games, mean reward -200.000, eps 0.39, speed 424.03 f/s\n",
            "49400: done 247 games, mean reward -200.000, eps 0.38, speed 422.85 f/s\n",
            "49600: done 248 games, mean reward -200.000, eps 0.38, speed 403.88 f/s\n",
            "49800: done 249 games, mean reward -200.000, eps 0.38, speed 422.45 f/s\n",
            "50000: done 250 games, mean reward -200.000, eps 0.38, speed 415.73 f/s\n",
            "50200: done 251 games, mean reward -200.000, eps 0.37, speed 423.97 f/s\n",
            "50400: done 252 games, mean reward -200.000, eps 0.37, speed 397.56 f/s\n",
            "50600: done 253 games, mean reward -200.000, eps 0.37, speed 420.98 f/s\n",
            "50800: done 254 games, mean reward -200.000, eps 0.36, speed 413.09 f/s\n",
            "51000: done 255 games, mean reward -200.000, eps 0.36, speed 424.21 f/s\n",
            "51200: done 256 games, mean reward -200.000, eps 0.36, speed 417.34 f/s\n",
            "51400: done 257 games, mean reward -200.000, eps 0.36, speed 415.56 f/s\n",
            "51600: done 258 games, mean reward -200.000, eps 0.35, speed 420.34 f/s\n",
            "51800: done 259 games, mean reward -200.000, eps 0.35, speed 407.66 f/s\n",
            "52000: done 260 games, mean reward -200.000, eps 0.35, speed 408.05 f/s\n",
            "52200: done 261 games, mean reward -200.000, eps 0.35, speed 408.88 f/s\n",
            "52400: done 262 games, mean reward -200.000, eps 0.34, speed 418.57 f/s\n",
            "52600: done 263 games, mean reward -200.000, eps 0.34, speed 412.21 f/s\n",
            "52800: done 264 games, mean reward -200.000, eps 0.34, speed 413.37 f/s\n",
            "53000: done 265 games, mean reward -200.000, eps 0.34, speed 417.88 f/s\n",
            "53200: done 266 games, mean reward -200.000, eps 0.33, speed 421.19 f/s\n",
            "53400: done 267 games, mean reward -200.000, eps 0.33, speed 406.27 f/s\n",
            "53600: done 268 games, mean reward -200.000, eps 0.33, speed 413.08 f/s\n",
            "53800: done 269 games, mean reward -200.000, eps 0.33, speed 412.11 f/s\n",
            "54000: done 270 games, mean reward -200.000, eps 0.32, speed 408.63 f/s\n",
            "54200: done 271 games, mean reward -200.000, eps 0.32, speed 405.99 f/s\n",
            "54400: done 272 games, mean reward -200.000, eps 0.32, speed 421.11 f/s\n",
            "54600: done 273 games, mean reward -200.000, eps 0.32, speed 388.81 f/s\n",
            "54800: done 274 games, mean reward -200.000, eps 0.31, speed 399.18 f/s\n",
            "55000: done 275 games, mean reward -200.000, eps 0.31, speed 405.45 f/s\n",
            "55200: done 276 games, mean reward -200.000, eps 0.31, speed 408.89 f/s\n",
            "55400: done 277 games, mean reward -200.000, eps 0.31, speed 407.29 f/s\n",
            "55600: done 278 games, mean reward -200.000, eps 0.31, speed 415.14 f/s\n",
            "55800: done 279 games, mean reward -200.000, eps 0.30, speed 384.48 f/s\n",
            "56000: done 280 games, mean reward -200.000, eps 0.30, speed 411.17 f/s\n",
            "56200: done 281 games, mean reward -200.000, eps 0.30, speed 410.66 f/s\n",
            "56400: done 282 games, mean reward -200.000, eps 0.30, speed 407.26 f/s\n",
            "56600: done 283 games, mean reward -200.000, eps 0.29, speed 409.16 f/s\n",
            "56800: done 284 games, mean reward -200.000, eps 0.29, speed 410.73 f/s\n",
            "57000: done 285 games, mean reward -200.000, eps 0.29, speed 413.70 f/s\n",
            "57200: done 286 games, mean reward -200.000, eps 0.29, speed 407.28 f/s\n",
            "57400: done 287 games, mean reward -200.000, eps 0.28, speed 416.24 f/s\n",
            "57600: done 288 games, mean reward -200.000, eps 0.28, speed 411.43 f/s\n",
            "57800: done 289 games, mean reward -200.000, eps 0.28, speed 409.75 f/s\n",
            "58000: done 290 games, mean reward -200.000, eps 0.28, speed 398.74 f/s\n",
            "58200: done 291 games, mean reward -200.000, eps 0.27, speed 412.52 f/s\n",
            "58400: done 292 games, mean reward -200.000, eps 0.27, speed 406.37 f/s\n",
            "58600: done 293 games, mean reward -200.000, eps 0.27, speed 399.87 f/s\n",
            "58800: done 294 games, mean reward -200.000, eps 0.27, speed 399.41 f/s\n",
            "59000: done 295 games, mean reward -200.000, eps 0.26, speed 402.92 f/s\n",
            "59200: done 296 games, mean reward -200.000, eps 0.26, speed 413.95 f/s\n",
            "59400: done 297 games, mean reward -200.000, eps 0.26, speed 418.67 f/s\n",
            "59600: done 298 games, mean reward -200.000, eps 0.26, speed 399.20 f/s\n",
            "59800: done 299 games, mean reward -200.000, eps 0.25, speed 419.85 f/s\n",
            "60000: done 300 games, mean reward -200.000, eps 0.25, speed 410.07 f/s\n",
            "60200: done 301 games, mean reward -200.000, eps 0.25, speed 417.85 f/s\n",
            "60400: done 302 games, mean reward -200.000, eps 0.24, speed 414.03 f/s\n",
            "60600: done 303 games, mean reward -200.000, eps 0.24, speed 416.52 f/s\n",
            "60800: done 304 games, mean reward -200.000, eps 0.24, speed 402.14 f/s\n",
            "61000: done 305 games, mean reward -200.000, eps 0.24, speed 424.08 f/s\n",
            "61177: done 306 games, mean reward -199.770, eps 0.24, speed 427.12 f/s\n",
            "Best mean reward updated -200.000 -> -199.770, model saved\n",
            "61344: done 307 games, mean reward -199.440, eps 0.23, speed 399.71 f/s\n",
            "Best mean reward updated -199.770 -> -199.440, model saved\n",
            "61544: done 308 games, mean reward -199.440, eps 0.23, speed 313.14 f/s\n",
            "61726: done 309 games, mean reward -199.260, eps 0.23, speed 353.27 f/s\n",
            "Best mean reward updated -199.440 -> -199.260, model saved\n",
            "61873: done 310 games, mean reward -198.730, eps 0.23, speed 412.38 f/s\n",
            "Best mean reward updated -199.260 -> -198.730, model saved\n",
            "62073: done 311 games, mean reward -198.730, eps 0.22, speed 412.77 f/s\n",
            "62220: done 312 games, mean reward -198.200, eps 0.22, speed 406.11 f/s\n",
            "Best mean reward updated -198.730 -> -198.200, model saved\n",
            "62420: done 313 games, mean reward -198.200, eps 0.22, speed 403.72 f/s\n",
            "62599: done 314 games, mean reward -197.990, eps 0.22, speed 412.79 f/s\n",
            "Best mean reward updated -198.200 -> -197.990, model saved\n",
            "62749: done 315 games, mean reward -197.490, eps 0.22, speed 404.32 f/s\n",
            "Best mean reward updated -197.990 -> -197.490, model saved\n",
            "62914: done 316 games, mean reward -197.140, eps 0.21, speed 392.07 f/s\n",
            "Best mean reward updated -197.490 -> -197.140, model saved\n",
            "63093: done 317 games, mean reward -196.930, eps 0.21, speed 397.09 f/s\n",
            "Best mean reward updated -197.140 -> -196.930, model saved\n",
            "63268: done 318 games, mean reward -196.680, eps 0.21, speed 401.49 f/s\n",
            "Best mean reward updated -196.930 -> -196.680, model saved\n",
            "63402: done 319 games, mean reward -196.020, eps 0.21, speed 413.01 f/s\n",
            "Best mean reward updated -196.680 -> -196.020, model saved\n",
            "63585: done 320 games, mean reward -195.850, eps 0.21, speed 411.83 f/s\n",
            "Best mean reward updated -196.020 -> -195.850, model saved\n",
            "63776: done 321 games, mean reward -195.760, eps 0.20, speed 414.33 f/s\n",
            "Best mean reward updated -195.850 -> -195.760, model saved\n",
            "63936: done 322 games, mean reward -195.360, eps 0.20, speed 414.59 f/s\n",
            "Best mean reward updated -195.760 -> -195.360, model saved\n",
            "64090: done 323 games, mean reward -194.900, eps 0.20, speed 415.42 f/s\n",
            "Best mean reward updated -195.360 -> -194.900, model saved\n",
            "64290: done 324 games, mean reward -194.900, eps 0.20, speed 419.36 f/s\n",
            "64490: done 325 games, mean reward -194.900, eps 0.19, speed 415.72 f/s\n",
            "64686: done 326 games, mean reward -194.860, eps 0.19, speed 418.48 f/s\n",
            "Best mean reward updated -194.900 -> -194.860, model saved\n",
            "64874: done 327 games, mean reward -194.740, eps 0.19, speed 423.09 f/s\n",
            "Best mean reward updated -194.860 -> -194.740, model saved\n",
            "64999: done 328 games, mean reward -193.990, eps 0.19, speed 400.93 f/s\n",
            "Best mean reward updated -194.740 -> -193.990, model saved\n",
            "65170: done 329 games, mean reward -193.700, eps 0.19, speed 417.72 f/s\n",
            "Best mean reward updated -193.990 -> -193.700, model saved\n",
            "65289: done 330 games, mean reward -192.890, eps 0.18, speed 416.51 f/s\n",
            "Best mean reward updated -193.700 -> -192.890, model saved\n",
            "65441: done 331 games, mean reward -192.410, eps 0.18, speed 399.44 f/s\n",
            "Best mean reward updated -192.890 -> -192.410, model saved\n",
            "65563: done 332 games, mean reward -191.630, eps 0.18, speed 403.54 f/s\n",
            "Best mean reward updated -192.410 -> -191.630, model saved\n",
            "65715: done 333 games, mean reward -191.150, eps 0.18, speed 402.00 f/s\n",
            "Best mean reward updated -191.630 -> -191.150, model saved\n",
            "65841: done 334 games, mean reward -190.410, eps 0.18, speed 396.51 f/s\n",
            "Best mean reward updated -191.150 -> -190.410, model saved\n",
            "66033: done 335 games, mean reward -190.330, eps 0.17, speed 412.03 f/s\n",
            "Best mean reward updated -190.410 -> -190.330, model saved\n",
            "66175: done 336 games, mean reward -189.750, eps 0.17, speed 400.04 f/s\n",
            "Best mean reward updated -190.330 -> -189.750, model saved\n",
            "66353: done 337 games, mean reward -189.530, eps 0.17, speed 405.82 f/s\n",
            "Best mean reward updated -189.750 -> -189.530, model saved\n",
            "66534: done 338 games, mean reward -189.340, eps 0.17, speed 411.23 f/s\n",
            "Best mean reward updated -189.530 -> -189.340, model saved\n",
            "66648: done 339 games, mean reward -188.480, eps 0.17, speed 391.74 f/s\n",
            "Best mean reward updated -189.340 -> -188.480, model saved\n",
            "66765: done 340 games, mean reward -187.650, eps 0.17, speed 399.87 f/s\n",
            "Best mean reward updated -188.480 -> -187.650, model saved\n",
            "66949: done 341 games, mean reward -187.490, eps 0.16, speed 417.56 f/s\n",
            "Best mean reward updated -187.650 -> -187.490, model saved\n",
            "67146: done 342 games, mean reward -187.460, eps 0.16, speed 396.54 f/s\n",
            "Best mean reward updated -187.490 -> -187.460, model saved\n",
            "67333: done 343 games, mean reward -187.330, eps 0.16, speed 404.47 f/s\n",
            "Best mean reward updated -187.460 -> -187.330, model saved\n",
            "67461: done 344 games, mean reward -186.610, eps 0.16, speed 394.75 f/s\n",
            "Best mean reward updated -187.330 -> -186.610, model saved\n",
            "67651: done 345 games, mean reward -186.510, eps 0.15, speed 404.26 f/s\n",
            "Best mean reward updated -186.610 -> -186.510, model saved\n",
            "67837: done 346 games, mean reward -186.370, eps 0.15, speed 402.78 f/s\n",
            "Best mean reward updated -186.510 -> -186.370, model saved\n",
            "68030: done 347 games, mean reward -186.300, eps 0.15, speed 411.84 f/s\n",
            "Best mean reward updated -186.370 -> -186.300, model saved\n",
            "68230: done 348 games, mean reward -186.300, eps 0.15, speed 408.49 f/s\n",
            "68420: done 349 games, mean reward -186.200, eps 0.14, speed 420.18 f/s\n",
            "Best mean reward updated -186.300 -> -186.200, model saved\n",
            "68620: done 350 games, mean reward -186.200, eps 0.14, speed 401.44 f/s\n",
            "68766: done 351 games, mean reward -185.660, eps 0.14, speed 404.56 f/s\n",
            "Best mean reward updated -186.200 -> -185.660, model saved\n",
            "68966: done 352 games, mean reward -185.660, eps 0.14, speed 405.36 f/s\n",
            "69166: done 353 games, mean reward -185.660, eps 0.14, speed 395.97 f/s\n",
            "69312: done 354 games, mean reward -185.120, eps 0.13, speed 405.27 f/s\n",
            "Best mean reward updated -185.660 -> -185.120, model saved\n",
            "69512: done 355 games, mean reward -185.120, eps 0.13, speed 398.28 f/s\n",
            "69712: done 356 games, mean reward -185.120, eps 0.13, speed 411.84 f/s\n",
            "69912: done 357 games, mean reward -185.120, eps 0.13, speed 400.54 f/s\n",
            "70112: done 358 games, mean reward -185.120, eps 0.12, speed 403.66 f/s\n",
            "70312: done 359 games, mean reward -185.120, eps 0.12, speed 400.04 f/s\n",
            "70512: done 360 games, mean reward -185.120, eps 0.12, speed 412.40 f/s\n",
            "70639: done 361 games, mean reward -184.390, eps 0.12, speed 411.64 f/s\n",
            "Best mean reward updated -185.120 -> -184.390, model saved\n",
            "70839: done 362 games, mean reward -184.390, eps 0.11, speed 404.86 f/s\n",
            "71039: done 363 games, mean reward -184.390, eps 0.11, speed 401.30 f/s\n",
            "71220: done 364 games, mean reward -184.200, eps 0.11, speed 397.29 f/s\n",
            "Best mean reward updated -184.390 -> -184.200, model saved\n",
            "71391: done 365 games, mean reward -183.910, eps 0.11, speed 405.35 f/s\n",
            "Best mean reward updated -184.200 -> -183.910, model saved\n",
            "71570: done 366 games, mean reward -183.700, eps 0.11, speed 404.38 f/s\n",
            "Best mean reward updated -183.910 -> -183.700, model saved\n",
            "71746: done 367 games, mean reward -183.460, eps 0.10, speed 404.63 f/s\n",
            "Best mean reward updated -183.700 -> -183.460, model saved\n",
            "71931: done 368 games, mean reward -183.310, eps 0.10, speed 411.37 f/s\n",
            "Best mean reward updated -183.460 -> -183.310, model saved\n",
            "72118: done 369 games, mean reward -183.180, eps 0.10, speed 417.04 f/s\n",
            "Best mean reward updated -183.310 -> -183.180, model saved\n",
            "72318: done 370 games, mean reward -183.180, eps 0.10, speed 402.86 f/s\n",
            "72481: done 371 games, mean reward -182.810, eps 0.09, speed 409.99 f/s\n",
            "Best mean reward updated -183.180 -> -182.810, model saved\n",
            "72681: done 372 games, mean reward -182.810, eps 0.09, speed 404.85 f/s\n",
            "72845: done 373 games, mean reward -182.450, eps 0.09, speed 402.79 f/s\n",
            "Best mean reward updated -182.810 -> -182.450, model saved\n",
            "73045: done 374 games, mean reward -182.450, eps 0.09, speed 406.76 f/s\n",
            "73245: done 375 games, mean reward -182.450, eps 0.08, speed 402.16 f/s\n",
            "73417: done 376 games, mean reward -182.170, eps 0.08, speed 406.41 f/s\n",
            "Best mean reward updated -182.450 -> -182.170, model saved\n",
            "73617: done 377 games, mean reward -182.170, eps 0.08, speed 397.33 f/s\n",
            "73786: done 378 games, mean reward -181.860, eps 0.08, speed 409.56 f/s\n",
            "Best mean reward updated -182.170 -> -181.860, model saved\n",
            "73956: done 379 games, mean reward -181.560, eps 0.08, speed 396.70 f/s\n",
            "Best mean reward updated -181.860 -> -181.560, model saved\n",
            "74106: done 380 games, mean reward -181.060, eps 0.07, speed 398.42 f/s\n",
            "Best mean reward updated -181.560 -> -181.060, model saved\n",
            "74306: done 381 games, mean reward -181.060, eps 0.07, speed 406.31 f/s\n",
            "74422: done 382 games, mean reward -180.220, eps 0.07, speed 391.17 f/s\n",
            "Best mean reward updated -181.060 -> -180.220, model saved\n",
            "74596: done 383 games, mean reward -179.960, eps 0.07, speed 404.17 f/s\n",
            "Best mean reward updated -180.220 -> -179.960, model saved\n",
            "74796: done 384 games, mean reward -179.960, eps 0.07, speed 399.34 f/s\n",
            "74954: done 385 games, mean reward -179.540, eps 0.06, speed 415.55 f/s\n",
            "Best mean reward updated -179.960 -> -179.540, model saved\n",
            "75116: done 386 games, mean reward -179.160, eps 0.06, speed 401.94 f/s\n",
            "Best mean reward updated -179.540 -> -179.160, model saved\n",
            "75269: done 387 games, mean reward -178.690, eps 0.06, speed 398.18 f/s\n",
            "Best mean reward updated -179.160 -> -178.690, model saved\n",
            "75426: done 388 games, mean reward -178.260, eps 0.06, speed 392.07 f/s\n",
            "Best mean reward updated -178.690 -> -178.260, model saved\n",
            "75591: done 389 games, mean reward -177.910, eps 0.06, speed 388.48 f/s\n",
            "Best mean reward updated -178.260 -> -177.910, model saved\n",
            "75756: done 390 games, mean reward -177.560, eps 0.05, speed 397.67 f/s\n",
            "Best mean reward updated -177.910 -> -177.560, model saved\n",
            "75909: done 391 games, mean reward -177.090, eps 0.05, speed 398.49 f/s\n",
            "Best mean reward updated -177.560 -> -177.090, model saved\n",
            "76012: done 392 games, mean reward -176.120, eps 0.05, speed 381.71 f/s\n",
            "Best mean reward updated -177.090 -> -176.120, model saved\n",
            "76161: done 393 games, mean reward -175.610, eps 0.05, speed 412.14 f/s\n",
            "Best mean reward updated -176.120 -> -175.610, model saved\n",
            "76328: done 394 games, mean reward -175.280, eps 0.05, speed 405.30 f/s\n",
            "Best mean reward updated -175.610 -> -175.280, model saved\n",
            "76488: done 395 games, mean reward -174.880, eps 0.04, speed 389.23 f/s\n",
            "Best mean reward updated -175.280 -> -174.880, model saved\n",
            "76633: done 396 games, mean reward -174.330, eps 0.04, speed 403.04 f/s\n",
            "Best mean reward updated -174.880 -> -174.330, model saved\n",
            "76783: done 397 games, mean reward -173.830, eps 0.04, speed 402.07 f/s\n",
            "Best mean reward updated -174.330 -> -173.830, model saved\n",
            "76949: done 398 games, mean reward -173.490, eps 0.04, speed 398.29 f/s\n",
            "Best mean reward updated -173.830 -> -173.490, model saved\n",
            "77100: done 399 games, mean reward -173.000, eps 0.04, speed 401.43 f/s\n",
            "Best mean reward updated -173.490 -> -173.000, model saved\n",
            "77245: done 400 games, mean reward -172.450, eps 0.03, speed 395.20 f/s\n",
            "Best mean reward updated -173.000 -> -172.450, model saved\n",
            "77333: done 401 games, mean reward -171.330, eps 0.03, speed 395.23 f/s\n",
            "Best mean reward updated -172.450 -> -171.330, model saved\n",
            "77476: done 402 games, mean reward -170.760, eps 0.03, speed 393.18 f/s\n",
            "Best mean reward updated -171.330 -> -170.760, model saved\n",
            "77561: done 403 games, mean reward -169.610, eps 0.03, speed 404.97 f/s\n",
            "Best mean reward updated -170.760 -> -169.610, model saved\n",
            "77738: done 404 games, mean reward -169.380, eps 0.03, speed 402.40 f/s\n",
            "Best mean reward updated -169.610 -> -169.380, model saved\n",
            "77876: done 405 games, mean reward -168.760, eps 0.03, speed 397.82 f/s\n",
            "Best mean reward updated -169.380 -> -168.760, model saved\n",
            "78067: done 406 games, mean reward -168.900, eps 0.02, speed 403.53 f/s\n",
            "78154: done 407 games, mean reward -168.100, eps 0.02, speed 404.76 f/s\n",
            "Best mean reward updated -168.760 -> -168.100, model saved\n",
            "78294: done 408 games, mean reward -167.500, eps 0.02, speed 402.12 f/s\n",
            "Best mean reward updated -168.100 -> -167.500, model saved\n",
            "78436: done 409 games, mean reward -167.100, eps 0.02, speed 394.03 f/s\n",
            "Best mean reward updated -167.500 -> -167.100, model saved\n",
            "78579: done 410 games, mean reward -167.060, eps 0.02, speed 390.04 f/s\n",
            "Best mean reward updated -167.100 -> -167.060, model saved\n",
            "78670: done 411 games, mean reward -165.970, eps 0.02, speed 390.31 f/s\n",
            "Best mean reward updated -167.060 -> -165.970, model saved\n",
            "78811: done 412 games, mean reward -165.910, eps 0.02, speed 397.12 f/s\n",
            "Best mean reward updated -165.970 -> -165.910, model saved\n",
            "78904: done 413 games, mean reward -164.840, eps 0.02, speed 385.28 f/s\n",
            "Best mean reward updated -165.910 -> -164.840, model saved\n",
            "79055: done 414 games, mean reward -164.560, eps 0.02, speed 395.10 f/s\n",
            "Best mean reward updated -164.840 -> -164.560, model saved\n",
            "79195: done 415 games, mean reward -164.460, eps 0.02, speed 393.05 f/s\n",
            "Best mean reward updated -164.560 -> -164.460, model saved\n",
            "79334: done 416 games, mean reward -164.200, eps 0.02, speed 391.91 f/s\n",
            "Best mean reward updated -164.460 -> -164.200, model saved\n",
            "79425: done 417 games, mean reward -163.320, eps 0.02, speed 394.29 f/s\n",
            "Best mean reward updated -164.200 -> -163.320, model saved\n",
            "79572: done 418 games, mean reward -163.040, eps 0.02, speed 403.00 f/s\n",
            "Best mean reward updated -163.320 -> -163.040, model saved\n",
            "79712: done 419 games, mean reward -163.100, eps 0.02, speed 397.02 f/s\n",
            "79859: done 420 games, mean reward -162.740, eps 0.02, speed 402.86 f/s\n",
            "Best mean reward updated -163.040 -> -162.740, model saved\n",
            "79945: done 421 games, mean reward -161.690, eps 0.02, speed 381.38 f/s\n",
            "Best mean reward updated -162.740 -> -161.690, model saved\n",
            "80087: done 422 games, mean reward -161.510, eps 0.02, speed 391.95 f/s\n",
            "Best mean reward updated -161.690 -> -161.510, model saved\n",
            "80227: done 423 games, mean reward -161.370, eps 0.02, speed 400.59 f/s\n",
            "Best mean reward updated -161.510 -> -161.370, model saved\n",
            "80375: done 424 games, mean reward -160.850, eps 0.02, speed 398.51 f/s\n",
            "Best mean reward updated -161.370 -> -160.850, model saved\n",
            "80516: done 425 games, mean reward -160.260, eps 0.02, speed 382.97 f/s\n",
            "Best mean reward updated -160.850 -> -160.260, model saved\n",
            "80658: done 426 games, mean reward -159.720, eps 0.02, speed 393.65 f/s\n",
            "Best mean reward updated -160.260 -> -159.720, model saved\n",
            "80803: done 427 games, mean reward -159.290, eps 0.02, speed 403.00 f/s\n",
            "Best mean reward updated -159.720 -> -159.290, model saved\n",
            "81003: done 428 games, mean reward -160.040, eps 0.02, speed 400.90 f/s\n",
            "81148: done 429 games, mean reward -159.780, eps 0.02, speed 403.55 f/s\n",
            "81289: done 430 games, mean reward -160.000, eps 0.02, speed 394.76 f/s\n",
            "81428: done 431 games, mean reward -159.870, eps 0.02, speed 396.53 f/s\n",
            "81624: done 432 games, mean reward -160.610, eps 0.02, speed 401.78 f/s\n",
            "81765: done 433 games, mean reward -160.500, eps 0.02, speed 402.56 f/s\n",
            "81952: done 434 games, mean reward -161.110, eps 0.02, speed 401.48 f/s\n",
            "82043: done 435 games, mean reward -160.100, eps 0.02, speed 392.86 f/s\n",
            "82177: done 436 games, mean reward -160.020, eps 0.02, speed 392.14 f/s\n",
            "82318: done 437 games, mean reward -159.650, eps 0.02, speed 398.53 f/s\n",
            "82459: done 438 games, mean reward -159.250, eps 0.02, speed 392.82 f/s\n",
            "Best mean reward updated -159.290 -> -159.250, model saved\n",
            "82629: done 439 games, mean reward -159.810, eps 0.02, speed 399.67 f/s\n",
            "82778: done 440 games, mean reward -160.130, eps 0.02, speed 400.68 f/s\n",
            "82944: done 441 games, mean reward -159.950, eps 0.02, speed 406.40 f/s\n",
            "83106: done 442 games, mean reward -159.600, eps 0.02, speed 401.87 f/s\n",
            "83193: done 443 games, mean reward -158.600, eps 0.02, speed 392.24 f/s\n",
            "Best mean reward updated -159.250 -> -158.600, model saved\n",
            "83349: done 444 games, mean reward -158.880, eps 0.02, speed 381.84 f/s\n",
            "83452: done 445 games, mean reward -158.010, eps 0.02, speed 397.76 f/s\n",
            "Best mean reward updated -158.600 -> -158.010, model saved\n",
            "83543: done 446 games, mean reward -157.060, eps 0.02, speed 391.91 f/s\n",
            "Best mean reward updated -158.010 -> -157.060, model saved\n",
            "83714: done 447 games, mean reward -156.840, eps 0.02, speed 394.54 f/s\n",
            "Best mean reward updated -157.060 -> -156.840, model saved\n",
            "83803: done 448 games, mean reward -155.730, eps 0.02, speed 395.89 f/s\n",
            "Best mean reward updated -156.840 -> -155.730, model saved\n",
            "83981: done 449 games, mean reward -155.610, eps 0.02, speed 407.69 f/s\n",
            "Best mean reward updated -155.730 -> -155.610, model saved\n",
            "84067: done 450 games, mean reward -154.470, eps 0.02, speed 384.58 f/s\n",
            "Best mean reward updated -155.610 -> -154.470, model saved\n",
            "84151: done 451 games, mean reward -153.850, eps 0.02, speed 395.49 f/s\n",
            "Best mean reward updated -154.470 -> -153.850, model saved\n",
            "84325: done 452 games, mean reward -153.590, eps 0.02, speed 404.65 f/s\n",
            "Best mean reward updated -153.850 -> -153.590, model saved\n",
            "84498: done 453 games, mean reward -153.320, eps 0.02, speed 389.75 f/s\n",
            "Best mean reward updated -153.590 -> -153.320, model saved\n",
            "84618: done 454 games, mean reward -153.060, eps 0.02, speed 394.38 f/s\n",
            "Best mean reward updated -153.320 -> -153.060, model saved\n",
            "84718: done 455 games, mean reward -152.060, eps 0.02, speed 399.93 f/s\n",
            "Best mean reward updated -153.060 -> -152.060, model saved\n",
            "84882: done 456 games, mean reward -151.700, eps 0.02, speed 380.52 f/s\n",
            "Best mean reward updated -152.060 -> -151.700, model saved\n",
            "84974: done 457 games, mean reward -150.620, eps 0.02, speed 388.34 f/s\n",
            "Best mean reward updated -151.700 -> -150.620, model saved\n",
            "85093: done 458 games, mean reward -149.810, eps 0.02, speed 404.24 f/s\n",
            "Best mean reward updated -150.620 -> -149.810, model saved\n",
            "85215: done 459 games, mean reward -149.030, eps 0.02, speed 387.03 f/s\n",
            "Best mean reward updated -149.810 -> -149.030, model saved\n",
            "85338: done 460 games, mean reward -148.260, eps 0.02, speed 389.90 f/s\n",
            "Best mean reward updated -149.030 -> -148.260, model saved\n",
            "85459: done 461 games, mean reward -148.200, eps 0.02, speed 401.87 f/s\n",
            "Best mean reward updated -148.260 -> -148.200, model saved\n",
            "85582: done 462 games, mean reward -147.430, eps 0.02, speed 392.94 f/s\n",
            "Best mean reward updated -148.200 -> -147.430, model saved\n",
            "85706: done 463 games, mean reward -146.670, eps 0.02, speed 394.80 f/s\n",
            "Best mean reward updated -147.430 -> -146.670, model saved\n",
            "85833: done 464 games, mean reward -146.130, eps 0.02, speed 394.22 f/s\n",
            "Best mean reward updated -146.670 -> -146.130, model saved\n",
            "85955: done 465 games, mean reward -145.640, eps 0.02, speed 404.59 f/s\n",
            "Best mean reward updated -146.130 -> -145.640, model saved\n",
            "86155: done 466 games, mean reward -145.850, eps 0.02, speed 401.41 f/s\n",
            "86268: done 467 games, mean reward -145.220, eps 0.02, speed 409.52 f/s\n",
            "Best mean reward updated -145.640 -> -145.220, model saved\n",
            "86382: done 468 games, mean reward -144.510, eps 0.02, speed 397.72 f/s\n",
            "Best mean reward updated -145.220 -> -144.510, model saved\n",
            "86494: done 469 games, mean reward -143.760, eps 0.02, speed 386.75 f/s\n",
            "Best mean reward updated -144.510 -> -143.760, model saved\n",
            "86608: done 470 games, mean reward -142.900, eps 0.02, speed 396.90 f/s\n",
            "Best mean reward updated -143.760 -> -142.900, model saved\n",
            "86723: done 471 games, mean reward -142.420, eps 0.02, speed 397.86 f/s\n",
            "Best mean reward updated -142.900 -> -142.420, model saved\n",
            "86836: done 472 games, mean reward -141.550, eps 0.02, speed 400.07 f/s\n",
            "Best mean reward updated -142.420 -> -141.550, model saved\n",
            "86947: done 473 games, mean reward -141.020, eps 0.02, speed 387.44 f/s\n",
            "Best mean reward updated -141.550 -> -141.020, model saved\n",
            "87054: done 474 games, mean reward -140.090, eps 0.02, speed 391.89 f/s\n",
            "Best mean reward updated -141.020 -> -140.090, model saved\n",
            "87164: done 475 games, mean reward -139.190, eps 0.02, speed 393.31 f/s\n",
            "Best mean reward updated -140.090 -> -139.190, model saved\n",
            "87280: done 476 games, mean reward -138.630, eps 0.02, speed 391.45 f/s\n",
            "Best mean reward updated -139.190 -> -138.630, model saved\n",
            "87389: done 477 games, mean reward -137.720, eps 0.02, speed 386.84 f/s\n",
            "Best mean reward updated -138.630 -> -137.720, model saved\n",
            "87495: done 478 games, mean reward -137.090, eps 0.02, speed 392.82 f/s\n",
            "Best mean reward updated -137.720 -> -137.090, model saved\n",
            "87607: done 479 games, mean reward -136.510, eps 0.02, speed 401.28 f/s\n",
            "Best mean reward updated -137.090 -> -136.510, model saved\n",
            "87716: done 480 games, mean reward -136.100, eps 0.02, speed 389.78 f/s\n",
            "Best mean reward updated -136.510 -> -136.100, model saved\n",
            "87835: done 481 games, mean reward -135.290, eps 0.02, speed 394.83 f/s\n",
            "Best mean reward updated -136.100 -> -135.290, model saved\n",
            "87942: done 482 games, mean reward -135.200, eps 0.02, speed 392.95 f/s\n",
            "Best mean reward updated -135.290 -> -135.200, model saved\n",
            "88054: done 483 games, mean reward -134.580, eps 0.02, speed 397.31 f/s\n",
            "Best mean reward updated -135.200 -> -134.580, model saved\n",
            "88166: done 484 games, mean reward -133.700, eps 0.02, speed 383.24 f/s\n",
            "Best mean reward updated -134.580 -> -133.700, model saved\n",
            "88274: done 485 games, mean reward -133.200, eps 0.02, speed 397.82 f/s\n",
            "Best mean reward updated -133.700 -> -133.200, model saved\n",
            "88383: done 486 games, mean reward -132.670, eps 0.02, speed 387.27 f/s\n",
            "Best mean reward updated -133.200 -> -132.670, model saved\n",
            "88496: done 487 games, mean reward -132.270, eps 0.02, speed 382.08 f/s\n",
            "Best mean reward updated -132.670 -> -132.270, model saved\n",
            "88614: done 488 games, mean reward -131.880, eps 0.02, speed 398.25 f/s\n",
            "Best mean reward updated -132.270 -> -131.880, model saved\n",
            "88749: done 489 games, mean reward -131.580, eps 0.02, speed 392.79 f/s\n",
            "Best mean reward updated -131.880 -> -131.580, model saved\n",
            "88857: done 490 games, mean reward -131.010, eps 0.02, speed 399.56 f/s\n",
            "Best mean reward updated -131.580 -> -131.010, model saved\n",
            "88963: done 491 games, mean reward -130.540, eps 0.02, speed 386.56 f/s\n",
            "Best mean reward updated -131.010 -> -130.540, model saved\n",
            "89067: done 492 games, mean reward -130.550, eps 0.02, speed 389.50 f/s\n",
            "89173: done 493 games, mean reward -130.120, eps 0.02, speed 398.37 f/s\n",
            "Best mean reward updated -130.540 -> -130.120, model saved\n",
            "89279: done 494 games, mean reward -129.510, eps 0.02, speed 376.01 f/s\n",
            "Best mean reward updated -130.120 -> -129.510, model saved\n",
            "89382: done 495 games, mean reward -128.940, eps 0.02, speed 395.67 f/s\n",
            "Best mean reward updated -129.510 -> -128.940, model saved\n",
            "89489: done 496 games, mean reward -128.560, eps 0.02, speed 397.77 f/s\n",
            "Best mean reward updated -128.940 -> -128.560, model saved\n",
            "89595: done 497 games, mean reward -128.120, eps 0.02, speed 397.12 f/s\n",
            "Best mean reward updated -128.560 -> -128.120, model saved\n",
            "89703: done 498 games, mean reward -127.540, eps 0.02, speed 387.05 f/s\n",
            "Best mean reward updated -128.120 -> -127.540, model saved\n",
            "89897: done 499 games, mean reward -127.970, eps 0.02, speed 400.24 f/s\n",
            "90001: done 500 games, mean reward -127.560, eps 0.02, speed 391.24 f/s\n",
            "90145: done 501 games, mean reward -128.120, eps 0.02, speed 387.99 f/s\n",
            "90302: done 502 games, mean reward -128.260, eps 0.02, speed 396.78 f/s\n",
            "90406: done 503 games, mean reward -128.450, eps 0.02, speed 394.77 f/s\n",
            "90512: done 504 games, mean reward -127.740, eps 0.02, speed 384.66 f/s\n",
            "90696: done 505 games, mean reward -128.200, eps 0.02, speed 398.65 f/s\n",
            "90802: done 506 games, mean reward -127.350, eps 0.02, speed 401.89 f/s\n",
            "Best mean reward updated -127.540 -> -127.350, model saved\n",
            "90893: done 507 games, mean reward -127.390, eps 0.02, speed 367.36 f/s\n",
            "90998: done 508 games, mean reward -127.040, eps 0.02, speed 392.77 f/s\n",
            "Best mean reward updated -127.350 -> -127.040, model saved\n",
            "91162: done 509 games, mean reward -127.260, eps 0.02, speed 405.66 f/s\n",
            "91320: done 510 games, mean reward -127.410, eps 0.02, speed 395.26 f/s\n",
            "91430: done 511 games, mean reward -127.600, eps 0.02, speed 404.68 f/s\n",
            "91529: done 512 games, mean reward -127.180, eps 0.02, speed 404.49 f/s\n",
            "91638: done 513 games, mean reward -127.340, eps 0.02, speed 407.95 f/s\n",
            "91742: done 514 games, mean reward -126.870, eps 0.02, speed 388.02 f/s\n",
            "Best mean reward updated -127.040 -> -126.870, model saved\n",
            "91913: done 515 games, mean reward -127.180, eps 0.02, speed 401.80 f/s\n",
            "92096: done 516 games, mean reward -127.620, eps 0.02, speed 395.66 f/s\n",
            "92243: done 517 games, mean reward -128.180, eps 0.02, speed 399.38 f/s\n",
            "92348: done 518 games, mean reward -127.760, eps 0.02, speed 404.19 f/s\n",
            "92436: done 519 games, mean reward -127.240, eps 0.02, speed 410.18 f/s\n",
            "92541: done 520 games, mean reward -126.820, eps 0.02, speed 386.42 f/s\n",
            "Best mean reward updated -126.870 -> -126.820, model saved\n",
            "92681: done 521 games, mean reward -127.360, eps 0.02, speed 404.31 f/s\n",
            "92787: done 522 games, mean reward -127.000, eps 0.02, speed 399.62 f/s\n",
            "92924: done 523 games, mean reward -126.970, eps 0.02, speed 393.96 f/s\n",
            "93062: done 524 games, mean reward -126.870, eps 0.02, speed 405.32 f/s\n",
            "93171: done 525 games, mean reward -126.550, eps 0.02, speed 398.59 f/s\n",
            "Best mean reward updated -126.820 -> -126.550, model saved\n",
            "93343: done 526 games, mean reward -126.850, eps 0.02, speed 391.86 f/s\n",
            "93448: done 527 games, mean reward -126.450, eps 0.02, speed 397.55 f/s\n",
            "Best mean reward updated -126.550 -> -126.450, model saved\n",
            "93553: done 528 games, mean reward -125.500, eps 0.02, speed 394.55 f/s\n",
            "Best mean reward updated -126.450 -> -125.500, model saved\n",
            "93726: done 529 games, mean reward -125.780, eps 0.02, speed 398.71 f/s\n",
            "93832: done 530 games, mean reward -125.430, eps 0.02, speed 408.70 f/s\n",
            "Best mean reward updated -125.500 -> -125.430, model saved\n",
            "93968: done 531 games, mean reward -125.400, eps 0.02, speed 409.31 f/s\n",
            "Best mean reward updated -125.430 -> -125.400, model saved\n",
            "94073: done 532 games, mean reward -124.490, eps 0.02, speed 403.09 f/s\n",
            "Best mean reward updated -125.400 -> -124.490, model saved\n",
            "94211: done 533 games, mean reward -124.460, eps 0.02, speed 391.46 f/s\n",
            "Best mean reward updated -124.490 -> -124.460, model saved\n",
            "94321: done 534 games, mean reward -123.690, eps 0.02, speed 398.73 f/s\n",
            "Best mean reward updated -124.460 -> -123.690, model saved\n",
            "94407: done 535 games, mean reward -123.640, eps 0.02, speed 396.46 f/s\n",
            "Best mean reward updated -123.690 -> -123.640, model saved\n",
            "94556: done 536 games, mean reward -123.790, eps 0.02, speed 390.55 f/s\n",
            "94663: done 537 games, mean reward -123.450, eps 0.02, speed 392.83 f/s\n",
            "Best mean reward updated -123.640 -> -123.450, model saved\n",
            "94801: done 538 games, mean reward -123.420, eps 0.02, speed 399.18 f/s\n",
            "Best mean reward updated -123.450 -> -123.420, model saved\n",
            "94944: done 539 games, mean reward -123.150, eps 0.02, speed 394.36 f/s\n",
            "Best mean reward updated -123.420 -> -123.150, model saved\n",
            "95050: done 540 games, mean reward -122.720, eps 0.02, speed 396.78 f/s\n",
            "Best mean reward updated -123.150 -> -122.720, model saved\n",
            "95197: done 541 games, mean reward -122.530, eps 0.02, speed 397.59 f/s\n",
            "Best mean reward updated -122.720 -> -122.530, model saved\n",
            "95306: done 542 games, mean reward -122.000, eps 0.02, speed 391.62 f/s\n",
            "Best mean reward updated -122.530 -> -122.000, model saved\n",
            "95393: done 543 games, mean reward -122.000, eps 0.02, speed 392.96 f/s\n",
            "95562: done 544 games, mean reward -122.130, eps 0.02, speed 397.71 f/s\n",
            "95709: done 545 games, mean reward -122.570, eps 0.02, speed 403.98 f/s\n",
            "95853: done 546 games, mean reward -123.100, eps 0.02, speed 399.38 f/s\n",
            "96036: done 547 games, mean reward -123.220, eps 0.02, speed 407.53 f/s\n",
            "96180: done 548 games, mean reward -123.770, eps 0.02, speed 392.92 f/s\n",
            "96293: done 549 games, mean reward -123.120, eps 0.02, speed 397.24 f/s\n",
            "96439: done 550 games, mean reward -123.720, eps 0.02, speed 407.80 f/s\n",
            "96535: done 551 games, mean reward -123.840, eps 0.02, speed 390.65 f/s\n",
            "96716: done 552 games, mean reward -123.910, eps 0.02, speed 395.92 f/s\n",
            "96861: done 553 games, mean reward -123.630, eps 0.02, speed 403.86 f/s\n",
            "97004: done 554 games, mean reward -123.860, eps 0.02, speed 398.03 f/s\n",
            "97094: done 555 games, mean reward -123.760, eps 0.02, speed 400.69 f/s\n",
            "97181: done 556 games, mean reward -122.990, eps 0.02, speed 408.46 f/s\n",
            "97267: done 557 games, mean reward -122.930, eps 0.02, speed 394.39 f/s\n",
            "97417: done 558 games, mean reward -123.240, eps 0.02, speed 403.43 f/s\n",
            "97502: done 559 games, mean reward -122.870, eps 0.02, speed 380.46 f/s\n",
            "97652: done 560 games, mean reward -123.140, eps 0.02, speed 409.24 f/s\n",
            "97739: done 561 games, mean reward -122.800, eps 0.02, speed 398.42 f/s\n",
            "97885: done 562 games, mean reward -123.030, eps 0.02, speed 391.75 f/s\n",
            "98054: done 563 games, mean reward -123.480, eps 0.02, speed 399.28 f/s\n",
            "98209: done 564 games, mean reward -123.760, eps 0.02, speed 398.68 f/s\n",
            "98374: done 565 games, mean reward -124.190, eps 0.02, speed 411.14 f/s\n",
            "98477: done 566 games, mean reward -123.220, eps 0.02, speed 394.86 f/s\n",
            "98566: done 567 games, mean reward -122.980, eps 0.02, speed 402.69 f/s\n",
            "98695: done 568 games, mean reward -123.130, eps 0.02, speed 394.01 f/s\n",
            "98848: done 569 games, mean reward -123.540, eps 0.02, speed 403.70 f/s\n",
            "98935: done 570 games, mean reward -123.270, eps 0.02, speed 379.59 f/s\n",
            "99022: done 571 games, mean reward -122.990, eps 0.02, speed 385.17 f/s\n",
            "99139: done 572 games, mean reward -123.030, eps 0.02, speed 401.24 f/s\n",
            "99303: done 573 games, mean reward -123.560, eps 0.02, speed 399.89 f/s\n",
            "99387: done 574 games, mean reward -123.330, eps 0.02, speed 376.26 f/s\n",
            "99546: done 575 games, mean reward -123.820, eps 0.02, speed 389.12 f/s\n",
            "99707: done 576 games, mean reward -124.270, eps 0.02, speed 396.21 f/s\n",
            "99818: done 577 games, mean reward -124.290, eps 0.02, speed 385.38 f/s\n",
            "99930: done 578 games, mean reward -124.350, eps 0.02, speed 409.96 f/s\n",
            "100098: done 579 games, mean reward -124.910, eps 0.02, speed 404.47 f/s\n",
            "100240: done 580 games, mean reward -125.240, eps 0.02, speed 394.22 f/s\n",
            "100394: done 581 games, mean reward -125.590, eps 0.02, speed 402.29 f/s\n",
            "100551: done 582 games, mean reward -126.090, eps 0.02, speed 398.45 f/s\n",
            "100697: done 583 games, mean reward -126.430, eps 0.02, speed 390.50 f/s\n",
            "100847: done 584 games, mean reward -126.810, eps 0.02, speed 396.08 f/s\n",
            "101047: done 585 games, mean reward -127.730, eps 0.02, speed 399.00 f/s\n",
            "101193: done 586 games, mean reward -128.100, eps 0.02, speed 409.87 f/s\n",
            "101342: done 587 games, mean reward -128.460, eps 0.02, speed 402.77 f/s\n",
            "101495: done 588 games, mean reward -128.810, eps 0.02, speed 397.95 f/s\n",
            "101645: done 589 games, mean reward -128.960, eps 0.02, speed 388.44 f/s\n",
            "101788: done 590 games, mean reward -129.310, eps 0.02, speed 406.72 f/s\n",
            "101872: done 591 games, mean reward -129.090, eps 0.02, speed 383.80 f/s\n",
            "102037: done 592 games, mean reward -129.700, eps 0.02, speed 405.38 f/s\n",
            "102180: done 593 games, mean reward -130.070, eps 0.02, speed 403.80 f/s\n",
            "102330: done 594 games, mean reward -130.510, eps 0.02, speed 400.92 f/s\n",
            "102419: done 595 games, mean reward -130.370, eps 0.02, speed 399.49 f/s\n",
            "102516: done 596 games, mean reward -130.270, eps 0.02, speed 397.68 f/s\n",
            "102664: done 597 games, mean reward -130.690, eps 0.02, speed 392.60 f/s\n",
            "102813: done 598 games, mean reward -131.100, eps 0.02, speed 406.98 f/s\n",
            "102897: done 599 games, mean reward -130.000, eps 0.02, speed 393.81 f/s\n",
            "102993: done 600 games, mean reward -129.920, eps 0.02, speed 402.64 f/s\n",
            "103193: done 601 games, mean reward -130.480, eps 0.02, speed 401.31 f/s\n",
            "103343: done 602 games, mean reward -130.410, eps 0.02, speed 403.41 f/s\n",
            "103434: done 603 games, mean reward -130.280, eps 0.02, speed 392.02 f/s\n",
            "103627: done 604 games, mean reward -131.150, eps 0.02, speed 401.16 f/s\n",
            "103777: done 605 games, mean reward -130.810, eps 0.02, speed 402.83 f/s\n",
            "103911: done 606 games, mean reward -131.090, eps 0.02, speed 391.53 f/s\n",
            "104060: done 607 games, mean reward -131.670, eps 0.02, speed 402.67 f/s\n",
            "104146: done 608 games, mean reward -131.480, eps 0.02, speed 403.65 f/s\n",
            "104346: done 609 games, mean reward -131.840, eps 0.02, speed 395.72 f/s\n",
            "104503: done 610 games, mean reward -131.830, eps 0.02, speed 405.56 f/s\n",
            "104651: done 611 games, mean reward -132.210, eps 0.02, speed 405.95 f/s\n",
            "104739: done 612 games, mean reward -132.100, eps 0.02, speed 391.71 f/s\n",
            "104889: done 613 games, mean reward -132.510, eps 0.02, speed 403.40 f/s\n",
            "104973: done 614 games, mean reward -132.310, eps 0.02, speed 413.13 f/s\n",
            "105112: done 615 games, mean reward -131.990, eps 0.02, speed 392.29 f/s\n",
            "105215: done 616 games, mean reward -131.190, eps 0.02, speed 400.35 f/s\n",
            "105312: done 617 games, mean reward -130.690, eps 0.02, speed 400.42 f/s\n",
            "105506: done 618 games, mean reward -131.580, eps 0.02, speed 399.74 f/s\n",
            "105650: done 619 games, mean reward -132.140, eps 0.02, speed 408.43 f/s\n",
            "105749: done 620 games, mean reward -132.080, eps 0.02, speed 394.62 f/s\n",
            "105883: done 621 games, mean reward -132.020, eps 0.02, speed 402.42 f/s\n",
            "106025: done 622 games, mean reward -132.380, eps 0.02, speed 398.63 f/s\n",
            "106165: done 623 games, mean reward -132.410, eps 0.02, speed 406.88 f/s\n",
            "106314: done 624 games, mean reward -132.520, eps 0.02, speed 394.69 f/s\n",
            "106458: done 625 games, mean reward -132.870, eps 0.02, speed 396.94 f/s\n",
            "106614: done 626 games, mean reward -132.710, eps 0.02, speed 408.39 f/s\n",
            "106756: done 627 games, mean reward -133.080, eps 0.02, speed 394.54 f/s\n",
            "106848: done 628 games, mean reward -132.950, eps 0.02, speed 394.26 f/s\n",
            "106989: done 629 games, mean reward -132.630, eps 0.02, speed 401.15 f/s\n",
            "107129: done 630 games, mean reward -132.970, eps 0.02, speed 400.84 f/s\n",
            "107216: done 631 games, mean reward -132.480, eps 0.02, speed 400.07 f/s\n",
            "107363: done 632 games, mean reward -132.900, eps 0.02, speed 402.25 f/s\n",
            "107558: done 633 games, mean reward -133.470, eps 0.02, speed 397.43 f/s\n",
            "107702: done 634 games, mean reward -133.810, eps 0.02, speed 385.70 f/s\n",
            "107879: done 635 games, mean reward -134.720, eps 0.02, speed 400.72 f/s\n",
            "108034: done 636 games, mean reward -134.780, eps 0.02, speed 391.43 f/s\n",
            "108183: done 637 games, mean reward -135.200, eps 0.02, speed 408.66 f/s\n",
            "108268: done 638 games, mean reward -134.670, eps 0.02, speed 398.45 f/s\n",
            "108468: done 639 games, mean reward -135.240, eps 0.02, speed 403.62 f/s\n",
            "108668: done 640 games, mean reward -136.180, eps 0.02, speed 402.95 f/s\n",
            "108815: done 641 games, mean reward -136.180, eps 0.02, speed 385.02 f/s\n",
            "108955: done 642 games, mean reward -136.490, eps 0.02, speed 405.49 f/s\n",
            "109048: done 643 games, mean reward -136.550, eps 0.02, speed 397.97 f/s\n",
            "109193: done 644 games, mean reward -136.310, eps 0.02, speed 399.70 f/s\n",
            "109299: done 645 games, mean reward -135.900, eps 0.02, speed 396.31 f/s\n",
            "109445: done 646 games, mean reward -135.920, eps 0.02, speed 404.91 f/s\n",
            "109630: done 647 games, mean reward -135.940, eps 0.02, speed 400.61 f/s\n",
            "109719: done 648 games, mean reward -135.390, eps 0.02, speed 395.55 f/s\n",
            "109805: done 649 games, mean reward -135.120, eps 0.02, speed 406.17 f/s\n",
            "109895: done 650 games, mean reward -134.560, eps 0.02, speed 382.74 f/s\n",
            "109994: done 651 games, mean reward -134.590, eps 0.02, speed 393.68 f/s\n",
            "110141: done 652 games, mean reward -134.250, eps 0.02, speed 407.03 f/s\n",
            "110227: done 653 games, mean reward -133.660, eps 0.02, speed 392.72 f/s\n",
            "110314: done 654 games, mean reward -133.100, eps 0.02, speed 402.10 f/s\n",
            "110421: done 655 games, mean reward -133.270, eps 0.02, speed 380.96 f/s\n",
            "110568: done 656 games, mean reward -133.870, eps 0.02, speed 397.76 f/s\n",
            "110712: done 657 games, mean reward -134.450, eps 0.02, speed 401.13 f/s\n",
            "110861: done 658 games, mean reward -134.440, eps 0.02, speed 391.54 f/s\n",
            "110947: done 659 games, mean reward -134.450, eps 0.02, speed 392.10 f/s\n",
            "111048: done 660 games, mean reward -133.960, eps 0.02, speed 390.13 f/s\n",
            "111132: done 661 games, mean reward -133.930, eps 0.02, speed 411.74 f/s\n",
            "111272: done 662 games, mean reward -133.870, eps 0.02, speed 381.08 f/s\n",
            "111357: done 663 games, mean reward -133.030, eps 0.02, speed 399.14 f/s\n",
            "111503: done 664 games, mean reward -132.940, eps 0.02, speed 405.55 f/s\n",
            "111586: done 665 games, mean reward -132.120, eps 0.02, speed 353.75 f/s\n",
            "111729: done 666 games, mean reward -132.520, eps 0.02, speed 405.21 f/s\n",
            "111872: done 667 games, mean reward -133.060, eps 0.02, speed 403.79 f/s\n",
            "112020: done 668 games, mean reward -133.250, eps 0.02, speed 402.93 f/s\n",
            "112163: done 669 games, mean reward -133.150, eps 0.02, speed 403.53 f/s\n",
            "112306: done 670 games, mean reward -133.710, eps 0.02, speed 405.07 f/s\n",
            "112452: done 671 games, mean reward -134.300, eps 0.02, speed 391.50 f/s\n",
            "112595: done 672 games, mean reward -134.560, eps 0.02, speed 405.89 f/s\n",
            "112712: done 673 games, mean reward -134.090, eps 0.02, speed 405.89 f/s\n",
            "112860: done 674 games, mean reward -134.730, eps 0.02, speed 391.32 f/s\n",
            "112974: done 675 games, mean reward -134.280, eps 0.02, speed 403.71 f/s\n",
            "113115: done 676 games, mean reward -134.080, eps 0.02, speed 411.40 f/s\n",
            "113199: done 677 games, mean reward -133.810, eps 0.02, speed 376.74 f/s\n",
            "113303: done 678 games, mean reward -133.730, eps 0.02, speed 410.23 f/s\n",
            "113503: done 679 games, mean reward -134.050, eps 0.02, speed 403.80 f/s\n",
            "113643: done 680 games, mean reward -134.030, eps 0.02, speed 390.03 f/s\n",
            "113728: done 681 games, mean reward -133.340, eps 0.02, speed 407.96 f/s\n",
            "113928: done 682 games, mean reward -133.770, eps 0.02, speed 407.96 f/s\n",
            "114074: done 683 games, mean reward -133.770, eps 0.02, speed 386.03 f/s\n",
            "114157: done 684 games, mean reward -133.100, eps 0.02, speed 398.22 f/s\n",
            "114305: done 685 games, mean reward -132.580, eps 0.02, speed 406.77 f/s\n",
            "114393: done 686 games, mean reward -132.000, eps 0.02, speed 391.68 f/s\n",
            "114532: done 687 games, mean reward -131.900, eps 0.02, speed 397.00 f/s\n",
            "114621: done 688 games, mean reward -131.260, eps 0.02, speed 400.42 f/s\n",
            "114761: done 689 games, mean reward -131.160, eps 0.02, speed 407.91 f/s\n",
            "114898: done 690 games, mean reward -131.100, eps 0.02, speed 391.51 f/s\n",
            "115037: done 691 games, mean reward -131.650, eps 0.02, speed 406.52 f/s\n",
            "115176: done 692 games, mean reward -131.390, eps 0.02, speed 405.41 f/s\n",
            "115262: done 693 games, mean reward -130.820, eps 0.02, speed 384.78 f/s\n",
            "115407: done 694 games, mean reward -130.770, eps 0.02, speed 408.92 f/s\n",
            "115547: done 695 games, mean reward -131.280, eps 0.02, speed 408.78 f/s\n",
            "115689: done 696 games, mean reward -131.730, eps 0.02, speed 393.85 f/s\n",
            "115780: done 697 games, mean reward -131.160, eps 0.02, speed 398.62 f/s\n",
            "115922: done 698 games, mean reward -131.090, eps 0.02, speed 408.71 f/s\n",
            "116061: done 699 games, mean reward -131.640, eps 0.02, speed 390.88 f/s\n",
            "116202: done 700 games, mean reward -132.090, eps 0.02, speed 403.45 f/s\n",
            "116311: done 701 games, mean reward -131.180, eps 0.02, speed 403.29 f/s\n",
            "116478: done 702 games, mean reward -131.350, eps 0.02, speed 395.74 f/s\n",
            "116622: done 703 games, mean reward -131.880, eps 0.02, speed 405.08 f/s\n",
            "116764: done 704 games, mean reward -131.370, eps 0.02, speed 410.95 f/s\n",
            "116907: done 705 games, mean reward -131.300, eps 0.02, speed 389.53 f/s\n",
            "116993: done 706 games, mean reward -130.820, eps 0.02, speed 378.71 f/s\n",
            "117141: done 707 games, mean reward -130.810, eps 0.02, speed 401.91 f/s\n",
            "117225: done 708 games, mean reward -130.790, eps 0.02, speed 410.93 f/s\n",
            "117369: done 709 games, mean reward -130.230, eps 0.02, speed 389.15 f/s\n",
            "117488: done 710 games, mean reward -129.850, eps 0.02, speed 393.33 f/s\n",
            "117575: done 711 games, mean reward -129.240, eps 0.02, speed 398.07 f/s\n",
            "117715: done 712 games, mean reward -129.760, eps 0.02, speed 390.69 f/s\n",
            "117858: done 713 games, mean reward -129.690, eps 0.02, speed 402.30 f/s\n",
            "117971: done 714 games, mean reward -129.980, eps 0.02, speed 396.52 f/s\n",
            "118112: done 715 games, mean reward -130.000, eps 0.02, speed 396.14 f/s\n",
            "118260: done 716 games, mean reward -130.450, eps 0.02, speed 389.75 f/s\n",
            "118345: done 717 games, mean reward -130.330, eps 0.02, speed 396.55 f/s\n",
            "118433: done 718 games, mean reward -129.270, eps 0.02, speed 399.05 f/s\n",
            "118575: done 719 games, mean reward -129.250, eps 0.02, speed 392.65 f/s\n",
            "118715: done 720 games, mean reward -129.660, eps 0.02, speed 408.46 f/s\n",
            "118859: done 721 games, mean reward -129.760, eps 0.02, speed 392.09 f/s\n",
            "118945: done 722 games, mean reward -129.200, eps 0.02, speed 384.81 f/s\n",
            "119099: done 723 games, mean reward -129.340, eps 0.02, speed 396.03 f/s\n",
            "119255: done 724 games, mean reward -129.410, eps 0.02, speed 396.86 f/s\n",
            "119343: done 725 games, mean reward -128.850, eps 0.02, speed 391.41 f/s\n",
            "119524: done 726 games, mean reward -129.100, eps 0.02, speed 368.62 f/s\n",
            "119611: done 727 games, mean reward -128.550, eps 0.02, speed 381.14 f/s\n",
            "119752: done 728 games, mean reward -129.040, eps 0.02, speed 406.17 f/s\n",
            "119894: done 729 games, mean reward -129.050, eps 0.02, speed 412.81 f/s\n",
            "119978: done 730 games, mean reward -128.490, eps 0.02, speed 404.05 f/s\n",
            "120121: done 731 games, mean reward -129.050, eps 0.02, speed 389.44 f/s\n",
            "120268: done 732 games, mean reward -129.050, eps 0.02, speed 398.79 f/s\n",
            "120409: done 733 games, mean reward -128.510, eps 0.02, speed 394.43 f/s\n",
            "120497: done 734 games, mean reward -127.950, eps 0.02, speed 384.97 f/s\n",
            "120639: done 735 games, mean reward -127.600, eps 0.02, speed 411.62 f/s\n",
            "120782: done 736 games, mean reward -127.480, eps 0.02, speed 402.14 f/s\n",
            "120928: done 737 games, mean reward -127.450, eps 0.02, speed 386.95 f/s\n",
            "121072: done 738 games, mean reward -128.040, eps 0.02, speed 400.18 f/s\n",
            "121216: done 739 games, mean reward -127.480, eps 0.02, speed 397.63 f/s\n",
            "121359: done 740 games, mean reward -126.910, eps 0.02, speed 390.76 f/s\n",
            "121507: done 741 games, mean reward -126.920, eps 0.02, speed 400.90 f/s\n",
            "121654: done 742 games, mean reward -126.990, eps 0.02, speed 393.93 f/s\n",
            "121797: done 743 games, mean reward -127.490, eps 0.02, speed 386.41 f/s\n",
            "121977: done 744 games, mean reward -127.840, eps 0.02, speed 401.63 f/s\n",
            "122177: done 745 games, mean reward -128.780, eps 0.02, speed 404.82 f/s\n",
            "122323: done 746 games, mean reward -128.780, eps 0.02, speed 387.09 f/s\n",
            "122417: done 747 games, mean reward -127.870, eps 0.02, speed 393.35 f/s\n",
            "122560: done 748 games, mean reward -128.410, eps 0.02, speed 390.79 f/s\n",
            "122703: done 749 games, mean reward -128.980, eps 0.02, speed 376.16 f/s\n",
            "122845: done 750 games, mean reward -129.500, eps 0.02, speed 347.43 f/s\n",
            "122938: done 751 games, mean reward -129.440, eps 0.02, speed 296.72 f/s\n",
            "123042: done 752 games, mean reward -129.010, eps 0.02, speed 389.17 f/s\n",
            "123185: done 753 games, mean reward -129.580, eps 0.02, speed 399.61 f/s\n",
            "123333: done 754 games, mean reward -130.190, eps 0.02, speed 390.78 f/s\n",
            "123477: done 755 games, mean reward -130.560, eps 0.02, speed 402.70 f/s\n",
            "123567: done 756 games, mean reward -129.990, eps 0.02, speed 373.09 f/s\n",
            "123708: done 757 games, mean reward -129.960, eps 0.02, speed 380.91 f/s\n",
            "123800: done 758 games, mean reward -129.390, eps 0.02, speed 402.52 f/s\n",
            "123944: done 759 games, mean reward -129.970, eps 0.02, speed 400.85 f/s\n",
            "124089: done 760 games, mean reward -130.410, eps 0.02, speed 398.34 f/s\n",
            "124247: done 761 games, mean reward -131.150, eps 0.02, speed 386.44 f/s\n",
            "124420: done 762 games, mean reward -131.480, eps 0.02, speed 386.75 f/s\n",
            "124564: done 763 games, mean reward -132.070, eps 0.02, speed 395.84 f/s\n",
            "124666: done 764 games, mean reward -131.630, eps 0.02, speed 400.74 f/s\n",
            "124759: done 765 games, mean reward -131.730, eps 0.02, speed 403.77 f/s\n",
            "124904: done 766 games, mean reward -131.750, eps 0.02, speed 394.89 f/s\n",
            "125007: done 767 games, mean reward -131.350, eps 0.02, speed 402.14 f/s\n",
            "125105: done 768 games, mean reward -130.850, eps 0.02, speed 398.03 f/s\n",
            "125261: done 769 games, mean reward -130.980, eps 0.02, speed 387.75 f/s\n",
            "125410: done 770 games, mean reward -131.040, eps 0.02, speed 407.41 f/s\n",
            "125507: done 771 games, mean reward -130.550, eps 0.02, speed 393.00 f/s\n",
            "125596: done 772 games, mean reward -130.010, eps 0.02, speed 394.24 f/s\n",
            "125785: done 773 games, mean reward -130.730, eps 0.02, speed 397.20 f/s\n",
            "125930: done 774 games, mean reward -130.700, eps 0.02, speed 406.68 f/s\n",
            "126119: done 775 games, mean reward -131.450, eps 0.02, speed 399.10 f/s\n",
            "126257: done 776 games, mean reward -131.420, eps 0.02, speed 400.19 f/s\n",
            "126406: done 777 games, mean reward -132.070, eps 0.02, speed 390.19 f/s\n",
            "126552: done 778 games, mean reward -132.490, eps 0.02, speed 400.37 f/s\n",
            "126701: done 779 games, mean reward -131.980, eps 0.02, speed 407.92 f/s\n",
            "126784: done 780 games, mean reward -131.410, eps 0.02, speed 393.87 f/s\n",
            "126984: done 781 games, mean reward -132.560, eps 0.02, speed 400.21 f/s\n",
            "127126: done 782 games, mean reward -131.980, eps 0.02, speed 404.30 f/s\n",
            "127271: done 783 games, mean reward -131.970, eps 0.02, speed 392.40 f/s\n",
            "127426: done 784 games, mean reward -132.690, eps 0.02, speed 402.21 f/s\n",
            "127517: done 785 games, mean reward -132.120, eps 0.02, speed 385.05 f/s\n",
            "127654: done 786 games, mean reward -132.610, eps 0.02, speed 392.33 f/s\n",
            "127747: done 787 games, mean reward -132.150, eps 0.02, speed 395.90 f/s\n",
            "127835: done 788 games, mean reward -132.140, eps 0.02, speed 396.39 f/s\n",
            "127923: done 789 games, mean reward -131.620, eps 0.02, speed 406.27 f/s\n",
            "128068: done 790 games, mean reward -131.700, eps 0.02, speed 396.84 f/s\n",
            "128187: done 791 games, mean reward -131.500, eps 0.02, speed 402.49 f/s\n",
            "128293: done 792 games, mean reward -131.170, eps 0.02, speed 390.89 f/s\n",
            "128402: done 793 games, mean reward -131.400, eps 0.02, speed 408.55 f/s\n",
            "128492: done 794 games, mean reward -130.850, eps 0.02, speed 396.59 f/s\n",
            "128576: done 795 games, mean reward -130.290, eps 0.02, speed 406.26 f/s\n",
            "128659: done 796 games, mean reward -129.700, eps 0.02, speed 393.02 f/s\n",
            "128744: done 797 games, mean reward -129.640, eps 0.02, speed 394.91 f/s\n",
            "128827: done 798 games, mean reward -129.050, eps 0.02, speed 390.23 f/s\n",
            "128935: done 799 games, mean reward -128.740, eps 0.02, speed 394.70 f/s\n",
            "129046: done 800 games, mean reward -128.440, eps 0.02, speed 399.10 f/s\n",
            "129135: done 801 games, mean reward -128.240, eps 0.02, speed 400.94 f/s\n",
            "129315: done 802 games, mean reward -128.370, eps 0.02, speed 401.56 f/s\n",
            "129461: done 803 games, mean reward -128.390, eps 0.02, speed 404.50 f/s\n",
            "129608: done 804 games, mean reward -128.440, eps 0.02, speed 396.35 f/s\n",
            "129695: done 805 games, mean reward -127.880, eps 0.02, speed 386.31 f/s\n",
            "129808: done 806 games, mean reward -128.150, eps 0.02, speed 400.53 f/s\n",
            "129916: done 807 games, mean reward -127.750, eps 0.02, speed 401.06 f/s\n",
            "130001: done 808 games, mean reward -127.760, eps 0.02, speed 395.82 f/s\n",
            "130107: done 809 games, mean reward -127.380, eps 0.02, speed 387.08 f/s\n",
            "130294: done 810 games, mean reward -128.060, eps 0.02, speed 396.71 f/s\n",
            "130466: done 811 games, mean reward -128.910, eps 0.02, speed 402.89 f/s\n",
            "130574: done 812 games, mean reward -128.590, eps 0.02, speed 371.77 f/s\n",
            "130682: done 813 games, mean reward -128.240, eps 0.02, speed 396.50 f/s\n",
            "130784: done 814 games, mean reward -128.130, eps 0.02, speed 381.88 f/s\n",
            "130890: done 815 games, mean reward -127.780, eps 0.02, speed 386.75 f/s\n",
            "130991: done 816 games, mean reward -127.310, eps 0.02, speed 400.73 f/s\n",
            "131091: done 817 games, mean reward -127.460, eps 0.02, speed 394.99 f/s\n",
            "131175: done 818 games, mean reward -127.420, eps 0.02, speed 394.81 f/s\n",
            "131282: done 819 games, mean reward -127.070, eps 0.02, speed 392.47 f/s\n",
            "131385: done 820 games, mean reward -126.700, eps 0.02, speed 386.08 f/s\n",
            "131483: done 821 games, mean reward -126.240, eps 0.02, speed 395.48 f/s\n",
            "131598: done 822 games, mean reward -126.530, eps 0.02, speed 405.72 f/s\n",
            "131685: done 823 games, mean reward -125.860, eps 0.02, speed 396.32 f/s\n",
            "131842: done 824 games, mean reward -125.870, eps 0.02, speed 403.64 f/s\n",
            "131947: done 825 games, mean reward -126.040, eps 0.02, speed 401.56 f/s\n",
            "132059: done 826 games, mean reward -125.350, eps 0.02, speed 400.74 f/s\n",
            "132164: done 827 games, mean reward -125.530, eps 0.02, speed 396.03 f/s\n",
            "132297: done 828 games, mean reward -125.450, eps 0.02, speed 401.98 f/s\n",
            "132401: done 829 games, mean reward -125.070, eps 0.02, speed 397.80 f/s\n",
            "132504: done 830 games, mean reward -125.260, eps 0.02, speed 373.33 f/s\n",
            "132610: done 831 games, mean reward -124.890, eps 0.02, speed 392.70 f/s\n",
            "132716: done 832 games, mean reward -124.480, eps 0.02, speed 396.78 f/s\n",
            "132808: done 833 games, mean reward -123.990, eps 0.02, speed 398.18 f/s\n",
            "132895: done 834 games, mean reward -123.980, eps 0.02, speed 401.99 f/s\n",
            "133003: done 835 games, mean reward -123.640, eps 0.02, speed 395.88 f/s\n",
            "133109: done 836 games, mean reward -123.270, eps 0.02, speed 397.41 f/s\n",
            "133207: done 837 games, mean reward -122.790, eps 0.02, speed 392.18 f/s\n",
            "133299: done 838 games, mean reward -122.270, eps 0.02, speed 397.45 f/s\n",
            "133407: done 839 games, mean reward -121.910, eps 0.02, speed 393.11 f/s\n",
            "Best mean reward updated -122.000 -> -121.910, model saved\n",
            "133513: done 840 games, mean reward -121.540, eps 0.02, speed 402.91 f/s\n",
            "Best mean reward updated -121.910 -> -121.540, model saved\n",
            "133618: done 841 games, mean reward -121.110, eps 0.02, speed 404.04 f/s\n",
            "Best mean reward updated -121.540 -> -121.110, model saved\n",
            "133709: done 842 games, mean reward -120.550, eps 0.02, speed 392.08 f/s\n",
            "Best mean reward updated -121.110 -> -120.550, model saved\n",
            "133817: done 843 games, mean reward -120.200, eps 0.02, speed 398.64 f/s\n",
            "Best mean reward updated -120.550 -> -120.200, model saved\n",
            "133925: done 844 games, mean reward -119.480, eps 0.02, speed 408.51 f/s\n",
            "Best mean reward updated -120.200 -> -119.480, model saved\n",
            "Solved in 133925 frames!\n"
          ]
        }
      ],
      "source": [
        "DQN_TRAIN(\n",
        "    env = env, \n",
        "    env_name = ENV_NAME, \n",
        "    qnet = qnet,\n",
        "    qnet_lr = LEARNING_RATE,\n",
        "    tgt_qnet = qtarget, \n",
        "    target_update_freq = SYNC_TARGET_FRAMES,\n",
        "    gamma = GAMMA, \n",
        "    replay_size = REPLAY_SIZE, \n",
        "    batch_size = BATCH_SIZE, \n",
        "    epsilon_f = EPSILON_FINAL, \n",
        "    epsilon_decay_last_step = EPSILON_DECAY_LAST_FRAME, \n",
        "    MEAN_REWARD_BOUND = REWARD_BOUND)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ2hasvindhG",
        "outputId": "950cf554-6626-454f-9398-0f7c18a7392b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPISODE 1\n",
            "- steps: 89\n",
            "- return: -89.0\n",
            "EPISODE 2\n",
            "- steps: 106\n",
            "- return: -106.0\n",
            "EPISODE 3\n",
            "- steps: 106\n",
            "- return: -106.0\n",
            "EPISODE 4\n",
            "- steps: 126\n",
            "- return: -126.0\n",
            "EPISODE 5\n",
            "- steps: 87\n",
            "- return: -87.0\n",
            "EPISODE 6\n",
            "- steps: 95\n",
            "- return: -95.0\n",
            "EPISODE 7\n",
            "- steps: 105\n",
            "- return: -105.0\n",
            "EPISODE 8\n",
            "- steps: 86\n",
            "- return: -86.0\n",
            "EPISODE 9\n",
            "- steps: 126\n",
            "- return: -126.0\n",
            "EPISODE 10\n",
            "- steps: 94\n",
            "- return: -94.0\n",
            "EPISODE 11\n",
            "- steps: 107\n",
            "- return: -107.0\n",
            "EPISODE 12\n",
            "- steps: 107\n",
            "- return: -107.0\n",
            "EPISODE 13\n",
            "- steps: 87\n",
            "- return: -87.0\n",
            "EPISODE 14\n",
            "- steps: 106\n",
            "- return: -106.0\n",
            "EPISODE 15\n",
            "- steps: 91\n",
            "- return: -91.0\n",
            "EPISODE 16\n",
            "- steps: 97\n",
            "- return: -97.0\n",
            "EPISODE 17\n",
            "- steps: 98\n",
            "- return: -98.0\n",
            "EPISODE 18\n",
            "- steps: 88\n",
            "- return: -88.0\n",
            "EPISODE 19\n",
            "- steps: 105\n",
            "- return: -105.0\n",
            "EPISODE 20\n",
            "- steps: 106\n",
            "- return: -106.0\n",
            "RESULTADO FINAL: média (por episódio): -100.6, episódios: 20, total de passos: 2012\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(-100.6,\n",
              " [-89.0,\n",
              "  -106.0,\n",
              "  -106.0,\n",
              "  -126.0,\n",
              "  -87.0,\n",
              "  -95.0,\n",
              "  -105.0,\n",
              "  -86.0,\n",
              "  -126.0,\n",
              "  -94.0,\n",
              "  -107.0,\n",
              "  -107.0,\n",
              "  -87.0,\n",
              "  -106.0,\n",
              "  -91.0,\n",
              "  -97.0,\n",
              "  -98.0,\n",
              "  -88.0,\n",
              "  -105.0,\n",
              "  -106.0])"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Para carregar de arquivo\n",
        "#qnet.load_state_dict(torch.load(\"/content/MountainCar-v0-XXXXXX-best.dat\", map_location=lambda storage, loc: storage))\n",
        "\n",
        "# Faz alguns testes com o modelo e salva o vídeo em arquivo\n",
        "video = VideoRecorder(env, \"politica-treinada.mp4\")\n",
        "test_Qpolicy(env, qnet, 0.0, 20, render=False, videorec=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "N2AnUp0fndhH",
        "outputId": "9507ea89-220d-4243-e045-936dd90b1931"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<video width=400 controls><source src=\"data:video/mp4;base64,\" type=\"video/mp4\"></video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "render_mp4(\"politica-treinada.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP1Xyu4wndhH"
      },
      "source": [
        "## 4. Rodando no Jogo Pong (Atari)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvEJ8CNvndhH"
      },
      "outputs": [],
      "source": [
        "--\n",
        "ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "REWARD_BOUND = 15 #19.5\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 3e-4 #1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 10**5\n",
        "EPSILON_FINAL = 0.02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWJM1U-pndhK"
      },
      "outputs": [],
      "source": [
        "env = gym.make(ENV_NAME)\n",
        "env = MaxAndSkipEnv(env)\n",
        "env = FireResetEnv(env)\n",
        "env = ProcessFrame84(env)\n",
        "env = ImageToPyTorch(env)\n",
        "env = BufferWrapper(env, 4)\n",
        "\n",
        "net = dqn_model.DQNNet(env.observation_space.shape, env.action_space.n)\n",
        "tgt_net = dqn_model.DQNNet(env.observation_space.shape, env.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umDKxHFVndhK"
      },
      "outputs": [],
      "source": [
        "DQN_TRAIN(\n",
        "    env = env, \n",
        "    env_name = ENV_NAME, \n",
        "    qnet = qnet,\n",
        "    qnet_lr = LEARNING_RATE,\n",
        "    tgt_qnet = qtarget, \n",
        "    target_update_freq = SYNC_TARGET_FRAMES,\n",
        "    gamma = GAMMA, \n",
        "    replay_size = REPLAY_SIZE, \n",
        "    batch_size = BATCH_SIZE, \n",
        "    epsilon_f = EPSILON_FINAL, \n",
        "    epsilon_decay_last_step = EPSILON_DECAY_LAST_FRAME, \n",
        "    MEAN_REWARD_BOUND = REWARD_BOUND)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpy7XZwDndhK"
      },
      "outputs": [],
      "source": [
        "# Faz alguns testes com o modelo de forma DETERMINÍSTICA e salva o vídeo em arquivo\n",
        "video = VideoRecorder(ENV, \"politica-treinada.mp4\")\n",
        "test_policy(ENV, policy, True, 5, render=False, videorec=video)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "cap07-main.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('rlx')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "27dbc9ce4cc602e4f15257b7b0018d8dff5b9ce9a7d73bc4399cb5afb1e02c4a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
