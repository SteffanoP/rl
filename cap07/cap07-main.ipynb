{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeJ3wCaKe2Wl"
      },
      "source": [
        "# Capítulo 7 - DQN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AT7LdIRGf4I"
      },
      "source": [
        "Você pode rodar este notebook no Colab ou localmente. Para abrir diretamente no Colab, basta clicar no link abaixo.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap07/cap07-main.ipynb) \n",
        "\n",
        "Para rodar localmente, primeiro, baixe todo o repositório do github: https://github.com/pablo-sampaio/rl_facil."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAHITU7VhsM7"
      },
      "source": [
        "## 1. Configurações Iniciais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyTpn6lxGf4J"
      },
      "source": [
        "### Configurações Dependentes do Sistema\n",
        "\n",
        "Rode a célula abaixo, mesmo sem estar no Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NS23BU8R1vq-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from IPython.display import clear_output\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    !pip install gym\n",
        "    !pip install gym[box2d]\n",
        "    !pip install gym[atari]\n",
        "    !pip install opencv-python\n",
        "    !pip install autorom[accept-rom-license]\n",
        "    !pip install tensorboardX\n",
        "\n",
        "    # para salvar videos\n",
        "    !apt-get install -y xvfb x11-utils\n",
        "    !pip install pyvirtualdisplay==0.2.*\n",
        "    !apt-get install ffmpeg\n",
        "\n",
        "    from pyvirtualdisplay import Display\n",
        "    global display\n",
        "    display = Display(visible=False, size=(1400, 900))\n",
        "    _ = display.start()\n",
        "\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    clear_output()\n",
        "\n",
        "    !mv /content/rl_facil/cap07/* /content/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNQSmOCRGf4L"
      },
      "source": [
        "### Configurações para Exibir Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xTUzR6SNGf4M"
      },
      "outputs": [],
      "source": [
        "# ideias adaptadas de : https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google\n",
        "from base64 import b64encode\n",
        "from IPython.display import HTML\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def render_mp4(videopath: str) -> str:\n",
        "  \"\"\"\n",
        "  Gets a string containing a b4-encoded version of the MP4 video\n",
        "  at the specified path.\n",
        "  \"\"\"\n",
        "  mp4 = open(videopath, 'rb').read()\n",
        "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
        "  html_code = f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
        "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'\n",
        "  return HTML(html_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztvJdbKVh20Y"
      },
      "source": [
        "## 2. Imports e Definições Usadas pelo DQN\n",
        "\n",
        "Código adaptado do código explicado no livro de M. Lapan, cap. 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8YL1jIkndhA"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0Gzf7VhkiHxQ"
      },
      "outputs": [],
      "source": [
        "import dqn_models\n",
        "from atari_wrappers import *\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_79PzMRmndhC"
      },
      "source": [
        "### Classes Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8OACm0r-iuh2"
      },
      "outputs": [],
      "source": [
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "\n",
        "class DQNExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, s1, a, r, done, s2):\n",
        "        experience = Experience(s1, a, r, done, s2)\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.state = self.env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a, dtype=torch.float32).to(device)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        self.exp_buffer.append(self.state, action, reward, is_done, new_state)\n",
        "        self.state = new_state\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self.state = self.env.reset()\n",
        "            self.total_reward = 0.0\n",
        "        return done_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHZQuSxmndhD"
      },
      "source": [
        "### Função de Perda (*Loss Function*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3PH-mJQ6ndhD"
      },
      "outputs": [],
      "source": [
        "def calc_loss(batch, net, tgt_net, gamma, device=\"cpu\"):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "    states_v = torch.tensor(states, dtype=torch.float32).to(device)\n",
        "    next_states_v = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
        "    actions_v = torch.tensor(actions, dtype=torch.int64).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.tensor(dones, dtype=torch.bool).to(device)\n",
        "\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
        "    next_state_values[done_mask] = 0.0\n",
        "    next_state_values = next_state_values.detach()\n",
        "\n",
        "    expected_state_action_values = next_state_values * gamma + rewards_v\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c241EVuundhE"
      },
      "source": [
        "## 2. Função Principal do DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsqgk9RNndhE"
      },
      "source": [
        "Esta é a função que faz o aprendizado. (Porém, o DQN é uma solução que incluir também as ideias dos wrappers.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JO3eR_sfndhE"
      },
      "outputs": [],
      "source": [
        "def DQN_TRAIN(env, env_name, qnet, qnet_lr, tgt_qnet, target_update_freq, gamma, replay_size, batch_size, epsilon_f, epsilon_decay_last_step, MEAN_REWARD_BOUND):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    qnet = qnet.to(device)\n",
        "    tgt_qnet = tgt_qnet.to(device)\n",
        "\n",
        "    writer = SummaryWriter(comment=\"-\" + env_name)\n",
        "    print(qnet)\n",
        "\n",
        "    buffer = DQNExperienceBuffer(replay_size)\n",
        "    agent = DQNAgent(env, buffer)\n",
        "    epsilon = 1.0\n",
        "\n",
        "    optimizer = optim.Adam(qnet.parameters(), lr=qnet_lr)\n",
        "    total_rewards = []\n",
        "    frame_idx = 0\n",
        "    ts_frame = 0\n",
        "    ts = time.time()\n",
        "    best_mean_reward = None\n",
        "    start_time_str = datetime.now().strftime(\"%Y-%m-%d,%H-%M-%S\")\n",
        "\n",
        "    while True:\n",
        "        frame_idx += 1\n",
        "        epsilon = max(epsilon_f, 1.0 - frame_idx / epsilon_decay_last_step)\n",
        "\n",
        "        reward = agent.play_step(qnet, epsilon, device=device)\n",
        "        if reward is not None:\n",
        "            total_rewards.append(reward)\n",
        "            if (time.time() - ts) == 0:\n",
        "                speed = float(\"-inf\")\n",
        "            else:\n",
        "                speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "            ts_frame = frame_idx\n",
        "            ts = time.time()\n",
        "            mean_reward = np.mean(total_rewards[-100:])\n",
        "            print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
        "                frame_idx, len(total_rewards), mean_reward, epsilon,\n",
        "                speed\n",
        "            ))\n",
        "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "            writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
        "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "                torch.save(qnet.state_dict(), env_name + \"-\" + start_time_str + \"-best.dat\")\n",
        "                # IDEIA: salvar também a iteração, para poder retomar\n",
        "                if best_mean_reward is not None:\n",
        "                    print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
        "                best_mean_reward = mean_reward\n",
        "            if mean_reward > MEAN_REWARD_BOUND:\n",
        "                print(\"Solved in %d frames!\" % frame_idx)\n",
        "                break\n",
        "\n",
        "        if len(buffer) < replay_size:\n",
        "            continue\n",
        "\n",
        "        if frame_idx % target_update_freq == 0:\n",
        "            tgt_qnet.load_state_dict(qnet.state_dict())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch = buffer.sample(batch_size)\n",
        "        loss_t = calc_loss(batch, qnet, tgt_qnet, gamma, device=device)\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZq9U2FGndhF"
      },
      "source": [
        "## 3. Rodando em Ambientes Simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SigPdPRcndhF"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"MountainCar-v0\"\n",
        "REWARD_BOUND = -110\n",
        "#ENV_NAME = \"CartPole-v0\"\n",
        "#REWARD_BOUND = 200\n",
        "\n",
        "GAMMA = 0.999\n",
        "REPLAY_SIZE = 2000\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "SYNC_TARGET_FRAMES = 250\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 80000\n",
        "EPSILON_FINAL = 0.02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "psuJKKQlndhF"
      },
      "outputs": [],
      "source": [
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "qnet = dqn_models.MLP(env.observation_space.shape[0], [128,256], env.action_space.n)\n",
        "qtarget = dqn_models.MLP(env.observation_space.shape[0], [128,256], env.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5E2S9WAndhG",
        "outputId": "1c914049-f1ba-48e6-d620-56dfd40c061b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=2, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=256, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "200: done 1 games, mean reward -200.000, eps 1.00, speed 11882.72 f/s\n",
            "400: done 2 games, mean reward -200.000, eps 0.99, speed 12992.70 f/s\n",
            "600: done 3 games, mean reward -200.000, eps 0.99, speed 18417.88 f/s\n",
            "800: done 4 games, mean reward -200.000, eps 0.99, speed 17399.78 f/s\n",
            "1000: done 5 games, mean reward -200.000, eps 0.99, speed 21983.88 f/s\n",
            "1200: done 6 games, mean reward -200.000, eps 0.98, speed 15990.48 f/s\n",
            "1400: done 7 games, mean reward -200.000, eps 0.98, speed 18938.90 f/s\n",
            "1600: done 8 games, mean reward -200.000, eps 0.98, speed 16884.94 f/s\n",
            "1800: done 9 games, mean reward -200.000, eps 0.98, speed 16850.68 f/s\n",
            "2000: done 10 games, mean reward -200.000, eps 0.97, speed 17729.28 f/s\n",
            "2200: done 11 games, mean reward -200.000, eps 0.97, speed 384.36 f/s\n",
            "2400: done 12 games, mean reward -200.000, eps 0.97, speed 438.89 f/s\n",
            "2600: done 13 games, mean reward -200.000, eps 0.97, speed 453.27 f/s\n",
            "2800: done 14 games, mean reward -200.000, eps 0.96, speed 452.80 f/s\n",
            "3000: done 15 games, mean reward -200.000, eps 0.96, speed 428.95 f/s\n",
            "3200: done 16 games, mean reward -200.000, eps 0.96, speed 461.94 f/s\n",
            "3400: done 17 games, mean reward -200.000, eps 0.96, speed 438.90 f/s\n",
            "3600: done 18 games, mean reward -200.000, eps 0.95, speed 448.50 f/s\n",
            "3800: done 19 games, mean reward -200.000, eps 0.95, speed 451.12 f/s\n",
            "4000: done 20 games, mean reward -200.000, eps 0.95, speed 451.32 f/s\n",
            "4200: done 21 games, mean reward -200.000, eps 0.95, speed 449.97 f/s\n",
            "4400: done 22 games, mean reward -200.000, eps 0.94, speed 446.58 f/s\n",
            "4600: done 23 games, mean reward -200.000, eps 0.94, speed 456.10 f/s\n",
            "4800: done 24 games, mean reward -200.000, eps 0.94, speed 448.50 f/s\n",
            "5000: done 25 games, mean reward -200.000, eps 0.94, speed 455.98 f/s\n",
            "5200: done 26 games, mean reward -200.000, eps 0.94, speed 443.15 f/s\n",
            "5400: done 27 games, mean reward -200.000, eps 0.93, speed 456.04 f/s\n",
            "5600: done 28 games, mean reward -200.000, eps 0.93, speed 450.40 f/s\n",
            "5800: done 29 games, mean reward -200.000, eps 0.93, speed 452.30 f/s\n",
            "6000: done 30 games, mean reward -200.000, eps 0.93, speed 460.35 f/s\n",
            "6200: done 31 games, mean reward -200.000, eps 0.92, speed 447.14 f/s\n",
            "6400: done 32 games, mean reward -200.000, eps 0.92, speed 461.40 f/s\n",
            "6600: done 33 games, mean reward -200.000, eps 0.92, speed 448.08 f/s\n",
            "6800: done 34 games, mean reward -200.000, eps 0.92, speed 461.21 f/s\n",
            "7000: done 35 games, mean reward -200.000, eps 0.91, speed 449.09 f/s\n",
            "7200: done 36 games, mean reward -200.000, eps 0.91, speed 453.84 f/s\n",
            "7400: done 37 games, mean reward -200.000, eps 0.91, speed 451.12 f/s\n",
            "7600: done 38 games, mean reward -200.000, eps 0.91, speed 431.36 f/s\n",
            "7800: done 39 games, mean reward -200.000, eps 0.90, speed 448.60 f/s\n",
            "8000: done 40 games, mean reward -200.000, eps 0.90, speed 443.64 f/s\n",
            "8200: done 41 games, mean reward -200.000, eps 0.90, speed 454.79 f/s\n",
            "8400: done 42 games, mean reward -200.000, eps 0.90, speed 446.67 f/s\n",
            "8600: done 43 games, mean reward -200.000, eps 0.89, speed 456.85 f/s\n",
            "8800: done 44 games, mean reward -200.000, eps 0.89, speed 440.82 f/s\n",
            "9000: done 45 games, mean reward -200.000, eps 0.89, speed 458.09 f/s\n",
            "9200: done 46 games, mean reward -200.000, eps 0.89, speed 450.76 f/s\n",
            "9400: done 47 games, mean reward -200.000, eps 0.88, speed 432.24 f/s\n",
            "9600: done 48 games, mean reward -200.000, eps 0.88, speed 436.22 f/s\n",
            "9800: done 49 games, mean reward -200.000, eps 0.88, speed 434.88 f/s\n",
            "10000: done 50 games, mean reward -200.000, eps 0.88, speed 454.00 f/s\n",
            "10200: done 51 games, mean reward -200.000, eps 0.87, speed 448.34 f/s\n",
            "10400: done 52 games, mean reward -200.000, eps 0.87, speed 459.87 f/s\n",
            "10600: done 53 games, mean reward -200.000, eps 0.87, speed 444.67 f/s\n",
            "10800: done 54 games, mean reward -200.000, eps 0.86, speed 453.79 f/s\n",
            "11000: done 55 games, mean reward -200.000, eps 0.86, speed 450.63 f/s\n",
            "11200: done 56 games, mean reward -200.000, eps 0.86, speed 434.53 f/s\n",
            "11400: done 57 games, mean reward -200.000, eps 0.86, speed 446.57 f/s\n",
            "11600: done 58 games, mean reward -200.000, eps 0.85, speed 434.83 f/s\n",
            "11800: done 59 games, mean reward -200.000, eps 0.85, speed 447.28 f/s\n",
            "12000: done 60 games, mean reward -200.000, eps 0.85, speed 436.27 f/s\n",
            "12200: done 61 games, mean reward -200.000, eps 0.85, speed 440.44 f/s\n",
            "12400: done 62 games, mean reward -200.000, eps 0.84, speed 437.04 f/s\n",
            "12600: done 63 games, mean reward -200.000, eps 0.84, speed 444.77 f/s\n",
            "12800: done 64 games, mean reward -200.000, eps 0.84, speed 451.13 f/s\n",
            "13000: done 65 games, mean reward -200.000, eps 0.84, speed 441.18 f/s\n",
            "13200: done 66 games, mean reward -200.000, eps 0.83, speed 448.93 f/s\n",
            "13400: done 67 games, mean reward -200.000, eps 0.83, speed 436.04 f/s\n",
            "13600: done 68 games, mean reward -200.000, eps 0.83, speed 445.54 f/s\n",
            "13800: done 69 games, mean reward -200.000, eps 0.83, speed 434.25 f/s\n",
            "14000: done 70 games, mean reward -200.000, eps 0.82, speed 432.90 f/s\n",
            "14200: done 71 games, mean reward -200.000, eps 0.82, speed 434.94 f/s\n",
            "14400: done 72 games, mean reward -200.000, eps 0.82, speed 442.69 f/s\n",
            "14600: done 73 games, mean reward -200.000, eps 0.82, speed 444.05 f/s\n",
            "14800: done 74 games, mean reward -200.000, eps 0.81, speed 435.94 f/s\n",
            "15000: done 75 games, mean reward -200.000, eps 0.81, speed 436.98 f/s\n",
            "15200: done 76 games, mean reward -200.000, eps 0.81, speed 433.60 f/s\n",
            "15400: done 77 games, mean reward -200.000, eps 0.81, speed 449.88 f/s\n",
            "15600: done 78 games, mean reward -200.000, eps 0.80, speed 427.20 f/s\n",
            "15800: done 79 games, mean reward -200.000, eps 0.80, speed 449.98 f/s\n",
            "16000: done 80 games, mean reward -200.000, eps 0.80, speed 433.36 f/s\n",
            "16200: done 81 games, mean reward -200.000, eps 0.80, speed 452.06 f/s\n",
            "16400: done 82 games, mean reward -200.000, eps 0.80, speed 440.59 f/s\n",
            "16600: done 83 games, mean reward -200.000, eps 0.79, speed 437.19 f/s\n",
            "16800: done 84 games, mean reward -200.000, eps 0.79, speed 430.93 f/s\n",
            "17000: done 85 games, mean reward -200.000, eps 0.79, speed 432.31 f/s\n",
            "17200: done 86 games, mean reward -200.000, eps 0.79, speed 444.81 f/s\n",
            "17400: done 87 games, mean reward -200.000, eps 0.78, speed 427.87 f/s\n",
            "17600: done 88 games, mean reward -200.000, eps 0.78, speed 445.64 f/s\n",
            "17800: done 89 games, mean reward -200.000, eps 0.78, speed 429.85 f/s\n",
            "18000: done 90 games, mean reward -200.000, eps 0.78, speed 444.41 f/s\n",
            "18200: done 91 games, mean reward -200.000, eps 0.77, speed 436.26 f/s\n",
            "18400: done 92 games, mean reward -200.000, eps 0.77, speed 440.58 f/s\n",
            "18600: done 93 games, mean reward -200.000, eps 0.77, speed 439.37 f/s\n",
            "18800: done 94 games, mean reward -200.000, eps 0.77, speed 429.53 f/s\n",
            "19000: done 95 games, mean reward -200.000, eps 0.76, speed 437.69 f/s\n",
            "19200: done 96 games, mean reward -200.000, eps 0.76, speed 435.54 f/s\n",
            "19400: done 97 games, mean reward -200.000, eps 0.76, speed 436.60 f/s\n",
            "19600: done 98 games, mean reward -200.000, eps 0.76, speed 436.38 f/s\n",
            "19800: done 99 games, mean reward -200.000, eps 0.75, speed 443.61 f/s\n",
            "20000: done 100 games, mean reward -200.000, eps 0.75, speed 442.65 f/s\n",
            "20200: done 101 games, mean reward -200.000, eps 0.75, speed 419.75 f/s\n",
            "20400: done 102 games, mean reward -200.000, eps 0.74, speed 357.42 f/s\n",
            "20600: done 103 games, mean reward -200.000, eps 0.74, speed 335.81 f/s\n",
            "20800: done 104 games, mean reward -200.000, eps 0.74, speed 344.69 f/s\n",
            "21000: done 105 games, mean reward -200.000, eps 0.74, speed 447.44 f/s\n",
            "21200: done 106 games, mean reward -200.000, eps 0.73, speed 432.97 f/s\n",
            "21400: done 107 games, mean reward -200.000, eps 0.73, speed 447.65 f/s\n",
            "21600: done 108 games, mean reward -200.000, eps 0.73, speed 433.94 f/s\n",
            "21800: done 109 games, mean reward -200.000, eps 0.73, speed 432.55 f/s\n",
            "22000: done 110 games, mean reward -200.000, eps 0.72, speed 432.30 f/s\n",
            "22200: done 111 games, mean reward -200.000, eps 0.72, speed 444.95 f/s\n",
            "22400: done 112 games, mean reward -200.000, eps 0.72, speed 438.46 f/s\n",
            "22600: done 113 games, mean reward -200.000, eps 0.72, speed 434.86 f/s\n",
            "22800: done 114 games, mean reward -200.000, eps 0.72, speed 431.83 f/s\n",
            "23000: done 115 games, mean reward -200.000, eps 0.71, speed 437.04 f/s\n",
            "23200: done 116 games, mean reward -200.000, eps 0.71, speed 432.40 f/s\n",
            "23400: done 117 games, mean reward -200.000, eps 0.71, speed 435.36 f/s\n",
            "23600: done 118 games, mean reward -200.000, eps 0.71, speed 435.11 f/s\n",
            "23800: done 119 games, mean reward -200.000, eps 0.70, speed 439.71 f/s\n",
            "24000: done 120 games, mean reward -200.000, eps 0.70, speed 431.43 f/s\n",
            "24200: done 121 games, mean reward -200.000, eps 0.70, speed 440.33 f/s\n"
          ]
        }
      ],
      "source": [
        "DQN_TRAIN(\n",
        "    env = env, \n",
        "    env_name = ENV_NAME, \n",
        "    qnet = qnet,\n",
        "    qnet_lr = LEARNING_RATE,\n",
        "    tgt_qnet = qtarget, \n",
        "    target_update_freq = SYNC_TARGET_FRAMES,\n",
        "    gamma = GAMMA, \n",
        "    replay_size = REPLAY_SIZE, \n",
        "    batch_size = BATCH_SIZE, \n",
        "    epsilon_f = EPSILON_FINAL, \n",
        "    epsilon_decay_last_step = EPSILON_DECAY_LAST_FRAME, \n",
        "    MEAN_REWARD_BOUND = REWARD_BOUND)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xU9mp2undhG"
      },
      "outputs": [],
      "source": [
        "!tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ2hasvindhG"
      },
      "outputs": [],
      "source": [
        "# Faz alguns testes com o modelo de forma DETERMINÍSTICA e salva o vídeo em arquivo\n",
        "video = VideoRecorder(env, \"politica-treinada.mp4\")\n",
        "dqn_models.test_Qpolicy(env, qnet, 0.0, 5, render=False, videorec=video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2AnUp0fndhH"
      },
      "outputs": [],
      "source": [
        "render_mp4(\"politica-treinada.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP1Xyu4wndhH"
      },
      "source": [
        "## 4. Rodando no Jogo Pong (Atari)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvEJ8CNvndhH"
      },
      "outputs": [],
      "source": [
        "--\n",
        "ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "REWARD_BOUND = 15 #19.5\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 3e-4 #1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 10**5\n",
        "EPSILON_FINAL = 0.02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWJM1U-pndhK"
      },
      "outputs": [],
      "source": [
        "env = gym.make(ENV_NAME)\n",
        "env = MaxAndSkipEnv(env)\n",
        "env = FireResetEnv(env)\n",
        "env = ProcessFrame84(env)\n",
        "env = ImageToPyTorch(env)\n",
        "env = BufferWrapper(env, 4)\n",
        "\n",
        "net = dqn_model.DQNNet(env.observation_space.shape, env.action_space.n)\n",
        "tgt_net = dqn_model.DQNNet(env.observation_space.shape, env.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umDKxHFVndhK"
      },
      "outputs": [],
      "source": [
        "DQN_TRAIN(\n",
        "    env = env, \n",
        "    env_name = ENV_NAME, \n",
        "    qnet = qnet,\n",
        "    qnet_lr = LEARNING_RATE,\n",
        "    tgt_qnet = qtarget, \n",
        "    target_update_freq = SYNC_TARGET_FRAMES,\n",
        "    gamma = GAMMA, \n",
        "    replay_size = REPLAY_SIZE, \n",
        "    batch_size = BATCH_SIZE, \n",
        "    epsilon_f = EPSILON_FINAL, \n",
        "    epsilon_decay_last_step = EPSILON_DECAY_LAST_FRAME, \n",
        "    MEAN_REWARD_BOUND = REWARD_BOUND)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpy7XZwDndhK"
      },
      "outputs": [],
      "source": [
        "# Faz alguns testes com o modelo de forma DETERMINÍSTICA e salva o vídeo em arquivo\n",
        "video = VideoRecorder(ENV, \"politica-treinada.mp4\")\n",
        "test_policy(ENV, policy, True, 5, render=False, videorec=video)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "cap06-main.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('rlx')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "27dbc9ce4cc602e4f15257b7b0018d8dff5b9ce9a7d73bc4399cb5afb1e02c4a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}