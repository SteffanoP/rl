{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap03/cap03-main.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Capítulo 3 - MDPs, Retornos e Funções de Valor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos focar nesses três ambientes por serem mais simples\n",
    "#env = gym.make(\"MountainCar-v0\")\n",
    "#env = gym.make(\"CartPole-v1\")\n",
    "#env = gym.make(\"Taxi-v3\")\n",
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figura mostrando interação agente(política)-ambiente](figura_mdp.png \"Interação agente-ambiente\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1 - Episódio e Trajetória"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um *episódio* é uma execução da tarefa (ou do ambiente gym). \n",
    "\n",
    "E a *trajetória* é a sequência de estados (observações), ações e recompensas do episódio. Assumindo um episódio de $n$ passos (ações aplicadas):\n",
    "\n",
    "$S_0 \\rightarrow A_0 \\rightarrow R_1 \\rightarrow S_1 \\rightarrow A_1 \\rightarrow R_2 \\rightarrow S_2 \\rightarrow \\cdots S_{n-1} \\rightarrow A_{n-1} \\rightarrow R_n \\rightarrow S_n$\n",
    "\n",
    "Vamos ilustrar um episódio em um MDP usando o ambiente *\"env\"* escolhido no código acima. \n",
    "\n",
    "Estamos assumindo que o episódio encerrou de fato (chegou em um estado final) em *$n$=\"TOTAL_STEPS\"* passos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_STEPS = 5\n",
    "\n",
    "i = 0\n",
    "obs = env.reset()\n",
    "print(f\"S0 = {obs}\")\n",
    "\n",
    "done = False\n",
    "\n",
    "# roda apenas alguns passos\n",
    "for i in range(0,TOTAL_STEPS):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    print(f\" A{i} = {action}\")\n",
    "\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    print(f\"  R{i+1} = {reward}\")\n",
    "    print(f\"S{i+1} = {next_obs}\")\n",
    "\n",
    "    obs = next_obs\n",
    "    #time.sleep(0.1)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os detalhes do *episódio* que mostramos acima são chamamos de *trajectory* ou *rollout*.\n",
    "\n",
    "Dependendo do algoritmo, vamos precisar analisar essas informações em trios (S,A,R) ou quádruplas (S,A,R,S) ou até quíntuplas (S,A,R,S',A').\n",
    "\n",
    "Abaixo, vamos agrupar e guardar em trio os dados de 1 episódio completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "trajectory = [] \n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, done, _ = env.step(action)\n",
    "    trajectory.append( (obs, action, reward) )\n",
    "    obs = next_obs\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"Trajetória como sequência de trios (STATE, ACTION, REWARD):\")\n",
    "trajectory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A escolha de uma ação a cada estado é chamada de **política**. Até aqui, estamos usando uma política aleatória (que escolhe qualquer das ações disponíveis com igual probabilidade). \n",
    "\n",
    "O nosso objetivo é ver os algoritmos que aprendem uma política \"boa\". Mas ainda não veremos nesta aula.\n",
    "\n",
    "Para o restante deste notebook, vamos usar uma política simples que chamamos de **policy_0**. Ela:\n",
    "- com 50% de chance, escolhe a ação **0**\n",
    "- com 5o% de chance, escolhe qualquer ação aleatoriamente\n",
    "\n",
    "Veja o código dela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "\n",
    "def policy0(obs):\n",
    "    x = np.random.random()\n",
    "    if x <= 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.random.randint(1, num_actions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também vamos definir uma função `run_episode()` para gerar uma trajetória (de 1 episódio completo) usando uma política dada como parâmetro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent_policy):\n",
    "    obs = env.reset()\n",
    "    trajectory = [] \n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent_policy(obs)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        trajectory.append( (obs, action, reward) )\n",
    "        obs = next_obs\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    #trajectory.append( (obs, None, None) )\n",
    "    return trajectory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos criar uma trajetória com a política proposta antes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trajetória:\")\n",
    "trajectory = run_episode(env, agent_policy=policy0)\n",
    "print(trajectory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2 - Calcular os Retornos\n",
    "\n",
    "O *retorno (final)* $G$ é uma medida da recompensa total obtida ao longo de um episódio. \n",
    "\n",
    "Em um MDP, o objetivo é otimizar o valor médio de $G$, para infinitos episódios.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 - Retorno final do episódio ($G$)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para um episódio com $n$ passos, o **retorno (não-descontado)** é calculado assim:\n",
    "\n",
    "$ G = R_1 + R_2 + R_3 + \\cdots + R_n = \\displaystyle\\sum_{i=1}^{n} R_i$\n",
    "\n",
    "No código a seguir, vamos calcular o *retorno não-descontado* da trajetória calculada antes.\n",
    "\n",
    "*Observação*: Em código, como \"return\" é uma palavra reservada de Python, o *retorno* do episódio será representando por nomes como:\n",
    "- `sum_rewards`\n",
    "- ou `episode_return`\n",
    "- ou `episode_reward` \n",
    "- ou versões abreviadas desses nomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_rewards = 0.0\n",
    "for (s, a, r) in trajectory:\n",
    "    sum_rewards += r\n",
    "\n",
    "print(\"Retorno não-descontado:\", sum_rewards)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porém, é mais usado o **retorno descontado** de um episódio.\n",
    "\n",
    "Neste caso, $G$ é uma soma que \"atenua\" recompensas mais distantes, valorizando mais as recompensas iniciais. (Você prefere receber 100 reais agora, de uma vez, ou em 100 parcelas de 1 real?)\n",
    "\n",
    "Para isso, a cada passo, a recompensa tem uma *redução* na sua relevância, dada por um parâmetro $\\gamma\\;$, tal que $0 < \\gamma \\leq 1$.\n",
    "\n",
    "Para um episódio com $n$ passos, o *retorno descontado* é calculado assim:\n",
    "\n",
    "$ G = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\cdots + \\gamma^{(n-1)} R_n = \\displaystyle\\sum_{i=1}^{n} \\gamma^{(i-1)} R_i$\n",
    "\n",
    "Vamos criar uma função para fazer esse cálculo, a partir de uma dada trajetória (de 1 episódio):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode_return(trajectory, gamma):\n",
    "    step = 0\n",
    "    discounted_sum_rewards = 0.0\n",
    "    for (s, a, r) in trajectory:\n",
    "        discounted_sum_rewards += (gamma ** step) * r\n",
    "        step += 1\n",
    "    return discounted_sum_rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, calculamos o *retorno descontado* da trajetória calculada na seção anterior, assumindo um valor específico de $\\gamma$ (variável `GAMMA`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "\n",
    "epi_return = get_episode_return(trajectory, gamma=GAMMA)\n",
    "print(\"Retorno descontado:\", epi_return)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 - Retornos intermediários a cada passo ($G_i$)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Também podemos calcular um retorno parcial, a partir de um passo específico $i$ de um dado episódio:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "   && &\\quad G_0     &= &\\;\\;R_1 &+ \\gamma &R_2 &+ \\gamma^2 &R_3 &+ \\gamma^3 &R_4 &+ \\gamma^4 &R_5 &+ \\;\\cdots \\;&+ \\gamma^{n-1} &R_n & \\;\\;\\;(= G) \\\\\n",
    "   && &\\quad G_1     &= &        &         &R_2 &+ \\gamma   &R_3 &+ \\gamma^2 &R_4 &+ \\gamma^3 &R_5 &+ \\;\\cdots \\;&+ \\gamma^{n-2} &R_n &       \\\\ \n",
    "   && &\\quad G_2     &= &        &         &    &           &R_3 &+ \\gamma   &R_4 &+ \\gamma^2 &R_5 &+ \\;\\cdots \\;&+ \\gamma^{n-3} &R_n &       \\\\\n",
    "   && &\\quad G_3     &= &        &         &    &           &    &           &R_4 &+ \\gamma   &R_5 &+ \\;\\cdots \\;&+ \\gamma^{n-4} &R_n &       \\\\\n",
    "   && &\\quad         &\\cdots   & & & & & & & & & & & &    & \\\\\n",
    "   && &\\quad G_{n-1} &= &        & & & & & & & & & & &R_n & \\\\\n",
    "   && &\\quad G_n     &= &\\;\\;0 & & & & & & & & & & &      & \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Podemos calcular um retorno parcial $G_i$ simplesmente omitindo os $i$ passos inicias da trajectória:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trajectory)+1):\n",
    "    Gi = get_episode_return(trajectory[i:], GAMMA)\n",
    "    print(f\"Retorno parcial G_{i} :\", Gi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe novamente a série de equações anteriores para os retornos parciais $G_0$, $G_1$, $G_2$, etc.\n",
    "\n",
    "Percebe que existe apenas uma pequena mudança entre cada equação (para $G_i$) e a equação logo abaixo?\n",
    "\n",
    "De fato, existe uma relação (recursiva) entre $G_i$ e $G_{i+1}$ que pode ser expressa assim:\n",
    "$$\n",
    "   G_{i} = R_{i+1} + \\gamma G_{i+1}\n",
    "$$\n",
    "\n",
    "Usando essa relação recursiva, podemos calcular todos os retornos parciais de maneira mais simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula os retornos parciais a partir de cada passo \n",
    "# em ordem invertida (G_i, para cada i de n a 0) \n",
    "i = len(trajectory)\n",
    "\n",
    "Gi = 0.0\n",
    "print(f\"G_{i} =\", Gi)\n",
    "\n",
    "for (s, a, r) in reversed(trajectory):   \n",
    "    i = i - 1\n",
    "    Gi = r + GAMMA*Gi\n",
    "    print(f\"G_{i} =\", Gi)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3 - Funções de Valor\n",
    "\n",
    "São funções que não fazem parte da essência de um MDP, mas são úteis para criar algoritmos.\n",
    "\n",
    "Todas elas fazem avaliações dos *retornos esperados* (médios) para uma *política específica*! \n",
    "\n",
    "Veremos dois tipos de função de valor, a seguir. Ambas são calculadas a partir dos retornos intermediários.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Função de valor do estado $V(s)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta função dá o retorno esperado a partir de cada estado $s$, para uma política específica.\n",
    "\n",
    "De forma matemática, ela é definida assim:\n",
    "\n",
    "$$V(s) = E[G_t | S_t=s]$$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visão Algorítmica\n",
    "\n",
    "Uma explicação mais algorítmica do valor de $V(s)$ para um $s$ específico é dada a seguir:\n",
    "\n",
    "---\n",
    "1. rode infinitos episódios com a política\n",
    "2. analise cada episódio, e a cada passo iniciado no estado $s_i$:\n",
    "   - calcule o retorno parcial $G_i$\n",
    "   - salve $G_i$ no histórico do estado $s_i$\n",
    "3. Para cada estado $s$:\n",
    "   - $V(s)$ = média de todos os retornos do histórico do estado $s$\n",
    "\n",
    "---\n",
    "\n",
    "Implementamos esta ideia abaixo, rodando 5000 episódios. Para isso, vamos anexar cada retorno a um dicionário `returns_history` (um histórico dos retornos) indexado pelo $s$ onde se originou o retorno intermediário.\n",
    "\n",
    "Esta implementação assume ambiente de *estado discreto*, representados por inteiros iniciados em 0. Exemplos de ambientes assim são *FrozenLake* e *Taxi*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associa cada estado a uma lista de retornos parciais \n",
    "# obtidos a partir do estado (em um episódio qualquer)\n",
    "returns_history = dict()\n",
    "\n",
    "# roda muitos episódios, com a política desejada\n",
    "for epi in range(5000):\n",
    "    trajectory = run_episode(env, policy0)\n",
    "    \n",
    "    # calcula os retornos a cada passo (G_i, para cada i de n a 0) do episódio\n",
    "    # guardando o valor em returns_history\n",
    "    Gi = 0.0\n",
    "    for (s, a, r) in reversed(trajectory):   \n",
    "        Gi = r + GAMMA*Gi\n",
    "        if s not in returns_history.keys():\n",
    "            returns_history[s] = [ Gi ]\n",
    "        else:\n",
    "            returns_history[s].append(Gi)\n",
    "\n",
    "# associa cada estado à média dos retornos parciais\n",
    "V = np.zeros(env.observation_space.n)\n",
    "\n",
    "# calcula V\n",
    "for s in returns_history.keys():\n",
    "    V[s] = np.mean( returns_history[s] )\n",
    "\n",
    "V"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o $V$ calculado para estados discretos (que são representados por inteiros) pode ser representado como um array. O estado é usado como o índice para acessar o array, onde está guardado o valor daquele estado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Função de valor do estado-ação Q(s,a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De maneira análoga ao $V(s)$, o $Q(s,a)$ representa o retorno esperado a partir do par $(s,a)$:\n",
    "\n",
    "$$Q(s,a) = E[G_t | S_t=s, A_t=a]$$ \n",
    "\n",
    "Em outras palavras, $Q(s,a)$ responde a esta pergunta:\n",
    "\n",
    "*\"Quando estava no estado **s** e fez a ação **a**, qual o retorno esperado (se continuar seguindo a política no restante do episódio)?\"*\n",
    "\n",
    "A definição e a forma de calcular é análoga ao $V(s)$, mas vamos usar um dicionário `returns_history` indexado pelo par $(s,a)$ onde se originou cada retorno $G_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associa cada para (estado, ação) a uma lista de retornos parciais \n",
    "# obtidos a partir do estado ao realizar aquela ação (em um episódio qualquer)\n",
    "returns_history = dict()\n",
    "\n",
    "# roda muitos episódios, com a política desejada\n",
    "for epi in range(5000):\n",
    "    trajectory = run_episode(env, policy0)\n",
    "\n",
    "    # calcula os retornos a cada passo (G_i, para cada i de n a 0) do episódio\n",
    "    # guardando o valor em returns_history\n",
    "\n",
    "    # completar...\n",
    "    \n",
    "\n",
    "# matriz para associar cada par (estado, ação) à média dos retornos parciais\n",
    "Q = np.zeros(shape=(env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# calcula Q\n",
    "\n",
    "# completar..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que a função $Q$ calculado para estados discretos e ações discretas (ambos representados por inteiros) pode ser representada como uma matriz. Ela pode ser vista como uma _tabela_ com linhas representando os *estados* e colunas representando as *ações*. \n",
    "\n",
    "Por esse motivo, chamamos essa representação de **Q-table** (tabela-Q)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Preparando para os Métodos Baseados em Q-table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora sim, podemos falar um pouquinho sobre os algoritmos de controle para RL, que são os algoritmos capazes de aprender políticas \"boas\", ou seja, políticas que dão altos retornos.\n",
    "\n",
    "Coloque-se no lugar do algoritmo e suponha que você inicie com uma política qualquer (provavelmente ruim) e que você tenha calculado o *Q* dessa política com uma Q-table.\n",
    "\n",
    "**De que forma você poderia melhorar a política olhando para os valores de Q?**\n",
    "\n",
    "**Ou, como escolher a melhor ação a cada estado, usando a Q-table?**\n",
    "\n",
    "---\n",
    "\n",
    "No próxima parte do curso, veremos um algoritmo para RL, da família Monte-Carlo, onde a política é implicitamente representada pelo $Q$. \n",
    "\n",
    "Este método repete $N$ vezes esses passos:\n",
    "1. Rode um episódio, usando a política representada pela tabela $Q$\n",
    "   - salve a trajetória\n",
    "1. Depois, calcule os valores de $G_i$ e use esses valores para atualizar $Q$\n",
    "   - ao atualizar $Q$, a política também muda\n",
    "\n",
    "---\n",
    "\n",
    "Veremos mais detalhes no próximo capítulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "47acfd36b4a698d100796428813311ecacef03b489c77dd1fdf080373e214244"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
