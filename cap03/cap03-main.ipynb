{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O58-wFDPykfr"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap03/cap03-main.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJLl2gwEykfx",
        "tags": []
      },
      "source": [
        "# Capítulo 3 - Ambientes, Retornos e Funções de Valor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aKTfNBr1y_vY"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from IPython.display import display, Image\n",
        "import time\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  !pip install gymnasium\n",
        "  clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yqr4tGCgykfy"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVqUpI92_Kvk"
      },
      "source": [
        "A maior parte do notebook foi pensada para o ambiente `FrozenLake`, mas você pode testar com outros ambientes. (Fica como exercício ajustar as partes onde o código não funcionar para outros ambientes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xBf4CB5Lykf0"
      },
      "outputs": [],
      "source": [
        "# vamos focar nesses três ambientes por serem mais simples\n",
        "# ver mais em: \n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "\n",
        "#env = gym.make(\"MountainCar-v0\")\n",
        "#env = gym.make(\"CartPole-v1\")\n",
        "#env = gym.make(\"Taxi-v3\")\n",
        "#env = gym.make(\"Blackjack-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2GhjTWfykf0"
      },
      "source": [
        "![Figura mostrando interação agente(política)-ambiente](figura_mdp.png \"Interação agente-ambiente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JIcbuGcykf1",
        "tags": []
      },
      "source": [
        "## 1 - Episódio e Trajetória"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUwD8YVoykf1"
      },
      "source": [
        "Um **episódio** é uma execução completa da tarefa (ou do ambiente gym).\n",
        "\n",
        "E a **trajetória** é a sequência de estados (observações), ações e recompensas do episódio. Assumindo um episódio de $T$ passos (ações aplicadas):\n",
        "\n",
        "$S_0 \\rightarrow A_0 \\rightarrow R_1 \\rightarrow S_1 \\rightarrow A_1 \\rightarrow R_2 \\rightarrow S_2 \\rightarrow \\cdots S_{T-1} \\rightarrow A_{n-1} \\rightarrow R_T \\rightarrow S_T$\n",
        "\n",
        "Vamos ilustrar um episódio em um MDP usando o ambiente **`env`** escolhido no código acima.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0Gv5YNiHykf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S0 = 0\n",
            " A0 = 1\n",
            "  R1 = 0.0\n",
            "S1 = 0\n",
            " A1 = 2\n",
            "  R2 = 0.0\n",
            "S2 = 4\n",
            " A2 = 1\n",
            "  R3 = 0.0\n",
            "S3 = 5\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "\n",
        "# inicia um novo episódio\n",
        "state, _ = env.reset()\n",
        "print(f\"S0 = {state}\")\n",
        "\n",
        "done = False\n",
        "\n",
        "# roda até o episódio encerrar\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    print(f\" A{i} = {action}\")\n",
        "\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    i += 1\n",
        "\n",
        "    print(f\"  R{i} = {reward}\")\n",
        "    print(f\"S{i} = {next_state}\")\n",
        "\n",
        "    state = next_state\n",
        "    #time.sleep(0.1)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkvECDpjykf3"
      },
      "source": [
        "Os detalhes do *episódio* que mostramos acima são chamamos de *trajetória* (ou *rollout*).\n",
        "\n",
        "Dependendo do algoritmo, vamos precisar analisar essas informações em trios (S,A,R) ou quádruplas (S,A,R,S) ou até quíntuplas (S,A,R,S',A').\n",
        "\n",
        "Abaixo, vamos guardar uma trajetória como uma lista de trios. Cada trio desses será chamado de um *passo* da trajetória."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9tUm11cpykf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trajetória como sequência de trios (STATE, ACTION, REWARD):\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(0, 2, 0.0), (4, 3, 0.0), (0, 3, 0.0), (0, 2, 0.0), (1, 1, 0.0)]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state, _ = env.reset()\n",
        "trajectory = []\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    trajectory.append( (state, action, reward) )\n",
        "    \n",
        "    done = terminated or truncated\n",
        "    state = next_state\n",
        "\n",
        "# o último estado pode ser incluído como um trio incompleto, se preciso\n",
        "# porém, o restante do notebook, assume que não tem essa informação\n",
        "#trajectory.append( (obs, None, None) )\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(\"Trajetória como sequência de trios (STATE, ACTION, REWARD):\")\n",
        "trajectory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8-vBP-9Hk5t"
      },
      "source": [
        "Em termos teóricos, um ambiente é modelado matematicamente como um **Markov Decision Process** (MDP), como vimos em sala."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxZMi3mIDfwe"
      },
      "source": [
        "## 2 - Política"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgVMSIOcykf4"
      },
      "source": [
        "A escolha de uma ação a cada estado é feita pela chamada **política**. Até aqui, estamos usando uma política aleatória (que escolhe qualquer das ações disponíveis com igual probabilidade).\n",
        "\n",
        "O nosso objetivo (para as próximas aulas) é ver os algoritmos que aprendem uma política \"boa\".\n",
        "\n",
        "Para o restante deste notebook, vamos usar uma política simples que chamamos de **policy_0**. Ela foi pensada para o ambiente `FrozenLake`:\n",
        "- com 45% de chance, escolhe a ação **1** (mover para baixo)\n",
        "- com 45% de chance, escolhe a ação **2** (para a direita)\n",
        "- com 10% de chance, escolhe qualquer ação aleatoriamente\n",
        "\n",
        "Veja o código dela:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EDPXQANOykf4"
      },
      "outputs": [],
      "source": [
        "def policy_0(state):\n",
        "    num_actions = env.action_space.n\n",
        "    x = np.random.random()\n",
        "    if x <= 0.45:\n",
        "        return 1\n",
        "    elif x <= 0.90:\n",
        "      return 2\n",
        "    else:\n",
        "        return np.random.randint(0, num_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def policy_1(state):\n",
        "    num_actions = env.action_space.n\n",
        "    x = np.random.random()\n",
        "    if x <= 0.55:\n",
        "        return 1\n",
        "    else:\n",
        "      return 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9ldvMmWykf4"
      },
      "source": [
        "Também vamos definir uma função `run_episode()` para gerar uma trajetória (de 1 episódio completo) usando uma política dada como parâmetro:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0TZck0Xcykf4"
      },
      "outputs": [],
      "source": [
        "def run_episode(env, agent_policy):\n",
        "    obs, _ = env.reset()\n",
        "    trajectory = []\n",
        "\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = agent_policy(obs)\n",
        "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        trajectory.append( (obs, action, reward) )\n",
        "        obs = next_obs\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    return trajectory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOCM6HBTykf5"
      },
      "source": [
        "Agora, vamos criar uma trajetória com a política proposta antes. Veja que passamos a função `policy_0` como parâmetro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 1, 0.0), (4, 2, 0.0)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_episode(env, policy_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A seguir, vamos criar uma trajetória que termine em sucesso (recompensa final == `1.0`), pensando no ambiente `FrozenLake`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NASrdu_qykf5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 1, 0.0),\n",
              " (4, 2, 0.0),\n",
              " (8, 2, 0.0),\n",
              " (9, 2, 0.0),\n",
              " (10, 2, 0.0),\n",
              " (14, 1, 1.0)]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "while trajectory[-1][2] < 1.0:\n",
        "  trajectory = run_episode(env, policy_0)\n",
        "\n",
        "trajectory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBDUUUPcykf5",
        "tags": []
      },
      "source": [
        "## 3 - Calcular os Retornos\n",
        "\n",
        "O *retorno (final)* $G$ é uma medida da recompensa total obtida ao longo de um episódio.\n",
        "\n",
        "Em um MDP, o objetivo é otimizar o valor médio de $G$, para infinitos episódios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vluMet1ykf5",
        "tags": []
      },
      "source": [
        "### 3.1 - Retorno (completo) do episódio ($G$)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_igU0LFzykf5"
      },
      "source": [
        "\n",
        "Para um episódio com $n$ passos, o **retorno (não-descontado)** é calculado assim:\n",
        "\n",
        "$ G = R_1 + R_2 + R_3 + \\cdots + R_n = \\displaystyle\\sum_{i=1}^{n} R_i$\n",
        "\n",
        "No código a seguir, vamos calcular o *retorno não-descontado* da trajetória calculada antes.\n",
        "\n",
        "*Observação*: Em código, como \"return\" é uma palavra reservada de Python, o *retorno* do episódio será representando por nomes como:\n",
        "- `sum_rewards`\n",
        "- ou `episode_return`\n",
        "- ou `episode_reward`\n",
        "- ou versões abreviadas desses nomes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "K5IYx5Rhykf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retorno não-descontado: 1.0\n"
          ]
        }
      ],
      "source": [
        "episode_reward = 0.0\n",
        "for (s, a, r) in trajectory:\n",
        "    episode_reward += r\n",
        "\n",
        "print(\"Retorno não-descontado:\", episode_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA7t1gfkykf6"
      },
      "source": [
        "Porém, é mais usado o **retorno descontado** de um episódio.\n",
        "\n",
        "Neste caso, $G$ é uma soma que \"atenua\" recompensas mais distantes, valorizando mais as recompensas iniciais. (Você prefere receber 100 reais agora, de uma vez, ou em 100 parcelas de 1 real?)\n",
        "\n",
        "Para isso, a cada passo, a recompensa tem uma *redução* na sua relevância, dada por um parâmetro $\\gamma\\;$, tal que $0 < \\gamma \\leq 1$.\n",
        "\n",
        "Para um episódio com $n$ passos, o *retorno descontado* é calculado assim:\n",
        "\n",
        "$ G = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\cdots + \\gamma^{(n-1)} R_n = \\displaystyle\\sum_{i=1}^{n} \\gamma^{(i-1)} R_i$\n",
        "\n",
        "Vamos criar uma função para fazer esse cálculo, a partir de uma dada trajetória (de 1 episódio):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OP_hO2bSykf6"
      },
      "outputs": [],
      "source": [
        "def get_episode_return(trajectory, gamma):\n",
        "    step = 0\n",
        "    episode_reward = 0.0\n",
        "    for (s, a, r) in trajectory:\n",
        "        episode_reward += (gamma ** step) * r\n",
        "        step += 1\n",
        "    return episode_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoa_Wv3bykf6"
      },
      "source": [
        "A seguir, calculamos o *retorno descontado* da trajetória calculada na seção anterior, assumindo um valor específico de $\\gamma$ (variável `GAMMA`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "o50jMy66ykf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retorno descontado: 0.9509900498999999\n"
          ]
        }
      ],
      "source": [
        "GAMMA = 0.99\n",
        "\n",
        "epi_return = get_episode_return(trajectory, GAMMA)\n",
        "print(\"Retorno descontado:\", epi_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-Kcpzzcykf7",
        "tags": []
      },
      "source": [
        "### 3.2 - Retornos parciais ($G_i$)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTn_Ikctykf7"
      },
      "source": [
        "\n",
        "Também podemos calcular um retorno parcial, a partir de um passo específico $i$ de um dado episódio:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "   && &\\quad G_0     &= &\\;\\;R_1 &+ \\gamma &R_2 &+ \\gamma^2 &R_3 &+ \\gamma^3 &R_4 &+ \\gamma^4 &R_5 &+ \\;\\cdots \\;&+ \\gamma^{n-1} &R_n & \\;\\;\\;(= G) \\\\\n",
        "   && &\\quad G_1     &= &        &         &R_2 &+ \\gamma   &R_3 &+ \\gamma^2 &R_4 &+ \\gamma^3 &R_5 &+ \\;\\cdots \\;&+ \\gamma^{n-2} &R_n &       \\\\\n",
        "   && &\\quad G_2     &= &        &         &    &           &R_3 &+ \\gamma   &R_4 &+ \\gamma^2 &R_5 &+ \\;\\cdots \\;&+ \\gamma^{n-3} &R_n &       \\\\\n",
        "   && &\\quad G_3     &= &        &         &    &           &    &           &R_4 &+ \\gamma   &R_5 &+ \\;\\cdots \\;&+ \\gamma^{n-4} &R_n &       \\\\\n",
        "   && &\\quad         &\\cdots   & & & & & & & & & & & &    & \\\\\n",
        "   && &\\quad G_{n-1} &= &        & & & & & & & & & & &R_n & \\\\\n",
        "   && &\\quad G_n     &= &\\;\\;0 & & & & & & & & & & &      & \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Podemos calcular um retorno parcial $G_i$ simplesmente omitindo os $i$ passos inicias da trajectória:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9801"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_episode_return(trajectory[3:], GAMMA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uflrQ3phykf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retorno parcial G_0 : 0.9509900498999999\n",
            "Retorno parcial G_1 : 0.96059601\n",
            "Retorno parcial G_2 : 0.970299\n",
            "Retorno parcial G_3 : 0.9801\n",
            "Retorno parcial G_4 : 0.99\n",
            "Retorno parcial G_5 : 1.0\n",
            "Retorno parcial G_6 : 0.0\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(trajectory)+1):\n",
        "    Gi = get_episode_return(trajectory[i:], GAMMA)\n",
        "    print(f\"Retorno parcial G_{i} :\", Gi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lykM4RVqykf7"
      },
      "source": [
        "Observe novamente a série de equações anteriores para os retornos parciais $G_0$, $G_1$, $G_2$, etc.\n",
        "\n",
        "Percebe que existe apenas uma pequena mudança entre cada equação (para $G_i$) e a equação logo abaixo?\n",
        "\n",
        "De fato, existe uma relação (recursiva) entre $G_i$ e $G_{i+1}$ que pode ser expressa assim:\n",
        "$$\n",
        "   G_{i} = R_{i+1} + \\gamma G_{i+1}\n",
        "$$\n",
        "\n",
        "(Por exemplo: $G_0 = R_1 + \\gamma G_1$).\n",
        "\n",
        "\n",
        "Usando essa relação recursiva, podemos calcular todos os retornos parciais de maneira mais simples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "e8Z7syEcykf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "G_6 = 0.0\n",
            "G_5 = 1.0\n",
            "G_4 = 0.99\n",
            "G_3 = 0.9801\n",
            "G_2 = 0.9702989999999999\n",
            "G_1 = 0.96059601\n",
            "G_0 = 0.9509900498999999\n"
          ]
        }
      ],
      "source": [
        "# calcula os retornos parciais a partir de cada passo\n",
        "# em ordem invertida (G_i, para cada i de n a 0)\n",
        "i = len(trajectory)\n",
        "\n",
        "Gi = 0.0\n",
        "print(f\"G_{i} =\", Gi)\n",
        "\n",
        "for (s, a, r) in reversed(trajectory):\n",
        "    i = i - 1\n",
        "    Gi = r + GAMMA*Gi\n",
        "    print(f\"G_{i} =\", Gi)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1HfVS-Cykf8",
        "tags": []
      },
      "source": [
        "## 4 - Funções de Valor (para uma Política Dada)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LLTRUw3pZiN"
      },
      "source": [
        "Elas são usadas para definir o que é uma política \"melhor\" do que outra em MDP. E são usadas diretamente em alguns algoritmos (família *value-based*).\n",
        "\n",
        "Veremos dois tipos de função de valor. Ambas dão os valores *retornos esperados* (médios) **para uma política específica**!\n",
        "\n",
        "Elas são calculadas a partir dos retornos parciais, que vimos antes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uM4E0UTykf8"
      },
      "source": [
        "### 4.1 - Função de valor do estado $V(s)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c_TD7CWykf8"
      },
      "source": [
        "Esta função dá o retorno esperado a partir de cada estado $s$, para uma política específica. Aqui estamos pensando na *identidade* do estado $s$, independente de sua posição na trajetória.\n",
        "\n",
        "De forma matemática, ela é definida assim:\n",
        "\n",
        "$$V(s) = E[G_t | S_t=s, \\mathrm{\\text{t qualquer}}]$$\n",
        "\n",
        "Ou seja, $V(s)$ é o valor médio dos valores $G_t$ calculados a partir do estado $s$ quando ele aparece em uma posição $t$ qualquer do episódio. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUrzTFCuykf8"
      },
      "source": [
        "#### Algoritmo para Estimativa de Valor por Monte Carlo\n",
        "\n",
        "Um algoritmo para calcular o valor de $V(s)$ para um $s$ específico é dado a seguir. Ele é um algoritmo de Monte Carlo para o **problema de predição**, ou seja, para calcular a estimativa de uma função de valor.\n",
        "\n",
        "**Pseudocódigo**\n",
        "\n",
        "1. Inicialize o histórico de retornos para todos os estados.\n",
        "2. Repita \"muitas\" vezes:\n",
        "   - Rode um episódio seguindo a política.\n",
        "   - Para cada passo no episódio, começando do estado $s_i$:\n",
        "     - calcule o retorno parcial $G_i$.\n",
        "     - guarde $G_i$ no histórico de retornos para o estado $s_i$.\n",
        "3. Calcule a estimativa de valor para cada estado $s$:\n",
        "   - Para cada estado $s$:\n",
        "     - $V(s)$ = média dos retornos no histórico para o estado $s$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementação\n",
        "\n",
        "Implementamos esta ideia abaixo, guardando cada retorno em um dicionário `returns_history` (um histórico dos retornos) indexado pelo $s$ onde se originou o retorno parcial.\n",
        "\n",
        "Esta implementação assume ambiente de *estado discreto*, representados por inteiros iniciados em 0. \n",
        "- Exemplos de ambientes assim: *FrozenLake-v1* e *Taxi-v3*.\n",
        "\n",
        "Nestes casos, o $V$ pode ser representado como um array, e o estado serve como índice de onde está guardado o valor daquele estado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0DVMbbLPykf8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.03539695, 0.02521524, 0.04803949, 0.01579362, 0.04419733,\n",
              "       0.        , 0.09595079, 0.        , 0.10048972, 0.22846086,\n",
              "       0.27130909, 0.        , 0.        , 0.37533681, 0.64659988,\n",
              "       0.        ])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# associa cada estado a uma lista de retornos parciais\n",
        "# obtidos a partir do estado (em um episódio qualquer)\n",
        "returns_history = dict()\n",
        "\n",
        "# roda muitos episódios, com a política desejada\n",
        "for epi in range(20000):\n",
        "    trajectory = run_episode(env, policy_0)\n",
        "\n",
        "    # calcula os retornos a cada passo (G_i, para cada i de n a 0) do episódio\n",
        "    # guardando o valor em returns_history\n",
        "    Gi = 0.0\n",
        "    for (s, a, r) in reversed(trajectory):\n",
        "        Gi = r + GAMMA*Gi\n",
        "        if s not in returns_history.keys():\n",
        "            returns_history[s] = [ Gi ]\n",
        "        else:\n",
        "            returns_history[s].append(Gi)\n",
        "\n",
        "# associa cada estado à média dos retornos parciais\n",
        "V = np.zeros(env.observation_space.n)\n",
        "\n",
        "# calcula V\n",
        "for s in returns_history.keys():\n",
        "    V[s] = np.mean( returns_history[s] )\n",
        "\n",
        "V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Abaixo, redimensionamos o array para deixá-lo bidimensional. Assim, conseguimos comparar com a imagem do ambiente e entender as posições de maior retorno esperado, quando seguimos a política em questão. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0cWfYMnszyvZ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://gymnasium.farama.org/_images/frozen_lake.gif\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.03539695 0.02521524 0.04803949 0.01579362]\n",
            " [0.04419733 0.         0.09595079 0.        ]\n",
            " [0.10048972 0.22846086 0.27130909 0.        ]\n",
            " [0.         0.37533681 0.64659988 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# Mostra o mapa de FrozenLake\n",
        "display(Image(url=\"https://gymnasium.farama.org/_images/frozen_lake.gif\"))\n",
        "\n",
        "# Mostra a função V na mesma disposição do mapa do FrozenLake\n",
        "print(V.reshape(4,4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Rhfou4oykf8"
      },
      "source": [
        "### 4.2 - Função de valor da ação $Q(s,a)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpBcSZp7ykf-"
      },
      "source": [
        "O $Q(s,a)$ responde a esta pergunta:\n",
        "\n",
        "*Quando estava no estado* **s** *e fez a ação* **a** *, qual o retorno esperado (se continuar seguindo a política no restante do episódio)?*\n",
        "\n",
        "Assim, de maneira análoga ao $V(s)$, o $Q(s,a)$ representa o retorno esperado a partir do par $(s,a)$:\n",
        "\n",
        "$$Q(s,a) = E[G_t | S_t=s, A_t=a]$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfdH34tmFNJh"
      },
      "source": [
        "#### Algoritmo\n",
        "\n",
        "Existe um algoritmo de Monte-Carlo para calcular $Q(s,a)$, que é análogo ao anterior.\n",
        "\n",
        "---\n",
        "1. rode infinitos episódios com a política\n",
        "2. analise cada episódio, e a cada passo iniciado no estado $s_i$ seguido de uma ção $a_i$:\n",
        "   - calcule o retorno parcial $G_i$\n",
        "   - salve $G_i$ no histórico do par $(s_i, a_i)$\n",
        "3. Para cada par $(s,a)$:\n",
        "   - $Q(s, a)$ = média de todos os retornos do histórico do par $(s, a)$\n",
        "\n",
        "---\n",
        "\n",
        "O código é semelhante ao anterior. Porém, aqui, vamos usar um dicionário `returns_history` indexado pelo par $(s,a)$ onde se originou cada retorno $G_i$.\n",
        "\n",
        "Tente completar a implementação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4kpIxoxRykf-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# associa cada par (estado, ação) a uma lista de retornos parciais\n",
        "# obtidos em episódios quaisquer onde esse par apareceu\n",
        "returns_history = dict()\n",
        "\n",
        "# roda muitos episódios, com a política desejada\n",
        "for epi in range(20000):\n",
        "    trajectory = run_episode(env, policy_0)\n",
        "\n",
        "    # calcula os retornos a cada passo (G_i, para cada i de n a 0) do episódio\n",
        "    # guardando o valor em returns_history\n",
        "    Gi = 0.0\n",
        "    for (s, a, r) in reversed(trajectory):\n",
        "        # FAÇA: complete o código (no lugar do pass)...\n",
        "        pass\n",
        "\n",
        "# matriz para associar cada par (estado, ação) à média dos retornos parciais\n",
        "Qtable = np.zeros(shape=(env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# calcula Q\n",
        "# FAÇA: complete o código...\n",
        "\n",
        "Qtable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L4Wl3B3ykf-"
      },
      "source": [
        "Note que a função $Q$ calculado para estados discretos e ações discretas (ambos representados por inteiros) pode ser representada como uma matriz. Ela pode ser vista como uma _tabela_ com linhas representando os *estados* e colunas representando as *ações*.\n",
        "\n",
        "Por esse motivo, chamamos essa representação de **Q-table** (tabela-Q)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tguKyxggykgH"
      },
      "source": [
        "## 5 - Preparando para os Métodos Baseados em Q-table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XDgWo0kykgI"
      },
      "source": [
        "Agora, sim, podemos falar um pouquinho sobre os *algoritmos de controle* da aprendizagem por reforço, que são os algoritmos capazes de aprender políticas \"boas\", ou seja, políticas que dão altos retornos.\n",
        "\n",
        "Coloque-se no lugar do algoritmo e suponha que você inicie com uma política qualquer (provavelmente ruim) e que você tenha calculado o *Q* dessa política com uma Q-table.\n",
        "\n",
        "**De que forma você poderia melhorar a política olhando para os valores de Q?**\n",
        "\n",
        "**Ou, como escolher a melhor ação a cada estado, usando a Q-table?**\n",
        "\n",
        "---\n",
        "\n",
        "No próxima parte do curso, veremos um algoritmo para RL, também da família Monte-Carlo, onde a política é implicitamente representada pelo $Q$.\n",
        "\n",
        "Este método repete $N$ vezes esses passos:\n",
        "1. Rode um episódio, usando a política representada pela tabela $Q$\n",
        "   - salve a trajetória\n",
        "1. Depois, calcule os valores de $G_i$ e use esses valores para atualizar $Q$\n",
        "   - ao atualizar $Q$, a política eventualmente muda\n",
        "\n",
        "---\n",
        "\n",
        "Veremos mais detalhes no próximo capítulo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puFH282FykgI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "47acfd36b4a698d100796428813311ecacef03b489c77dd1fdf080373e214244"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
