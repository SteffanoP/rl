{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# PPO com Stable Baselines3\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap11/cap11-PPO-stablebaselines.ipynb)\n",
        "\n",
        "Vamos usar o algoritmo **PPO** (*Proximal Policy Optimization*) neste Google Colab.\n",
        "\n",
        "Referências:\n",
        "- Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "- Documentação: https://stable-baselines3.readthedocs.io/en/master/index.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "## 1 - Configurações necessárias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thRA4CT9kH4G"
      },
      "source": [
        "### 1.1 Instalação de pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from IPython.display import clear_output\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install \"stable-baselines3[extra]==2.1.0\"\n",
        "    !pip install swig\n",
        "    !pip install gymnasium[box2d]   # works with gymnasium version 0.28.1\n",
        "    !pip install gymnasium[atari,accept-rom-license]\n",
        "    #clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRWPtPCsKiRI"
      },
      "outputs": [],
      "source": [
        "!mkdir log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLlWktrRj9GZ"
      },
      "source": [
        "### 1.2 Para salvar vídeo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRNUfulOGaF"
      },
      "source": [
        "*Atenção*: Reinicie o ambiente de execução antes de continuar.\n",
        "\n",
        "A gravação é feita com o wrapper [VecVideoRecorder](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "  # Start the video at step=0 and record the given number of steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "### 1.3 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import tensorboard\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "import stable_baselines3\n",
        "stable_baselines3.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmKML-NNXibO"
      },
      "source": [
        "Abaixo, importamos a classe que representa o algoritmo **PPO** e diversas funções auxiliares."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgAXxClR0BfH"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# para treinar com ambientes Atari\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0_8OQbOTTNT"
      },
      "source": [
        "Abaixo, importamos a classe que vai representar a **rede neural da política**. No caso, importamos uma rede com camadas totalmente conectadas (MLP) e uma rede convolucional (CNN), com arquiteturas (camadas, etc) pré-definidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROUJr675TT01"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.ppo import MlpPolicy, CnnPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## 2 - Cria o ambiente e instancia o agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0wyWYEKa5xo"
      },
      "source": [
        "Estamos criando o ambiente e passando para a classe **PPO**, que permite testar ou treinar o agente.\n",
        "\n",
        "Abaixo, criamos o PPO com os valores default dos seus parâmetros mais importantes. Definimos apenas estes:\n",
        "- `MlpPolicy`, para usar uma MLP com arquitetura pré-definida; para personalizar a arquitetura [veja aqui](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html)\n",
        "- `tensorboard_log`, que indica o diretório dos logs\n",
        "- `verbose=0` para evitar mostrar muitas mensagens.\n",
        "- para mais informações, veja [a documentação](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUWGZp3i9wyf"
      },
      "outputs": [],
      "source": [
        "# escolha um ID de ambiente do gymnasium\n",
        "#ENV_NAME = \"LunarLander-v2\"\n",
        "ENV_NAME = \"BipedalWalker-v3\"\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "model = PPO(MlpPolicy, env, tensorboard_log=\"log_dir\", verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAM4wBawmNex"
      },
      "source": [
        "Para rodar com **jogos de Atari**, descomente o código abaixo.\n",
        "\n",
        "*Observação*: Talvez você não consiga usar a função para gerar os vídeos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Wdw9LzwaFeN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# escolha um ID de ambiente correspondente a um game de Atari\n",
        "# veja em https://gymnasium.farama.org/environments/atari/\n",
        "ENV_NAME = 'PongNoFrameskip-v4' #'ALE/Pong-v5' \n",
        "\n",
        "# Esta função cria o ambiente, junto com wrappers especializados\n",
        "env = make_atari_env(ENV_NAME, n_envs=4, seed=0)\n",
        "\n",
        "# Wrapper para fazer cada estado ser uma \"pilha\" de 4 frames\n",
        "env = VecFrameStack(env, n_stack=4)\n",
        "\n",
        "# Cria com uma rede convolucional como política\n",
        "model = PPO(CnnPolicy, env, tensorboard_log=\"log_dir\", verbose=1)\n",
        "#'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEVOIY8NVeK"
      },
      "source": [
        "Vamos avaliar o agente não-treinado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDHLMA6NFk95"
      },
      "outputs": [],
      "source": [
        "# Random Agent, before training\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=30)\n",
        "\n",
        "print(f\"Retorno médio: {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTbAq8Ji_EBf"
      },
      "source": [
        "E agora vamos gravar um vídeo para vê-lo em ação *antes* de treinado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKo8i2aw_Iz5"
      },
      "outputs": [],
      "source": [
        "record_video(ENV_NAME, model, video_length=1_000, prefix='ppo-sem-treino')\n",
        "clear_output()\n",
        "show_videos('videos', prefix='ppo-sem-treino')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A00W6yY3NkHG"
      },
      "source": [
        "## 3 - Ativa Tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0ZVznfSt_AL"
      },
      "source": [
        "O comando abaixo abre o **Tensorboard**. Depois, execute o treinamento e acompanhe aqui os resultados, em tempo real."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MkjYOgx3itp"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## 4 - Treina e salva o agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT-lCHMOAHR3"
      },
      "source": [
        "Abaixo, rodamos a função de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4cfSXIB-pTF"
      },
      "outputs": [],
      "source": [
        "# Treina por 200 mil passos -- pode demorar mais de 5 min!\n",
        "model.learn(total_timesteps=200_000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF5htH8TAK8m"
      },
      "source": [
        "Depois, gravamos o vídeo do agente treinado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olb37dOk_rgz"
      },
      "outputs": [],
      "source": [
        "record_video(ENV_NAME, model, video_length=1_500, prefix='ppo-treinado')\n",
        "clear_output()\n",
        "show_videos('videos', prefix='ppo-treinado')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN7XVTupAN-g"
      },
      "source": [
        "Por fim, salvamos o agente treinado em arquivo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygl_gVmV_QP7"
      },
      "outputs": [],
      "source": [
        "model.save(\"ppo_model\")\n",
        "del model  # delete trained model from memory to demonstrate loading\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwbRZv1yiUO3"
      },
      "source": [
        "## 5 - Carrega e avalia o agente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INlUVon9iflA"
      },
      "outputs": [],
      "source": [
        "model = PPO.load(\"ppo_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SoCtCVAiX0O"
      },
      "outputs": [],
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=30)\n",
        "\n",
        "print(f\"Retorno médio: {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LYtT21aOs1l"
      },
      "source": [
        "## 6 - Treina por mais alguns passos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAdGk2ZFk0a2"
      },
      "source": [
        "Para treinar por mais alguns passos, use `.set_env()` para definir o ambiente e chame novamente `.learn()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfv5y7qmkvRT"
      },
      "outputs": [],
      "source": [
        "model.set_env(gym.make(ENV_NAME))\n",
        "model.learn(total_timesteps=100_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC5yuXefO2Nx"
      },
      "outputs": [],
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=30)\n",
        "print(f\"Retorno médio: {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "record_video(ENV_NAME, model, video_length=1_500, prefix='ppo-treinado')\n",
        "clear_output()\n",
        "show_videos('videos', prefix='ppo-treinado')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FtY8FhliLsGm",
        "xVm9QPNVwKXN"
      ],
      "name": "cap11-PPO-stablebaselines.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('rlx')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "27dbc9ce4cc602e4f15257b7b0018d8dff5b9ce9a7d73bc4399cb5afb1e02c4a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
