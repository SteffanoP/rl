{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O58-wFDPykfr"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap04/cap04-main.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FJLl2gwEykfx",
        "tags": []
      },
      "source": [
        "# Capítulo 4 - Do Monte Carlo ao Q-Learning\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qs9DpIGJ5gBE"
      },
      "source": [
        "Aqui, veremos dois algoritmos de tipos distintos para o problema de *controle* da aprendizagem por reforço -- ou seja, o problema de aprender a política ótima.\n",
        "\n",
        "O principal que eles têm em comum, é o fato de que **eles se baseiam na função de valor da ação `Q(s,a)`**. Esta função é calculada e usada durante a execução de ambos os algoritmos."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGBz5-DdJ-SG"
      },
      "source": [
        "Relembrando, o $Q(s,a)$ ele é o valor médio das recompensas parciais a partir do par $(s,a)$:\n",
        "\n",
        "$$Q(s,a) = E[G_t | S_t=s, A_t=a]$$\n",
        "\n",
        "De maneira informa, ele responde a esta pergunta:\n",
        "\n",
        "*Quando estava no estado* **s** *e fez a ação* **a** *, qual o retorno esperado (se continuar seguindo a política no restante do episódio)?*\n",
        "\n",
        "Vamos ver como usar esta informação para fazer a escolha de boas ações!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKTfNBr1y_vY"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    !pip install gym==0.23.1\n",
        "    # clone repository\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "    clear_output()\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqr4tGCgykfy"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from util.plot import plot_result\n",
        "from util.experiments import test_greedy_Q_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBf4CB5Lykf0",
        "outputId": "212de173-0842-49d5-9472-bb14544baddd"
      },
      "outputs": [],
      "source": [
        "# vamos focar nesses três ambientes por serem mais simples\n",
        "# ver mais em: https://www.gymlibrary.dev/\n",
        "#env = gym.make(\"Taxi-v3\")\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "#env = gym.make(\"Blackjack-v1\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2JIcbuGcykf1",
        "tags": []
      },
      "source": [
        "## 1 - Algoritmo (de Controle) de Monte Carlo - Versão 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gUwD8YVoykf1"
      },
      "source": [
        "Este algoritmo roda vários episódios, fazendo estes passos a cada episódio:\n",
        "\n",
        "1. Gera a **trajetória** completa (sequência de estados/observações, ações e recompensas) do episódio:\n",
        "\n",
        "    $S_0 \\rightarrow A_0 \\rightarrow R_1 \\rightarrow S_1 \\rightarrow A_1 \\rightarrow R_2 \\rightarrow S_2 \\rightarrow \\cdots S_{n-1} \\rightarrow A_{n-1} \\rightarrow R_T \\rightarrow S_T$\n",
        "\n",
        "1. Para escolher a ação $a$, a ser realizada em um estado $s$, ele usa a tabela $Q(s,a)$ com alguma estratégia de exploração -- vamos usar $\\epsilon$-greedy. Assim, ele escolhe dessa forma:\n",
        "   - com probabilidade $\\epsilon$, ele escolhe uma ação $a$ qualquer\n",
        "   - com probabilidade $1-\\epsilon$, ele escolhe a melhor ação, ou seja, $\\max_a{Q(s,a)}$\n",
        "\n",
        "1. Ao fim do episódio, para cada para intermediário ($S_t$, $A_t$) da trajetória, ele:\n",
        "   - calcula o retorno parcial $G_t$.\n",
        "   - usa $G_t$ para atualizar $Q(S_t, A_t)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdB0uVyZDFXh"
      },
      "outputs": [],
      "source": [
        "# Esta é a política. Neste caso, escolhe uma ação com base nos valores\n",
        "# da tabela Q, usando uma estratégia epsilon-greedy.\n",
        "def choose_action_1(Q, state, num_actions, epsilon):\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(0, num_actions)\n",
        "    else:\n",
        "        return np.argmax(Q[state])   # alt. para aleatorizar empates: np.random.choice(np.where(b == bmax)[0])\n",
        "\n",
        "\n",
        "# Algoritmo Monte-Carlo de Controle, variante \"toda-visita\".\n",
        "# Atenção: os espaços de estados e de ações precisam ser discretos, dados por valores inteiros\n",
        "def run_montecarlo1(env, episodes, gamma=0.95, epsilon=0.1, render=False):\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # dicionário com todos os retornos descontados, para cada par (estado,ação)\n",
        "    returns_history = dict()\n",
        "\n",
        "    # inicializa a tabela Q toda com zero,\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.zeros(shape = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-discontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "        ep_trajectory = []\n",
        "\n",
        "        state = env.reset()\n",
        "\n",
        "        # [1] Executa um episódio completo, salvando a trajetória\n",
        "        while not done:\n",
        "            # exibe/renderiza os passos no ambiente, durante 1 episódio a cada mil e também nos últimos 5 episódios\n",
        "            if render and (i >= (episodes - 5) or (i+1) % 1000 == 0):\n",
        "                env.render()\n",
        "\n",
        "            # [2] Escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = choose_action_1(Q, state, num_actions, epsilon)\n",
        "\n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # adiciona a tripla que representa este passo\n",
        "            ep_trajectory.append( (state, action, reward) )\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso\n",
        "        if (i+1) % 100 == 0:\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "        # [3] Calcula os retornos parciais e atualiza Q\n",
        "        Gt = 0\n",
        "        for (s, a, r) in reversed(ep_trajectory):\n",
        "            Gt = r + gamma*Gt\n",
        "\n",
        "            if returns_history.get((s,a)) is None:\n",
        "                returns_history[s,a] = [ Gt ]\n",
        "            else:\n",
        "                returns_history[s,a].append(Gt)\n",
        "\n",
        "            # média entre todas as ocorrências de (s,a) encontradas nos episódios\n",
        "            Q[s,a] = np.mean(returns_history[s,a]) # LENTO! -> vamos melhorar\n",
        "            # cálculo alternativo da média: M = M + (1/n)*(x - M)\n",
        "\n",
        "    return sum_rewards_per_ep, Q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0Gv5YNiHykf2",
        "outputId": "69d7b95d-42df-4bca-f020-01307776cbdb"
      },
      "outputs": [],
      "source": [
        "%timeit\n",
        "ENV_NAME = \"Taxi-v3\"\n",
        "#ENV_NAME = \"Blackjack-v1\"\n",
        "r_max_plot = 10\n",
        "\n",
        "EPISODES = 3000\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.1\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "# Roda o algoritmo Monte-Carlo para o problema de controle (ou seja, para achar a política ótima)\n",
        "rewards, Qtable = run_montecarlo1(env, EPISODES, GAMMA, EPSILON, render=False)\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))\n",
        "\n",
        "# Mostra um gráfico de episódios x retornos (não descontados)\n",
        "# Se quiser salvar, passe o nome do arquivo no 3o parâmetro\n",
        "filename = f\"results/montecarlo1-{ENV_NAME.lower()[0:8]}-ep{EPISODES}.png\"\n",
        "plot_result(rewards, r_max_plot, None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TmkT_Si2K0cX"
      },
      "source": [
        "## 2 - Algoritmo (de Controle) de Monte Carlo - Versão 2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mL9Fgp3lK4HW"
      },
      "source": [
        "Modifique o código acima com essas características:\n",
        "1. Mantenha o uso de `choose_action_1()` para escolher a ação.\n",
        "1. Remova o **histórico** de retornos parciais.\n",
        "1. Use uma **taxa de aprendizagem**, representada pelo parâmetro `lr`.\n",
        "\n",
        "Faça as melhorias abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gkAF9lnLEsu"
      },
      "outputs": [],
      "source": [
        "# Algoritmo Monte-Carlo de Controle, variante \"toda-visita\".\n",
        "# Atenção: os espaços de estados e de ações precisam ser discretos, dados por valores inteiros\n",
        "def run_montecarlo2(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1, render=False):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # inicializa a tabela Q toda com zero,\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.zeros(shape = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-discontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-discontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "        ep_trajectory = []\n",
        "\n",
        "        state = env.reset()\n",
        "\n",
        "        # [1] Executa um episódio completo, salvando a trajetória\n",
        "        while not done:\n",
        "            # exibe/renderiza os passos no ambiente, durante 1 episódio a cada mil e também nos últimos 5 episódios\n",
        "            if render and (i >= (episodes - 5) or (i+1) % 1000 == 0):\n",
        "                env.render()\n",
        "\n",
        "            # [2] Escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = choose_action_1(Q, state, num_actions, epsilon)\n",
        "\n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # adiciona a tripla que representa este passo\n",
        "            ep_trajectory.append( (state, action, reward) )\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso\n",
        "        if (i+1) % 100 == 0:\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "        # [3] Calcula os retornos parciais e atualiza Q\n",
        "        Gt = 0\n",
        "        for (s, a, r) in reversed(ep_trajectory):\n",
        "            Gt = r + gamma*Gt\n",
        "            # FAÇA AQUI A MODIFICAÇÂO PEDIDA !!!\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_LmN99MOLRE7"
      },
      "source": [
        "Quando o código estiver pronto, você poderá testar rodando o código abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CYpmGrpLWoU"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"FrozenLake-v1\" #\"Taxi-v3\"\n",
        "r_max_plot = 1.0\n",
        "\n",
        "EPISODES = 20000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.1\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "# Roda o algoritmo Monte-Carlo para o problema de controle (ou seja, para achar a política ótima)\n",
        "rewards, Qtable = run_montecarlo2(env, EPISODES, LR, GAMMA, EPSILON, render=False)\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))\n",
        "\n",
        "# Mostra um gráfico de episódios x retornos não descontados\n",
        "# Se quiser salvar, passe o nome do arquivo no 3o parâmetro\n",
        "#filename = f\"results/montecarlo2-{ENV_NAME.lower()[0:8]}-ep{EPISODES}.png\"\n",
        "plot_result(rewards, r_max_plot, None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WxZMi3mIDfwe"
      },
      "source": [
        "## 3 - Algoritmo Q-Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LgVMSIOcykf4"
      },
      "source": [
        "Explicar um pouco aqui..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXZzLQnKIhRO"
      },
      "outputs": [],
      "source": [
        "# Esta é a política. Neste caso, escolhe uma ação com base nos valores\n",
        "# da tabela Q, usando uma estratégia epsilon-greedy.\n",
        "def choose_action_2(Q, state, num_actions, epsilon):\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(0, num_actions)\n",
        "    else:\n",
        "        # melhoria: se houver empate, desempata aleatoriamente\n",
        "        max_q = np.max(Q[state])\n",
        "        return np.random.choice(np.where(Q[state] == max_q)[0])\n",
        "\n",
        "\n",
        "# Algoritmo Q-learning, online learning (TD-learning)\n",
        "# Atenção: os espaços de estados e de ações precisam ser discretos, dados por valores inteiros\n",
        "def run_qlearning(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1, render=False):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # inicializa a tabela Q com valores aleatórios de -1.0 a 0.0\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.random.uniform(low = -1.0, high = 0.0,\n",
        "                          size = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-discontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "\n",
        "        state = env.reset()\n",
        "\n",
        "        # executa um episódio completo, fazendo atualizações na Q-table\n",
        "        while not done:\n",
        "            # exibe/renderiza os passos no ambiente, durante 1 episódio a cada mil e também nos últimos 5 episódios\n",
        "            if render and (i >= (episodes - 5) or (i+1) % 1000 == 0):\n",
        "                env.render()\n",
        "\n",
        "            # escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = choose_action_2(Q, state, num_actions, epsilon)\n",
        "\n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                # para estados terminais\n",
        "                V_next_state = 0\n",
        "            else:\n",
        "                # para estados não-terminais -- valor máximo (melhor ação)\n",
        "                V_next_state = np.max(Q[next_state])\n",
        "\n",
        "            # atualiza a Q-table\n",
        "            # delta = (estimativa usando a nova recompensa) - estimativa antiga\n",
        "            delta = (reward + gamma * V_next_state) - Q[state,action]\n",
        "            Q[state,action] = Q[state,action] + lr * delta\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        #epsilon = np.exp(-0.005*i)\n",
        "\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso\n",
        "        if (i+1) % 100 == 0:\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wd4lJnxIevG"
      },
      "source": [
        "Agora, vamos testar o algoritmo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EDPXQANOykf4",
        "outputId": "8cc9a939-897a-4157-fbcb-5f81d154ab32"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"Taxi-v3\"\n",
        "r_max_plot = 10\n",
        "\n",
        "EPISODES = 20000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.1\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "# Roda o algoritmo Q-Learning\n",
        "rewards, Qtable = run_qlearning(env, EPISODES, LR, GAMMA, EPSILON, render=False)\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))\n",
        "\n",
        "# Mostra um gráfico de episódios x retornos não descontados\n",
        "# Se quiser salvar, passe o nome do arquivo no 3o parâmetro\n",
        "#filename = f\"results/qlearning-{ENV_NAME.lower()[0:8]}-ep{EPISODES}-lr{LR}.png\"\n",
        "plot_result(rewards, r_max_plot, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgvjeQlTUnoI"
      },
      "outputs": [],
      "source": [
        "test_greedy_Q_policy(env, Qtable, 10, False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KBDUUUPcykf5",
        "tags": []
      },
      "source": [
        "## 4 - Experimentos Finais\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDGW2aBUM0Pa"
      },
      "outputs": [],
      "source": [
        "from util.experiments import repeated_exec\n",
        "from util.plot import plot_multiple_results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2OOoTRBhJsr0"
      },
      "source": [
        "Vamos comparar os algoritmos aqui propostos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "puFH282FykgI",
        "outputId": "b1c9c759-cf5f-47a8-b119-360b62193138"
      },
      "outputs": [],
      "source": [
        "NUM_EPISODES = 12000\n",
        "\n",
        "enviroment = gym.make(\"Taxi-v3\")\n",
        "#enviroment = gym.make(\"FrozenLake-v1\")\n",
        "\n",
        "results = []\n",
        "# muito lento!\n",
        "results.append( repeated_exec(1, \"Monte-Carlo1\", run_montecarlo1, enviroment, NUM_EPISODES) )\n",
        "\n",
        "#for learning_rate in [0.01, 0.1, 0.5]:\n",
        "#    results.append( repeated_exec(1, f\"Monte-Carlo2 (LR={learning_rate})\", run_montecarlo2, enviroment, NUM_EPISODES, learning_rate) )\n",
        "\n",
        "for learning_rate in [0.05, 0.1, 0.5, 1.0]:\n",
        "    results.append( repeated_exec(1, f\"Q-Learning (LR={learning_rate})\", run_qlearning, enviroment, NUM_EPISODES, learning_rate) )\n",
        "\n",
        "clear_output()\n",
        "\n",
        "plot_multiple_results(results, cumulative=False, x_log_scale=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "47acfd36b4a698d100796428813311ecacef03b489c77dd1fdf080373e214244"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
