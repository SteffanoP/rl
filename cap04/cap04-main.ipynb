{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O58-wFDPykfr"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap04/cap04-main.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FJLl2gwEykfx",
        "tags": []
      },
      "source": [
        "# Capítulo 4 - Funções de Valor e Algoritmos de Monte Carlo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKTfNBr1y_vY"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # for saving videos\n",
        "    !apt-get install ffmpeg freeglut3-dev xvfb\n",
        "\n",
        "    !pip install gym==0.23.1\n",
        "\n",
        "    # clone repository\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "\n",
        "    clear_output()\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-KQrOjJr5yQ"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    # Set up fake display; otherwise rendering will fail\n",
        "    import os\n",
        "    os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "    os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "    from util.notebook import display_videos_from_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqr4tGCgykfy"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from util.plot import plot_result\n",
        "from util.experiments import test_greedy_Q_policy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JX23n7wir5yT"
      },
      "source": [
        "## 1 - Introdução"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "__sZSN5vr5yT"
      },
      "source": [
        "Vamos relembrar alguns conceitos da aula passada que serão muito úteis nesta aula.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YH3TWCNtr5yV"
      },
      "source": [
        "### 1.1 Retornos Parciais Descontados"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OmIibJQyr5yW"
      },
      "source": [
        "Os retornos parciais são calculados a partir de um passo $t$ qualquer da trajetória:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "   && &\\quad G_0     &= &\\;\\;R_1 &+ \\gamma &R_2 &+ \\gamma^2 &R_3 &+ \\gamma^3 &R_4 &+ \\gamma^4 &R_5 &+ \\;\\cdots \\;&+ \\gamma^{T-1} &R_T & \\;\\;\\;(= G) \\\\\n",
        "   && &\\quad G_1     &= &        &         &R_2 &+ \\gamma   &R_3 &+ \\gamma^2 &R_4 &+ \\gamma^3 &R_5 &+ \\;\\cdots \\;&+ \\gamma^{T-2} &R_T &       \\\\\n",
        "   && &\\quad G_2     &= &        &         &    &           &R_3 &+ \\gamma   &R_4 &+ \\gamma^2 &R_5 &+ \\;\\cdots \\;&+ \\gamma^{T-3} &R_T &       \\\\\n",
        "   && &\\quad         &\\cdots &   & & & & & & & & & & &    & \\\\\n",
        "   && &\\quad G_T     &= &        & & & & & & & & & & &0   & \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Lembrando que, para $t < T$ qualquer, observamos esta relação:\n",
        "\n",
        "$$\n",
        "   G_{t} = R_{t+1} + \\gamma G_{t+1}\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "snalEnror5yY"
      },
      "source": [
        "### 1.2 - Função de valor do estado $V(s)$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3BzcaPoDr5yY"
      },
      "source": [
        "Esta função dá o retorno parcial esperado a partir de cada estado $s$, para uma política específica.\n",
        "\n",
        "De forma matemática, ela é definida assim:\n",
        "\n",
        "$$V(s) = E[G_t | S_t=s]$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RM36lLzqr5yZ"
      },
      "source": [
        "### 1.3 - Função de valor da ação $Q(s,a)$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hu_3nT7cr5yZ"
      },
      "source": [
        "Esta é a função mais importante para esta aula.\n",
        "\n",
        "O $Q(s,a)$ dá o valor médio do retorno parcial a partir do par estado-ação $(s,a)$:\n",
        "\n",
        "$$Q(s,a) = E[G_t | S_t=s, A_t=a]$$\n",
        "\n",
        "De maneira informa, ele responde a esta pergunta:\n",
        "\n",
        "*Quando estava no estado* **s** *e fez a ação* **a** *, qual o retorno esperado (se continuar seguindo a mesma política)?*\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kbs4mVmYr5ya"
      },
      "source": [
        "### 1.4 Algoritmos de Monte Carlo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qs9DpIGJ5gBE"
      },
      "source": [
        "***O que são algoritmos de Monte Carlo?***\n",
        "\n",
        " - Utilizam amostragem aleatória para resolver problemas ou estimar quantidades que são difíceis de calcular diretamente.\n",
        " - Envolvem a geração de um grande número de amostras ou simulações aleatórias para tirar conclusões ou fazer previsões.\n",
        " - Usados em várias áreas (não só na Aprendizagem por Reforço)\n",
        "\n",
        "Vimos, antes, dois algoritmos de Monte Carlo para o problema de **predição** da aprendizagem por reforço.\n",
        "- Para aprender os valores de $V(s)$ e de $Q(s,a)$.\n",
        "\n",
        "Aqui, veremos algoritmos de Monte Carlo para o problema de **controle** da aprendizagem por reforço.\n",
        "- Para aprender a política ótima!\n",
        "\n",
        "Os algoritmos de controle que veremos se baseiam na **função de valor da ação `Q(s,a)`**."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2JIcbuGcykf1",
        "tags": []
      },
      "source": [
        "## 2 - Algoritmo (de Controle) de Monte Carlo - Versão 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gUwD8YVoykf1"
      },
      "source": [
        "Este algoritmo roda vários episódios, fazendo estes passos a cada episódio:\n",
        "\n",
        "1. Gera a **trajetória** completa (sequência de estados/observações, ações e recompensas) do episódio:\n",
        "\n",
        "    $S_0 \\rightarrow A_0 \\rightarrow R_1 \\rightarrow S_1 \\rightarrow A_1 \\rightarrow R_2 \\rightarrow S_2 \\rightarrow \\cdots S_{n-1} \\rightarrow A_{n-1} \\rightarrow R_T \\rightarrow S_T$\n",
        "\n",
        "1. Para escolher a ação $a$, a ser realizada em um estado $s$, ele usa a tabela $Q(s,a)$ com alguma estratégia de exploração -- vamos usar $\\epsilon$-greedy. Assim, ele escolhe dessa forma:\n",
        "   - com probabilidade $\\epsilon$, ele escolhe uma ação $a$ qualquer\n",
        "   - com probabilidade $1-\\epsilon$, ele escolhe a melhor ação, ou seja, $\\max_a{Q(s,a)}$\n",
        "\n",
        "1. Ao fim do episódio, para cada par intermediário ($S_t$, $A_t$) da trajetória, ele:\n",
        "   - calcula o retorno parcial $G_t$.\n",
        "   - usa $G_t$ para atualizar $Q(S_t, A_t)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjM4FQh4r5yb"
      },
      "outputs": [],
      "source": [
        "# Esta é a política. Neste caso, escolhe uma ação com base nos valores\n",
        "# da tabela Q, usando uma estratégia epsilon-greedy.\n",
        "def choose_action_1(Q, state, num_actions, epsilon):\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(0, num_actions)\n",
        "    else:\n",
        "        return np.argmax(Q[state])   # alt. para aleatorizar empates: np.random.choice(np.where(b == bmax)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU9qInCWr5yc"
      },
      "outputs": [],
      "source": [
        "def generate_and_show_video(env, Q):\n",
        "    num_actions = env.action_space.n\n",
        "    video_path = '/content/videos'\n",
        "    # roda dois episódios, gravando o resultado\n",
        "    rec_env = gym.wrappers.RecordVideo(env, video_path)\n",
        "    for _ in range(2):\n",
        "      state = rec_env.reset()\n",
        "      done = False\n",
        "      while not done:\n",
        "          action = choose_action_1(Q, state, num_actions, 0.0)\n",
        "          next_state, reward, done, _ = rec_env.step(action)\n",
        "    display_videos_from_path(video_path)\n",
        "    rec_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdB0uVyZDFXh"
      },
      "outputs": [],
      "source": [
        "# Algoritmo Monte-Carlo de Controle, variante \"toda-visita\".\n",
        "# Atenção: os espaços de estados e de ações precisam ser discretos, dados por valores inteiros\n",
        "def run_montecarlo1(env, episodes, gamma=0.95, epsilon=0.1, render=False):\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # dicionário com todos os retornos descontados, para cada par (estado,ação)\n",
        "    returns_history = dict()\n",
        "\n",
        "    # inicializa a tabela Q toda com zero,\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.zeros(shape = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "        ep_trajectory = []\n",
        "\n",
        "        state = env.reset()\n",
        "\n",
        "        # [1] Executa um episódio completo, salvando a trajetória\n",
        "        while not done:\n",
        "            # exibe/renderiza os passos no ambiente, durante 1 episódio a cada mil e também nos últimos 5 episódios\n",
        "            if render and (i >= (episodes - 5) or (i+1) % 1000 == 0) and not IN_COLAB:\n",
        "                env.render()\n",
        "\n",
        "            # [2] Escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = choose_action_1(Q, state, num_actions, epsilon)\n",
        "\n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # adiciona a tripla que representa este passo\n",
        "            ep_trajectory.append( (state, action, reward) )\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso\n",
        "        if (i+1) % 100 == 0:\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "        # [3] Calcula os retornos parciais e atualiza Q\n",
        "        Gt = 0\n",
        "        for (s, a, r) in reversed(ep_trajectory):\n",
        "            Gt = r + gamma*Gt\n",
        "\n",
        "            if returns_history.get((s,a)) is None:\n",
        "                returns_history[s,a] = [ Gt ]\n",
        "            else:\n",
        "                returns_history[s,a].append(Gt)\n",
        "\n",
        "            # média entre todas as ocorrências de (s,a) encontradas nos episódios\n",
        "            Q[s,a] = np.mean(returns_history[s,a]) # LENTO!\n",
        "            # IDEIA: usar cálculo alternativo da média: M = M + (1/n)*(x - M)\n",
        "\n",
        "    # para gerar um vídeo no Colab\n",
        "    if render and IN_COLAB:\n",
        "        clear_output()\n",
        "        generate_and_show_video(env, Q)\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gv5YNiHykf2"
      },
      "outputs": [],
      "source": [
        "%timeit\n",
        "# estes dois ambientes possuem estados e ações discretas\n",
        "# ver mais em: https://www.gymlibrary.dev/\n",
        "#ENV_NAME = \"Taxi-v3\"\n",
        "ENV_NAME = \"FrozenLake-v1\"\n",
        "\n",
        "# usar 10 para Taxi e 1.0 para FrozenLake\n",
        "r_max_plot = 10.0\n",
        "\n",
        "EPISODES = 1000\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.1\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "# Roda o algoritmo Monte-Carlo para o problema de controle (ou seja, para achar a política ótima)\n",
        "rewards, Qtable = run_montecarlo1(env, EPISODES, GAMMA, EPSILON, render=True)\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDx0xT_ssjS7"
      },
      "outputs": [],
      "source": [
        "# Mostra um gráfico de episódios x retornos não descontados\n",
        "# Se quiser salvar, passe o nome do arquivo no 3o parâmetro\n",
        "filename = f\"results/montecarlo1-{ENV_NAME.lower()[0:8]}-ep{EPISODES}.png\"\n",
        "plot_result(rewards, r_max_plot, None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TmkT_Si2K0cX"
      },
      "source": [
        "## 3 - Algoritmo (de Controle) de Monte Carlo - Versão 2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mL9Fgp3lK4HW"
      },
      "source": [
        "Modifique o código acima com essas características:\n",
        "1. Mantenha o uso de `choose_action_1()` para escolher a ação.\n",
        "1. Remova o **histórico** de retornos parciais.\n",
        "1. Use uma **taxa de aprendizagem**, representada pelo parâmetro `lr`.\n",
        "\n",
        "Faça as melhorias abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gkAF9lnLEsu"
      },
      "outputs": [],
      "source": [
        "# Algoritmo Monte-Carlo de Controle, variante \"toda-visita\".\n",
        "# Atenção: os espaços de estados e de ações precisam ser discretos, dados por valores inteiros\n",
        "def run_montecarlo2(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1, render=False):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # inicializa a tabela Q toda com zero,\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.zeros(shape = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "        ep_trajectory = []\n",
        "\n",
        "        state = env.reset()\n",
        "\n",
        "        # [1] Executa um episódio completo, salvando a trajetória\n",
        "        while not done:\n",
        "            # exibe/renderiza os passos no ambiente, durante 1 episódio a cada mil e também nos últimos 5 episódios\n",
        "            if render and (i >= (episodes - 5) or (i+1) % 1000 == 0) and not IN_COLAB:\n",
        "                env.render()\n",
        "\n",
        "            # [2] Escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = choose_action_1(Q, state, num_actions, epsilon)\n",
        "\n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # adiciona a tripla que representa este passo\n",
        "            ep_trajectory.append( (state, action, reward) )\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso\n",
        "        if (i+1) % 100 == 0:\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "        # [3] Calcula os retornos parciais e atualiza Q\n",
        "        Gt = 0\n",
        "        for (s, a, r) in reversed(ep_trajectory):\n",
        "            Gt = r + gamma*Gt\n",
        "            # FAÇA AQUI A MODIFICAÇÂO PEDIDA !!!\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_LmN99MOLRE7"
      },
      "source": [
        "Quando o código estiver pronto, você poderá testar rodando o código abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CYpmGrpLWoU"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"Taxi-v3\"\n",
        "#ENV_NAME = \"FrozenLake-v1\"\n",
        "\n",
        "r_max_plot = 10.0\n",
        "\n",
        "EPISODES = 20000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.1\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "# Roda o algoritmo Monte-Carlo para o problema de controle (ou seja, para achar a política ótima)\n",
        "rewards, Qtable = run_montecarlo2(env, EPISODES, LR, GAMMA, EPSILON, render=True)\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards[-20:]), \", desvio padrao =\", np.std(rewards[-20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNQ6SussthmA"
      },
      "outputs": [],
      "source": [
        "# Mostra um gráfico de episódios x retornos não descontados\n",
        "# Se quiser salvar, passe o nome do arquivo no 3o parâmetro\n",
        "#filename = f\"results/montecarlo2-{ENV_NAME.lower()[0:8]}-ep{EPISODES}.png\"\n",
        "plot_result(rewards, r_max_plot, None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KBDUUUPcykf5",
        "tags": []
      },
      "source": [
        "## 4 - Experimentos Finais\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDGW2aBUM0Pa"
      },
      "outputs": [],
      "source": [
        "from util.experiments import repeated_exec\n",
        "from util.plot import plot_multiple_results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2OOoTRBhJsr0"
      },
      "source": [
        "Vamos comparar os algoritmos aqui propostos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puFH282FykgI"
      },
      "outputs": [],
      "source": [
        "NUM_EPISODES = 5000\n",
        "\n",
        "#enviroment = gym.make(\"Taxi-v3\")\n",
        "enviroment = gym.make(\"FrozenLake-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "# muito lento, se usar o histórico completo!\n",
        "#results.append( repeated_exec(1, \"Monte-Carlo1\", run_montecarlo1, enviroment, NUM_EPISODES) )\n",
        "\n",
        "for learning_rate in [0.01, 0.1, 0.5]:\n",
        "    results.append( repeated_exec(10, f\"Monte-Carlo2 (LR={learning_rate})\", run_montecarlo2, enviroment, NUM_EPISODES, learning_rate) )\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYJXr1YR2atg"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results, cumulative=False, x_log_scale=False, window=50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "47acfd36b4a698d100796428813311ecacef03b489c77dd1fdf080373e214244"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
