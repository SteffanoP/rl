{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJLl2gwEykfx",
        "tags": []
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap04/cap04-main.ipynb)\n",
        "\n",
        "# Capítulo 4 - Funções de Valor e Algoritmos de Monte Carlo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKTfNBr1y_vY"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # for saving videos\n",
        "    !apt-get install ffmpeg\n",
        "\n",
        "    !pip install gymnasium   # conferir se precisa\n",
        "\n",
        "    # clone repository\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "\n",
        "    clear_output()\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqr4tGCgykfy"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "from util.plot import plot_result\n",
        "from util.qtable_helper import record_video_qtable, evaluate_qtable\n",
        "from util.notebook import display_videos_from_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX23n7wir5yT"
      },
      "source": [
        "## 1 - Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__sZSN5vr5yT"
      },
      "source": [
        "Vamos relembrar alguns conceitos da aula passada que serão muito úteis nesta aula.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH3TWCNtr5yV"
      },
      "source": [
        "### 1.1 Retornos Parciais Descontados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmIibJQyr5yW"
      },
      "source": [
        "Os retornos parciais são calculados a partir de um passo $t$ qualquer da trajetória:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "   && &\\quad G_0     &= &\\;\\;R_1 &+ \\gamma &R_2 &+ \\gamma^2 &R_3 &+ \\gamma^3 &R_4 &+ \\gamma^4 &R_5 &+ \\;\\cdots \\;&+ \\gamma^{T-1} &R_T & \\;\\;\\;(= G) \\\\\n",
        "   && &\\quad G_1     &= &        &         &R_2 &+ \\gamma   &R_3 &+ \\gamma^2 &R_4 &+ \\gamma^3 &R_5 &+ \\;\\cdots \\;&+ \\gamma^{T-2} &R_T &       \\\\\n",
        "   && &\\quad G_2     &= &        &         &    &           &R_3 &+ \\gamma   &R_4 &+ \\gamma^2 &R_5 &+ \\;\\cdots \\;&+ \\gamma^{T-3} &R_T &       \\\\\n",
        "   && &\\quad         &\\cdots &   & & & & & & & & & & &    & \\\\\n",
        "   && &\\quad G_T     &= &        & & & & & & & & & & &0   & \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Lembrando que, para $t < T$ qualquer, observamos esta relação:\n",
        "\n",
        "$$\n",
        "   G_{t} = R_{t+1} + \\gamma G_{t+1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snalEnror5yY"
      },
      "source": [
        "### 1.2 - Função de valor do estado $V(s)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BzcaPoDr5yY"
      },
      "source": [
        "Esta função dá o retorno parcial esperado a partir de cada estado $s$, para uma política específica.\n",
        "\n",
        "Informalmente, $V(s)$ responde esta pergunta:\n",
        "\n",
        "*Partindo do estado **s**, qual o retorno parcial esperado (médio), se o agente continuar seguindo a mesma política*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM36lLzqr5yZ"
      },
      "source": [
        "### 1.3 - Função de valor da ação $Q(s,a)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu_3nT7cr5yZ"
      },
      "source": [
        "Esta é a função mais importante para esta aula.\n",
        "\n",
        "De maneira informal, $Q(s,a)$ responde a esta pergunta:\n",
        "\n",
        "*Partindo do estado **s** e, em seguida, fazendo a ação **a**, qual o retorno esperado (médio), se continuar seguindo a mesma política?*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbs4mVmYr5ya"
      },
      "source": [
        "### 1.4 Algoritmos de Monte Carlo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs9DpIGJ5gBE"
      },
      "source": [
        "***O que são algoritmos de Monte Carlo?***\n",
        "\n",
        " - Utilizam amostragem aleatória para resolver problemas ou estimar quantidades que são difíceis de calcular diretamente.\n",
        " - Envolvem a geração de um grande número de amostras ou simulações aleatórias para tirar conclusões ou fazer previsões.\n",
        " - Usados em várias áreas (não só na Aprendizagem por Reforço).\n",
        "\n",
        "Vimos, antes, dois algoritmos de Monte Carlo para o problema de **predição** da aprendizagem por reforço, para estimar os valores de $V(s)$ e de $Q(s,a)$.\n",
        "\n",
        "Aqui, veremos algoritmos de Monte Carlo para o problema de **controle** da aprendizagem por reforço.\n",
        "- Para aprender a política ótima (ou quase)!\n",
        "\n",
        "Os algoritmos de controle que veremos se baseiam na **função de valor da ação `Q(s,a)`**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JIcbuGcykf1",
        "tags": []
      },
      "source": [
        "## 2 - Algoritmo de Monte Carlo (para Controle) - Versão 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUwD8YVoykf1"
      },
      "source": [
        "Este algoritmo roda vários episódios, fazendo estes passos a cada episódio:\n",
        "\n",
        "1. Gera a **trajetória** completa (sequência de estados/observações, ações e recompensas) do episódio:\n",
        "\n",
        "    $S_0 \\rightarrow A_0 \\rightarrow R_1 \\rightarrow S_1 \\rightarrow A_1 \\rightarrow R_2 \\rightarrow S_2 \\rightarrow \\cdots S_{n-1} \\rightarrow A_{n-1} \\rightarrow R_T \\rightarrow S_T$\n",
        "\n",
        "2. Para escolher a ação $a$, a ser realizada em um estado $s$, ele usa a tabela $Q(s,a)$ com alguma estratégia de exploração -- vamos usar $\\epsilon$-greedy. Assim, ele escolhe dessa forma:\n",
        "\n",
        "   - com probabilidade $\\epsilon$, ele escolhe uma ação $a$ qualquer\n",
        "   - com probabilidade $(1-\\epsilon)$, ele escolhe a melhor ação, ou seja, $\\operatorname{argmax}_a{Q(s,a)}$\n",
        "   <br>\n",
        "3. Ao fim do episódio, para cada par intermediário ($S_t$, $A_t$) da trajetória, ele:\n",
        "   - calcula o retorno parcial $G_t$.\n",
        "   - usa $G_t$ para atualizar $Q(S_t, A_t)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjM4FQh4r5yb"
      },
      "outputs": [],
      "source": [
        "# Esta é a política. Neste caso, escolhe uma ação com base nos valores\n",
        "# da tabela Q, usando uma estratégia epsilon-greedy.\n",
        "def choose_action(Q, state, num_actions, epsilon):\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(0, num_actions)\n",
        "    else:\n",
        "        return np.argmax(Q[state]) # em caso de empates: menor índice/ação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdB0uVyZDFXh"
      },
      "outputs": [],
      "source": [
        "# Algoritmo Monte-Carlo de Controle, variante \"toda-visita\".\n",
        "# Atenção: os espaços de estados e de ações precisam ser discretos, dados por valores inteiros\n",
        "def run_montecarlo1(env, episodes, gamma=0.95, epsilon=0.1, render=False):\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # dicionário com todos os retornos descontados, para cada par (estado,ação)\n",
        "    returns_history = dict()\n",
        "\n",
        "    # inicializa a tabela Q toda com zero,\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.zeros(shape = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        episode_reward, reward = 0, 0\n",
        "        ep_trajectory = []\n",
        "\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        # [1] Executa um episódio completo, salvando a trajetória\n",
        "        while not done:\n",
        "            # exibe/renderiza os passos no ambiente, durante 1 episódio a cada mil e também nos últimos 5 episódios\n",
        "            if render and (i >= (episodes - 5) or (i+1) % 1000 == 0) and not IN_COLAB:\n",
        "                env.render()\n",
        "\n",
        "            # [2] Escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = choose_action(Q, state, num_actions, epsilon)\n",
        "\n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # adiciona a tripla que representa este passo\n",
        "            ep_trajectory.append( (state, action, reward) )\n",
        "\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        sum_rewards_per_ep.append(episode_reward)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso\n",
        "        if (i+1) % 100 == 0:\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "        # [3] Calcula os retornos parciais e atualiza Q\n",
        "        Gt = 0\n",
        "        for (s, a, r) in reversed(ep_trajectory):\n",
        "            Gt = r + gamma*Gt\n",
        "\n",
        "            if returns_history.get((s,a)) is None:\n",
        "                returns_history[s,a] = [ Gt ]\n",
        "            else:\n",
        "                returns_history[s,a].append(Gt)\n",
        "\n",
        "            # média entre todas as ocorrências de (s,a) encontradas nos episódios\n",
        "            Q[s,a] = np.mean(returns_history[s,a]) # LENTO!\n",
        "            # IDEIA: usar cálculo alternativo da média: M = M + (1/n)*(x - M)\n",
        "\n",
        "    # para gerar um vídeo no Colab\n",
        "    if render and IN_COLAB:\n",
        "        clear_output()\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gv5YNiHykf2"
      },
      "outputs": [],
      "source": [
        "# estes dois ambientes possuem estados e ações discretas\n",
        "# ver mais em: https://gymnasium.farama.org/\n",
        "ENV_NAME = \"Taxi-v3\"\n",
        "#ENV_NAME = \"FrozenLake-v1\"\n",
        "\n",
        "# usar 10 para Taxi e 1.0 para FrozenLake\n",
        "r_max_plot = 10.0\n",
        "#r_max_plot = 1.0\n",
        "\n",
        "EPISODES = 3_000\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.1\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "# Roda o algoritmo Monte-Carlo para o problema de controle (ou seja, para achar a política ótima)\n",
        "rewards1, qtable1 = run_montecarlo1(env, EPISODES, GAMMA, EPSILON, render=True)\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards1[-20:]), \", desvio padrao =\", np.std(rewards1[-20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qJf8RPL0L0I"
      },
      "outputs": [],
      "source": [
        "# Mostra um gráfico de episódios x retornos não descontados\n",
        "# Se quiser salvar, passe o nome do arquivo no 3o parâmetro\n",
        "#filename = f\"results/montecarlo1-{ENV_NAME.lower()[0:8]}-ep{EPISODES}.png\"\n",
        "plot_result(rewards1, r_max_plot, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate_qtable(env, qtable1, 20, verbose=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtJCaKx20L0I"
      },
      "outputs": [],
      "source": [
        "record_video_qtable(ENV_NAME, qtable1, length=1_000, folder='videos/', prefix='mcarlo-1')\n",
        "display_videos_from_path('videos/', prefix='mcarlo-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmkT_Si2K0cX"
      },
      "source": [
        "## 3 - Algoritmo de Monte Carlo (para Controle) - Versão 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL9Fgp3lK4HW"
      },
      "source": [
        "Modifique o código acima com essas características:\n",
        "1. Mantenha o uso de `choose_action()` para escolher a ação.\n",
        "1. Remova o **histórico** de retornos parciais.\n",
        "1. Use uma **taxa de aprendizagem**, representada pelo parâmetro `lr`.\n",
        "\n",
        "Faça as melhorias abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gkAF9lnLEsu"
      },
      "outputs": [],
      "source": [
        "# Algoritmo Monte-Carlo de Controle, variante \"toda-visita\".\n",
        "# Atenção: os espaços de estados e de ações precisam ser discretos, dados por valores inteiros\n",
        "def run_montecarlo2(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1, render=False):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # inicializa a tabela Q toda com zero,\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.zeros(shape = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "        ep_trajectory = []\n",
        "\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        # [1] Executa um episódio completo, salvando a trajetória\n",
        "        while not done:\n",
        "            # exibe/renderiza os passos no ambiente, durante 1 episódio a cada mil e também nos últimos 5 episódios\n",
        "            if render and (i >= (episodes - 5) or (i+1) % 1000 == 0) and not IN_COLAB:\n",
        "                env.render()\n",
        "\n",
        "            # [2] Escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = choose_action(Q, state, num_actions, epsilon)\n",
        "\n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # adiciona a tripla que representa este passo\n",
        "            ep_trajectory.append( (state, action, reward) )\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso\n",
        "        if (i+1) % 100 == 0:\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "        # [3] Calcula os retornos parciais e atualiza Q\n",
        "        Gt = 0\n",
        "        for (s, a, r) in reversed(ep_trajectory):\n",
        "            Gt = r + gamma*Gt\n",
        "            # MODIFICAÇÂO SOLICITADA:\n",
        "            Q[s,a] = (1.0 - lr)*Q[s,a] + lr*Gt\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LmN99MOLRE7"
      },
      "source": [
        "Quando o código estiver pronto, você poderá testar rodando o código abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CYpmGrpLWoU"
      },
      "outputs": [],
      "source": [
        "## ATENÇÃO: faça as MELHORIAS ACIMA antes de executar aqui!\n",
        "\n",
        "ENV_NAME = \"Taxi-v3\"\n",
        "#ENV_NAME = \"FrozenLake-v1\"\n",
        "\n",
        "r_max_plot = 10.0\n",
        "#r_max_plot = 1.0\n",
        "\n",
        "EPISODES = 20_000\n",
        "LR = 0.05\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.1\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "# Roda o algoritmo Monte-Carlo para o problema de controle (ou seja, para achar a política ótima)\n",
        "rewards2, qtable2 = run_montecarlo2(env, EPISODES, LR, GAMMA, EPSILON, render=True)\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards2[-20:]), \", desvio padrao =\", np.std(rewards2[-20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNQ6SussthmA"
      },
      "outputs": [],
      "source": [
        "# Mostra um gráfico de episódios x retornos não descontados\n",
        "# Se quiser salvar, passe o nome do arquivo no 3o parâmetro\n",
        "#filename = f\"results/montecarlo2-{ENV_NAME.lower()[0:8]}-ep{EPISODES}.png\"\n",
        "plot_result(rewards2, r_max_plot, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate_qtable(env, qtable2, 20, verbose=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "record_video_qtable(ENV_NAME, qtable2, length=1_000, folder='videos/', prefix='mcarlo-2')\n",
        "display_videos_from_path('videos/', prefix='mcarlo-2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBDUUUPcykf5",
        "tags": []
      },
      "source": [
        "## 4 - Experimentos Finais\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDGW2aBUM0Pa"
      },
      "outputs": [],
      "source": [
        "from util.experiments import repeated_exec\n",
        "from util.plot import plot_multiple_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OOoTRBhJsr0"
      },
      "source": [
        "Vamos comparar os algoritmos aqui propostos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puFH282FykgI"
      },
      "outputs": [],
      "source": [
        "NUM_EPISODES = 15_000\n",
        "\n",
        "enviroment = gym.make(\"Taxi-v3\")\n",
        "#enviroment = gym.make(\"FrozenLake-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "470C2DNd0L0J"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "# muito lento, se usar o histórico completo!\n",
        "#results.append( repeated_exec(1, \"Monte-Carlo1\", run_montecarlo1, enviroment, NUM_EPISODES) )\n",
        "\n",
        "for learning_rate in [0.01, 0.1, 0.5]:\n",
        "    results.append( repeated_exec(5, f\"Monte-Carlo2 (LR={learning_rate})\", run_montecarlo2, enviroment, NUM_EPISODES, learning_rate) )\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYJXr1YR2atg"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results, cumulative=False, x_log_scale=False, window=50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "47acfd36b4a698d100796428813311ecacef03b489c77dd1fdf080373e214244"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
