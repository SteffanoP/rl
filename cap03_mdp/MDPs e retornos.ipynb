{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MDPs, Trajetórias e Retornos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# vamos focar nesses três ambientes por serem mais simples\n",
    "#env = gym.make(\"MountainCar-v0\")\n",
    "#env = gym.make(\"CartPole-v1\")\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "#env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figura mostrando interação agente(política)-ambiente](figura_mdp.png \"Interação agente-ambiente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Episódio e Trajetória"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um *episódio* é uma execução da tarefa (ou do ambiente gym). \n",
    "\n",
    "E a *trajetória* é a sequência de estados (observações), ações e recompensas do episódio. Assumindo um episódio de $n$ passos (ações aplicadas):\n",
    "\n",
    "$S_0 \\rightarrow A_0 \\rightarrow R_1 \\rightarrow S_1 \\rightarrow A_1 \\rightarrow R_2 \\rightarrow S_2 \\rightarrow \\cdots S_{n-1} \\rightarrow A_{n-1} \\rightarrow R_n \\rightarrow S_n$\n",
    "\n",
    "Vamos ilustrar um episódio em um MDP usando o ambiente *\"env\"* escolhido no código acima. \n",
    "\n",
    "Estamos assumindo que o episódio encerrou de fato (chegou em um estado final) em *$n$=\"TOTAL_STEPS\"* passos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0 = 62\n",
      " A0 = 4\n",
      "  R1 = -10\n",
      "S1 = 62\n",
      " A1 = 1\n",
      "  R2 = -1\n",
      "S2 = 62\n",
      " A2 = 3\n",
      "  R3 = -1\n",
      "S3 = 42\n",
      " A3 = 3\n",
      "  R4 = -1\n",
      "S4 = 42\n",
      " A4 = 5\n",
      "  R5 = -10\n",
      "S5 = 42\n"
     ]
    }
   ],
   "source": [
    "TOTAL_STEPS = 5\n",
    "\n",
    "i = 0\n",
    "obs = env.reset()\n",
    "print(f\"S0 = {obs}\")\n",
    "\n",
    "done = False\n",
    "\n",
    "# roda apenas alguns passos\n",
    "for i in range(0,TOTAL_STEPS):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    print(f\" A{i} = {action}\")\n",
    "\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    print(f\"  R{i+1} = {reward}\")\n",
    "    print(f\"S{i+1} = {next_obs}\")\n",
    "\n",
    "    obs = next_obs\n",
    "    time.sleep(0.1)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os detalhes do *episódio* que mostramos acima são chamamos de *trajectory* ou *rollout*.\n",
    "\n",
    "Dependendo do algoritmo, vamos precisar analisar essas informações em trios (S,A,R) ou quádruplas (S,A,R,S) ou até quíntuplas (S,A,R,S',A').\n",
    "\n",
    "Abaixo, vamos agrupar e guardar em trio, para preparar para o algoritmo Monte Carlo. E vamos rodar 1 episódio completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajetória como sequência de trios (STATE, ACTION, REWARD):\n",
      "[(288, 4, -10), (288, 0, -1), (388, 0, -1), (488, 0, -1), (488, 1, -1), (388, 4, -10), (388, 5, -10), (388, 5, -10), (388, 3, -1), (368, 5, -10), (368, 2, -1), (388, 0, -1), (488, 4, -10), (488, 2, -1), (488, 3, -1), (468, 0, -1), (468, 0, -1), (468, 4, -10), (468, 5, -10), (468, 5, -10), (468, 5, -10), (468, 0, -1), (468, 1, -1), (368, 1, -1), (268, 4, -10), (268, 3, -1), (248, 0, -1), (348, 1, -1), (248, 2, -1), (268, 0, -1), (368, 5, -10), (368, 0, -1), (468, 2, -1), (488, 1, -1), (388, 2, -1), (388, 0, -1), (488, 5, -10), (488, 1, -1), (388, 4, -10), (388, 2, -1), (388, 2, -1), (388, 2, -1), (388, 5, -10), (388, 1, -1), (288, 1, -1), (188, 0, -1), (288, 1, -1), (188, 1, -1), (88, 1, -1), (88, 0, -1), (188, 4, -10), (188, 5, -10), (188, 5, -10), (188, 1, -1), (88, 5, -10), (88, 3, -1), (68, 3, -1), (48, 3, -1), (48, 0, -1), (148, 2, -1), (168, 3, -1), (148, 2, -1), (168, 4, -10), (168, 3, -1), (148, 3, -1), (148, 4, -10), (148, 1, -1), (48, 5, -10), (48, 2, -1), (68, 2, -1), (88, 0, -1), (188, 3, -1), (168, 4, -10), (168, 0, -1), (268, 1, -1), (168, 2, -1), (188, 3, -1), (168, 5, -10), (168, 0, -1), (268, 2, -1), (288, 5, -10), (288, 3, -1), (268, 2, -1), (288, 3, -1), (268, 1, -1), (168, 4, -10), (168, 5, -10), (168, 2, -1), (188, 2, -1), (188, 0, -1), (288, 0, -1), (388, 4, -10), (388, 0, -1), (488, 5, -10), (488, 5, -10), (488, 0, -1), (488, 5, -10), (488, 5, -10), (488, 5, -10), (488, 0, -1), (488, 4, -10), (488, 0, -1), (488, 4, -10), (488, 3, -1), (468, 2, -1), (488, 4, -10), (488, 5, -10), (488, 3, -1), (468, 4, -10), (468, 0, -1), (468, 1, -1), (368, 0, -1), (468, 0, -1), (468, 5, -10), (468, 4, -10), (468, 0, -1), (468, 5, -10), (468, 5, -10), (468, 4, -10), (468, 4, -10), (468, 3, -1), (468, 1, -1), (368, 1, -1), (268, 2, -1), (288, 0, -1), (388, 1, -1), (288, 4, -10), (288, 5, -10), (288, 4, -10), (288, 0, -1), (388, 1, -1), (288, 5, -10), (288, 3, -1), (268, 3, -1), (248, 1, -1), (148, 2, -1), (168, 3, -1), (148, 5, -10), (148, 0, -1), (248, 1, -1), (148, 3, -1), (148, 4, -10), (148, 5, -10), (148, 3, -1), (148, 3, -1), (148, 0, -1), (248, 5, -10), (248, 1, -1), (148, 2, -1), (168, 3, -1), (148, 2, -1), (168, 5, -10), (168, 0, -1), (268, 3, -1), (248, 5, -10), (248, 4, -10), (248, 3, -1), (228, 3, -1), (208, 2, -1), (228, 3, -1), (208, 4, -10), (208, 4, -10), (208, 0, -1), (308, 1, -1), (208, 5, -10), (208, 4, -10), (208, 0, -1), (308, 1, -1), (208, 4, -10), (208, 4, -10), (208, 5, -10), (208, 1, -1), (108, 2, -1), (128, 3, -1), (108, 3, -1), (108, 5, -10), (108, 3, -1), (108, 2, -1), (128, 5, -10), (128, 1, -1), (28, 1, -1), (28, 0, -1), (128, 2, -1), (128, 3, -1), (108, 2, -1), (128, 3, -1), (108, 1, -1), (8, 2, -1), (28, 5, -10), (28, 2, -1), (28, 0, -1), (128, 3, -1), (108, 1, -1), (8, 4, -10), (8, 1, -1), (8, 4, -10), (8, 2, -1), (28, 4, -10), (28, 3, -1), (8, 5, -10)]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "trajectory = [] \n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    trajectory.append( (obs, action, reward) )\n",
    "\n",
    "    obs = next_obs\n",
    "    time.sleep(0.1)\n",
    "\n",
    "env.close()\n",
    "#trajectory.append( (obs, None, None) )\n",
    "\n",
    "print(\"Trajetória como sequência de trios (STATE, ACTION, REWARD):\")\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2.Calcular os Retornos\n",
    "\n",
    "O *retorno (final)* $G$ é uma medida da recompensa total obtida ao longo de um episódio. \n",
    "\n",
    "Em um MDP, o objetivo é otimizar o valor médio de $G$, para infinitos episódios.\n",
    "\n",
    "Para isso, vamos assumir a política abaixo, que escolhe a ação $0$ com 50% de probabilidade, ou outra ação, caso contrário.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "\n",
    "def policy(state):\n",
    "    x = np.random.random()\n",
    "    if x <= 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.random.randint(1, num_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E vamos criar uma trajetória com esta política:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajetória:\n",
      "[(106, 0, -1), (206, 0, -1), (306, 4, -10), (306, 0, -1), (406, 0, -1), (406, 5, -10), (406, 5, -10), (406, 2, -1), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 4, -10), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 5, -10), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 1, -1), (306, 3, -1), (306, 3, -1), (306, 0, -1), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 1, -1), (306, 0, -1), (406, 3, -1), (406, 0, -1), (406, 3, -1), (406, 4, -10), (406, 1, -1), (306, 0, -1), (406, 0, -1), (406, 3, -1), (406, 5, -10), (406, 5, -10), (406, 0, -1), (406, 4, -10), (406, 2, -1), (406, 4, -10), (406, 4, -10), (406, 2, -1), (406, 0, -1), (406, 0, -1), (406, 3, -1), (406, 1, -1), (306, 2, -1), (306, 0, -1), (406, 4, -10), (406, 5, -10), (406, 0, -1), (406, 2, -1), (406, 0, -1), (406, 2, -1), (406, 0, -1), (406, 3, -1), (406, 4, -10), (406, 0, -1), (406, 2, -1), (406, 5, -10), (406, 0, -1), (406, 0, -1), (406, 3, -1), (406, 1, -1), (306, 5, -10), (306, 5, -10), (306, 4, -10), (306, 0, -1), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 2, -1), (406, 0, -1), (406, 0, -1), (406, 4, -10), (406, 0, -1), (406, 2, -1), (406, 1, -1), (306, 4, -10), (306, 0, -1), (406, 3, -1), (406, 0, -1), (406, 0, -1), (406, 4, -10), (406, 0, -1), (406, 5, -10), (406, 4, -10), (406, 2, -1), (406, 3, -1), (406, 0, -1), (406, 5, -10), (406, 0, -1), (406, 4, -10), (406, 1, -1), (306, 0, -1), (406, 0, -1), (406, 1, -1), (306, 3, -1), (306, 5, -10), (306, 0, -1), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 4, -10), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 1, -1), (306, 0, -1), (406, 0, -1), (406, 5, -10), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 3, -1), (406, 3, -1), (406, 5, -10), (406, 0, -1), (406, 1, -1), (306, 0, -1), (406, 3, -1), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 4, -10), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 4, -10), (406, 0, -1), (406, 0, -1), (406, 4, -10), (406, 0, -1), (406, 2, -1), (406, 3, -1), (406, 3, -1), (406, 0, -1), (406, 4, -10), (406, 2, -1), (406, 4, -10), (406, 0, -1), (406, 0, -1), (406, 0, -1), (406, 3, -1), (406, 4, -10), (406, 0, -1), (406, 5, -10), (406, 2, -1), (406, 5, -10), (406, 5, -10), (406, 4, -10), (406, 0, -1), (406, 3, -1), (406, 4, -10), (406, 0, -1), (406, 5, -10), (406, 1, -1), (306, 3, -1), (306, 5, -10), (306, 1, -1), (206, 0, -1), (306, 0, -1), (406, 2, -1), (406, 3, -1), (406, 3, -1), (406, 4, -10), (406, 0, -1), (406, 1, -1), (306, 2, -1), (306, 2, -1), (306, 0, -1), (406, 0, -1), (406, 0, -1), (406, 1, -1), (306, 0, -1), (406, 5, -10), (406, 3, -1), (406, 0, -1), (406, 0, -1), (406, 1, -1), (306, 0, -1), (406, 0, -1), (406, 0, -1), (406, 5, -10), (406, 1, -1), (306, 4, -10), (306, 2, -1), (306, 0, -1), (406, 5, -10)]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "trajectory = [] \n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = policy(obs)\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "    trajectory.append( (obs, action, reward) )\n",
    "    obs = next_obs\n",
    "\n",
    "env.close()\n",
    "#trajectory.append( (obs, None, None) )\n",
    "\n",
    "print(\"Trajetória:\")\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Retorno final do episódio ($G$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para um episódio com $n$ passos, o *retorno* (não-descontado) é calculado assim:\n",
    "\n",
    "$ G = R_1 + R_2 + R_3 + \\cdots + R_n = \\displaystyle\\sum_{i=1}^{n} R_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retorno não-descontado: -623.0\n"
     ]
    }
   ],
   "source": [
    "sum_rewards = 0.0\n",
    "for (s, a, r) in trajectory:\n",
    "    sum_rewards += r\n",
    "\n",
    "print(\"Retorno não-descontado:\", sum_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mais usado é o *retorno descontado* de um episódio.\n",
    "\n",
    "Neste caso, $G$ é uma soma que \"atenua\" recompensas mais distantes, valorizando mais as recompensas iniciais. (Você prefere receber 100 reais agora, de uma vez, ou em 100 parcelas de 1 real?)\n",
    "\n",
    "Para isso, a cada passo, a recompensa tem uma redução dada por um parâmetro $\\gamma\\;$, tal que $0 < \\gamma \\leq 1$.\n",
    "\n",
    "Para um episódio com $n$ passos, o *retorno descontado* é calculado assim:\n",
    "\n",
    "$ G = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\cdots + \\gamma^{(n-1)} R_n = \\displaystyle\\sum_{t=1}^{n} \\gamma^{(t-1)} R_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retorno descontado: -199.99298946675006\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.95  # você escolhe esse valor\n",
    "\n",
    "step = 0\n",
    "discounted_sum_rewards = 0.0\n",
    "for (s, a, r) in trajectory:\n",
    "    discounted_sum_rewards += (GAMMA ** step) * reward\n",
    "    step += 1\n",
    "\n",
    "print(\"Retorno descontado:\", discounted_sum_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Retornos intermediários a cada passo ($G_i$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Também podemos calcular um retorno parcial, de um certo episódio, a partir de um passo específico $i$ do episódio:\n",
    "\n",
    "- $ G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\gamma^3 R_4 + \\gamma^4 R_5 + \\cdots \\;\\;= G$ \n",
    "- $ G_1 = \\;\\;\\;\\;\\;\\;\\;\\;\\;  R_2 + \\;\\gamma R_3 + \\;\\gamma^2 R_4 + \\;\\gamma^3 R_5  + \\cdots $ \n",
    "- $ G_2 = \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; R_3 + \\;\\gamma R_4 + \\;\\gamma^2 R_5 + \\cdots $ \n",
    "- $\\cdots$ \n",
    "- $ G_n = 0 $\n",
    "\n",
    "Propriedade:\n",
    "- $ G_{i-1} = R_i + \\gamma G_i $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-59.003513834546595, -61.05633035215431, -63.21718984437296, -56.01809457302417, -57.91378376107808, -59.90924606429272, -52.53604848872918, -44.77478788287282, -46.078724087234555, -47.45128851287848, -48.89609317145103, -50.41694018047477, -42.54414755839449, -43.730681640415256, -44.97966488464764, -46.294384089102785, -47.67829904116083, -49.13505162227456, -41.19479118134164, -42.31030650667542, -43.484533164921494, -44.72056122623315, -46.0216433960349, -47.39120357477358, -48.832845868182716, -50.35036407177128, -51.94775165449609, -53.62921226789062, -55.39917080830592, -57.262285061374655, -59.223457959341744, -61.28785048351763, -63.46089524580803, -65.7483107850611, -68.15611661585379, -70.69064906931979, -73.35857796770505, -76.16692417653164, -79.12307808055962, -72.7611348216417, -75.53803665435969, -78.46109121511546, -81.53799075275312, -84.77683237131907, -78.71245512770429, -72.32890013442557, -75.08305277307954, -68.50847660324163, -71.06155431920172, -64.27532033600181, -57.1319161431598, -59.08622751911558, -61.14339738854272, -63.308839356360764, -65.58825195406396, -67.98763363585681, -70.51329856405981, -73.17189322532612, -66.4967297108696, -59.47024180091537, -61.54762294833197, -63.734339945612604, -66.03614731117116, -68.45910243281175, -71.0095815082229, -73.69429632444516, -67.04662770994227, -69.52276601046555, -72.12922737943742, -65.3991867151973, -67.78861759494453, -70.30380799467845, -72.95137683650364, -75.73829140684595, -69.19820148089048, -62.313896295674176, -55.0672592586044, -56.91290448274148, -58.85568892920156, -60.900725188633224, -63.0533949354034, -65.31936308989832, -67.70459272620876, -70.21536076443027, -63.38459027834765, -65.66798976668174, -68.07156817545447, -70.60165071100471, -63.79121127474181, -66.09601186814928, -68.52211775594661, -71.07591342731223, -73.76411939717077, -67.1201256812324, -69.6001322960341, -62.736981364246425, -55.51261196236466, -57.38169680248912, -59.34915452893591, -61.420162662037804, -54.126487012671376, -55.92261790807513, -48.33959779797382, -49.83115557681455, -51.4012163966469, -53.053911996470426, -54.79359157523203, -56.62483323708635, -49.07877182851195, -50.609233503696785, -52.22024579336504, -53.91604820354215, -55.70110337214963, -48.10642460226277, -49.58571010764502, -51.142852744889495, -52.78195025777842, -54.50731606081939, -56.323490590336206, -58.23525325298548, -60.24763500314261, -52.89224737172907, -54.6234182860306, -56.44570345897958, -58.36389837787324, -60.3830509240771, -62.50847465692326, -55.27207858623502, -57.12850377498423, -59.08263555261498, -61.139616371173666, -63.30485933807755, -65.58406246113427, -67.98322364329923, -70.50865646663077, -63.69332259645344, -65.99297115416152, -68.41365384648581, -70.9617408910377, -73.64393778003969, -66.99361871583126, -69.46696706929606, -72.07049165189059, -65.33735963356905, -67.72353645638847, -70.2353015330405, -72.87926477162158, -75.66238397012798, -78.5919831264505, -72.20208750152685, -74.9495657910809, -68.36796399061147, -70.91364630590681, -73.59331190095455, -76.41401252732058, -79.38317108139009, -73.03491692777904, -75.82622834503057, -69.29076667897955, -71.88501755682059, -65.14212374402167, -58.044340783180715, -50.57299029808497, -52.18209505061576, -53.87588952696396, -46.18514687048838, -47.56331249525093, -39.540328942369406, -40.56876730775727, -41.65133400816555, -33.317193692805844, -34.01809862400615, -34.75589328842753, -35.53251925097635, -36.35002026418563, -37.210547646511195, -38.116365943695996, -29.596174677574737, -30.10123650271025, -30.632880529168688, -31.19250582017757, -31.781585073871128, -32.401668498811716, -33.054387893486016, -33.7414609405116, -34.464695726854316, -35.225995501951914, -26.553679475738853, -26.89860997446195, -27.26169470995995, -27.643889168378898, -28.04619912460937, -28.469683289062495, -28.915456093749995, -29.384690624999998, -20.4049375, -20.42625, -10.975, -10.5, -10.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# calcula os retornos a cada passo (G_i, para cada i=0...n) do episódio completo\n",
    "G = 0.0\n",
    "all_Gs = [ G ]\n",
    "for (s, a, r) in reversed(trajectory):   \n",
    "    G = r + GAMMA*G\n",
    "    all_Gs.insert(0, G)\n",
    "\n",
    "print(all_Gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Funções de Valor\n",
    "\n",
    "São funções que não fazem parte da essência de um MDP, mas são úteis para criar algoritmos.\n",
    "\n",
    "Todas elas fazem avaliações dos retornos esperados para uma política específica! Ambas são calculadas a partir dos retornos intermediários.\n",
    "\n",
    "Veremos dois tipos de função de valor, a seguir. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Função de valor do estado V(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcula o retorno esperado a partir de cada estado $s$.\n",
    "\n",
    "De forma uma pouco mais matemática:\n",
    "\n",
    "$V(s) = E[G_t | S_t=s]$ \n",
    "\n",
    "---\n",
    "\n",
    "Uma explicação mais algorítmica do valor de $V(s)$ para um $s$ específico, seria esta:\n",
    "1. rode infinitos episódios com a política\n",
    "2. para cada episódio:\n",
    "   - examine se o estado $s$ ocorre em algum passo $t$ (qualquer)\n",
    "   - se ocorrer, salve os possíveis valores $G_t$\n",
    "3. Tire a média dos valores salvos\n",
    "\n",
    "---\n",
    "\n",
    "Vamos implementar esta ideia, rodando apenas 1 episódio, e vamos calcular o $V(s)$ para todo estado que encontrarmos. Para isso, vamos anexar cada retorno a um dicionário \"returns-history\" (um histórico dos retornos) indexado pelo $s$ onde se originou cada retorno intermediário.\n",
    "\n",
    "Esta implementação assume ambiente de *estado discreto*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.         -59.00351383   0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.         -47.53721449   0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.         -49.33681792   0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.         -58.27280673   0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.        ]\n"
     ]
    }
   ],
   "source": [
    "returns_history = dict()\n",
    "V = np.zeros(env.observation_space.n)\n",
    "\n",
    "# calcula os retornos a cada passo (G_i, para cada i=0...n) do episódio completo\n",
    "G = 0.0\n",
    "for (s, a, r) in reversed(trajectory):   \n",
    "    G = r + GAMMA*G\n",
    "    \n",
    "    if s in returns_history.keys():\n",
    "        returns_history[s].append(G)\n",
    "    else:\n",
    "        returns_history[s] = [ G ]\n",
    "    \n",
    "    V[s] = np.mean( returns_history[s] )\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Função de valor do estado-ação Q(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De maneira análoga ao $V(s)$, o $Q(s,a)$ representa o retorno esperado a partir do par $(s,a)$. \n",
    "\n",
    "Em outras palavras, Q(s,a) responde esta pergunta:\n",
    "\n",
    "*\"Quando estava no estado $s$ e fez a ação $a$, qual o retorno esperado?\"*\n",
    "\n",
    "A definição e a forma de calcular é análoga ao $V(s)$, mas vamos usar um \"returns_history\" indexado por cada par $(s,a)$ onde se originou cada retorno $G_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "returns_history = dict()\n",
    "Q = np.zeros(shape=(env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# calcula os retornos a cada passo (G_i, para cada i=0...n) do episódio completo\n",
    "G = 0.0\n",
    "for (s, a, r) in reversed(trajectory):   \n",
    "    G = r + GAMMA*G\n",
    "    \n",
    "    if (s,a) in returns_history.keys():\n",
    "        returns_history[s,a].append(G)\n",
    "    else:\n",
    "        returns_history[s,a] = [ G ]\n",
    "    \n",
    "    Q[s,a] = np.mean( returns_history[s,a] )\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Introdução aos Métodos Baseados em Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponha que você partiu de uma política qualquer (provavelmente ruim) e calculou o *Q* dessa política.\n",
    "\n",
    "De que forma você poderia melhorá-la?\n",
    "\n",
    "Ou, como escolher a melhor ação a cada estado, usando Q?\n",
    "\n",
    "---\n",
    "\n",
    "No próxima parte do curso, veremos um método Monte-Carlo para RL, onde a política é implicitamente representada por Q. \n",
    "\n",
    "Este método repete N vezes esses passos:\n",
    "1. Roda um episódio, usando a política representada pela tabela Q\n",
    "   - salva a trajetória\n",
    "1. Depois, usa a trajetória para calcular os valores de $G_i$ e usa esses valores para atualizar Q\n",
    "   - ao atualizar Q, a política também muda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "17935451bad8d760d3ec9e03731ab7fc392b610a70f377bd5ec1362f80c504be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
