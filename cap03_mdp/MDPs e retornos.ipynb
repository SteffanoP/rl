{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MDPs, Trajetórias e Retornos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# vamos focar nesses três ambientes por serem mais simples\n",
    "#env = gym.make(\"MountainCar-v0\")\n",
    "#env = gym.make(\"CartPole-v1\")\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "#env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figura mostrando interação agente(política)-ambiente](figura_mdp.png \"Interação agente-ambiente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Episódio e Trajetória"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um *episódio* é uma execução da tarefa (ou do ambiente gym). \n",
    "\n",
    "E a *trajetória* é a sequência de estados (observações), ações e recompensas do episódio. Assumindo um episódio de $n$ passos (ações aplicadas):\n",
    "\n",
    "$S_0 \\rightarrow A_0 \\rightarrow R_1 \\rightarrow S_1 \\rightarrow A_1 \\rightarrow R_2 \\rightarrow S_2 \\rightarrow \\cdots S_{n-1} \\rightarrow A_{n-1} \\rightarrow R_n \\rightarrow S_n$\n",
    "\n",
    "Vamos ilustrar um episódio em um MDP usando o ambiente *\"env\"* escolhido no código acima. \n",
    "\n",
    "Estamos assumindo que o episódio encerrou de fato (chegou em um estado final) em *$n$=\"TOTAL_STEPS\"* passos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0 = 2\n",
      " A0 = 1\n",
      "  R1 = -1\n",
      "S1 = 2\n",
      " A1 = 4\n",
      "  R2 = -1\n",
      "S2 = 18\n",
      " A2 = 0\n",
      "  R3 = -1\n",
      "S3 = 118\n",
      " A3 = 5\n",
      "  R4 = -10\n",
      "S4 = 118\n",
      " A4 = 1\n",
      "  R5 = -1\n",
      "S5 = 18\n"
     ]
    }
   ],
   "source": [
    "TOTAL_STEPS = 5\n",
    "\n",
    "i = 0\n",
    "obs = env.reset()\n",
    "print(f\"S0 = {obs}\")\n",
    "\n",
    "done = False\n",
    "\n",
    "# roda apenas alguns passos\n",
    "for i in range(0,TOTAL_STEPS):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    print(f\" A{i} = {action}\")\n",
    "\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    print(f\"  R{i+1} = {reward}\")\n",
    "    print(f\"S{i+1} = {next_obs}\")\n",
    "\n",
    "    obs = next_obs\n",
    "    #time.sleep(0.1)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os detalhes do *episódio* que mostramos acima são chamamos de *trajectory* ou *rollout*.\n",
    "\n",
    "Dependendo do algoritmo, vamos precisar analisar essas informações em trios (S,A,R) ou quádruplas (S,A,R,S) ou até quíntuplas (S,A,R,S',A').\n",
    "\n",
    "Abaixo, vamos agrupar e guardar em trio, para preparar para o algoritmo Monte Carlo. E vamos rodar 1 episódio completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajetória como sequência de trios (STATE, ACTION, REWARD):\n",
      "[(252, 1, -1), (152, 2, -1), (172, 5, -10), (172, 3, -1), (152, 3, -1), (152, 1, -1), (52, 3, -1), (52, 4, -10), (52, 2, -1), (72, 2, -1), (92, 4, -10), (92, 4, -10), (92, 4, -10), (92, 1, -1), (92, 3, -1), (72, 5, -10), (72, 1, -1), (72, 5, -10), (72, 2, -1), (92, 0, -1), (192, 0, -1), (292, 4, -10), (292, 1, -1), (192, 1, -1), (92, 1, -1), (92, 1, -1), (92, 0, -1), (192, 1, -1), (92, 5, -10), (92, 4, -10), (92, 5, -10), (92, 0, -1), (192, 5, -10), (192, 4, -10), (192, 0, -1), (292, 4, -10), (292, 0, -1), (392, 5, -10), (392, 1, -1), (292, 2, -1), (292, 2, -1), (292, 1, -1), (192, 4, -10), (192, 0, -1), (292, 3, -1), (272, 1, -1), (172, 2, -1), (192, 1, -1), (92, 5, -10), (92, 1, -1), (92, 3, -1), (72, 4, -10), (72, 4, -10), (72, 5, -10), (72, 5, -10), (72, 1, -1), (72, 0, -1), (172, 2, -1), (192, 5, -10), (192, 2, -1), (192, 3, -1), (172, 3, -1), (152, 1, -1), (52, 2, -1), (72, 5, -10), (72, 2, -1), (92, 0, -1), (192, 5, -10), (192, 4, -10), (192, 5, -10), (192, 0, -1), (292, 5, -10), (292, 2, -1), (292, 3, -1), (272, 2, -1), (292, 1, -1), (192, 3, -1), (172, 3, -1), (152, 3, -1), (152, 5, -10), (152, 3, -1), (152, 4, -10), (152, 0, -1), (252, 2, -1), (272, 0, -1), (372, 4, -10), (372, 2, -1), (392, 2, -1), (392, 1, -1), (292, 5, -10), (292, 3, -1), (272, 1, -1), (172, 1, -1), (72, 1, -1), (72, 0, -1), (172, 1, -1), (72, 0, -1), (172, 5, -10), (172, 4, -10), (172, 1, -1), (72, 4, -10), (72, 0, -1), (172, 0, -1), (272, 1, -1), (172, 0, -1), (272, 5, -10), (272, 3, -1), (252, 0, -1), (352, 4, -10), (352, 2, -1), (352, 1, -1), (252, 2, -1), (272, 2, -1), (292, 3, -1), (272, 0, -1), (372, 2, -1), (392, 5, -10), (392, 1, -1), (292, 0, -1), (392, 3, -1), (372, 1, -1), (272, 0, -1), (372, 3, -1), (372, 3, -1), (372, 3, -1), (372, 4, -10), (372, 2, -1), (392, 2, -1), (392, 0, -1), (492, 2, -1), (492, 0, -1), (492, 1, -1), (392, 5, -10), (392, 5, -10), (392, 2, -1), (392, 5, -10), (392, 2, -1), (392, 4, -10), (392, 1, -1), (292, 5, -10), (292, 0, -1), (392, 2, -1), (392, 1, -1), (292, 0, -1), (392, 0, -1), (492, 4, -10), (492, 3, -1), (472, 4, -1), (476, 4, -10), (476, 5, -1), (472, 0, -1), (472, 1, -1), (372, 0, -1), (472, 5, -10), (472, 0, -1), (472, 5, -10), (472, 4, -1), (476, 1, -1), (376, 3, -1), (376, 0, -1), (476, 5, -1), (472, 5, -10), (472, 5, -10), (472, 0, -1), (472, 4, -1), (476, 2, -1), (496, 4, -10), (496, 4, -10), (496, 3, -1), (476, 4, -10), (476, 3, -1), (476, 3, -1), (476, 1, -1), (376, 0, -1), (476, 2, -1), (496, 1, -1), (396, 4, -10), (396, 3, -1), (376, 4, -10), (376, 1, -1), (276, 5, -10), (276, 3, -1), (256, 3, -1), (236, 3, -1), (216, 0, -1), (316, 0, -1), (416, 4, -10), (416, 4, -10), (416, 5, -1), (408, 3, -1), (408, 1, -1), (308, 4, -10), (308, 2, -1), (308, 4, -10), (308, 2, -1), (308, 4, -10), (308, 4, -10), (308, 1, -1), (208, 3, -1), (208, 0, -1)]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "trajectory = [] \n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    trajectory.append( (obs, action, reward) )\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "env.close()\n",
    "#trajectory.append( (obs, None, None) )\n",
    "\n",
    "print(\"Trajetória como sequência de trios (STATE, ACTION, REWARD):\")\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2.Calcular os Retornos\n",
    "\n",
    "O *retorno (final)* $G$ é uma medida da recompensa total obtida ao longo de um episódio. \n",
    "\n",
    "Em um MDP, o objetivo é otimizar o valor médio de $G$, para infinitos episódios.\n",
    "\n",
    "Para isso, vamos assumir a política abaixo, que escolhe a ação $0$ com 50% de probabilidade, ou outra ação, caso contrário.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "\n",
    "def policy(state):\n",
    "    x = np.random.random()\n",
    "    if x <= 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.random.randint(1, num_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E vamos criar uma trajetória com esta política:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajetória:\n",
      "[(423, 0, -1), (423, 0, -1), (423, 2, -1), (443, 0, -1), (443, 0, -1), (443, 1, -1), (343, 2, -1), (343, 2, -1), (343, 1, -1), (243, 5, -10), (243, 0, -1), (343, 0, -1), (443, 0, -1), (443, 0, -1), (443, 5, -10), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 1, -1), (343, 0, -1), (443, 4, -10), (443, 0, -1), (443, 3, -1), (423, 0, -1), (423, 0, -1), (423, 2, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 5, -10), (443, 0, -1), (443, 4, -10), (443, 4, -10), (443, 0, -1), (443, 0, -1), (443, 3, -1), (423, 0, -1), (423, 2, -1), (443, 1, -1), (343, 0, -1), (443, 3, -1), (423, 3, -1), (423, 0, -1), (423, 3, -1), (423, 0, -1), (423, 1, -1), (323, 2, -1), (343, 0, -1), (443, 0, -1), (443, 2, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 4, -10), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 2, -1), (443, 0, -1), (443, 2, -1), (443, 0, -1), (443, 0, -1), (443, 2, -1), (443, 1, -1), (343, 4, -10), (343, 0, -1), (443, 5, -10), (443, 2, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 2, -1), (443, 0, -1), (443, 2, -1), (443, 5, -10), (443, 5, -10), (443, 1, -1), (343, 3, -1), (323, 5, -10), (323, 0, -1), (423, 1, -1), (323, 0, -1), (423, 0, -1), (423, 0, -1), (423, 0, -1), (423, 0, -1), (423, 2, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 4, -10), (443, 3, -1), (423, 0, -1), (423, 0, -1), (423, 2, -1), (443, 2, -1), (443, 1, -1), (343, 0, -1), (443, 2, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 1, -1), (343, 0, -1), (443, 0, -1), (443, 2, -1), (443, 1, -1), (343, 2, -1), (343, 0, -1), (443, 1, -1), (343, 4, -10), (343, 0, -1), (443, 2, -1), (443, 0, -1), (443, 0, -1), (443, 5, -10), (443, 0, -1), (443, 1, -1), (343, 4, -10), (343, 0, -1), (443, 1, -1), (343, 0, -1), (443, 5, -10), (443, 0, -1), (443, 0, -1), (443, 2, -1), (443, 2, -1), (443, 0, -1), (443, 0, -1), (443, 5, -10), (443, 0, -1), (443, 1, -1), (343, 4, -10), (343, 1, -1), (243, 0, -1), (343, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 5, -10), (443, 4, -10), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 4, -10), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 1, -1), (343, 0, -1), (443, 2, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 3, -1), (423, 0, -1), (423, 5, -10), (423, 0, -1), (423, 0, -1), (423, 5, -10), (423, 3, -1), (423, 0, -1), (423, 0, -1), (423, 5, -10), (423, 3, -1), (423, 2, -1), (443, 0, -1), (443, 0, -1), (443, 0, -1), (443, 5, -10), (443, 3, -1), (423, 0, -1), (423, 1, -1), (323, 0, -1), (423, 3, -1), (423, 0, -1), (423, 3, -1), (423, 0, -1), (423, 0, -1), (423, 0, -1), (423, 4, -10), (423, 5, -10), (423, 0, -1), (423, 0, -1)]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "trajectory = [] \n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = policy(obs)\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "    trajectory.append( (obs, action, reward) )\n",
    "    obs = next_obs\n",
    "\n",
    "env.close()\n",
    "#trajectory.append( (obs, None, None) )\n",
    "\n",
    "print(\"Trajetória:\")\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Retorno final do episódio ($G$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para um episódio com $n$ passos, o *retorno* (não-descontado) é calculado assim:\n",
    "\n",
    "$ G = R_1 + R_2 + R_3 + \\cdots + R_n = \\displaystyle\\sum_{i=1}^{n} R_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retorno não-descontado: -452.0\n"
     ]
    }
   ],
   "source": [
    "sum_rewards = 0.0\n",
    "for (s, a, r) in trajectory:\n",
    "    sum_rewards += r\n",
    "\n",
    "print(\"Retorno não-descontado:\", sum_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mais usado é o *retorno descontado* de um episódio.\n",
    "\n",
    "Neste caso, $G$ é uma soma que \"atenua\" recompensas mais distantes, valorizando mais as recompensas iniciais. (Você prefere receber 100 reais agora, de uma vez, ou em 100 parcelas de 1 real?)\n",
    "\n",
    "Para isso, a cada passo, a recompensa tem uma redução dada por um parâmetro $\\gamma\\;$, tal que $0 < \\gamma \\leq 1$.\n",
    "\n",
    "Para um episódio com $n$ passos, o *retorno descontado* é calculado assim:\n",
    "\n",
    "$ G = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\cdots + \\gamma^{(n-1)} R_n = \\displaystyle\\sum_{t=1}^{n} \\gamma^{(t-1)} R_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retorno descontado: -19.999298946675\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.95  # você escolhe esse valor\n",
    "\n",
    "step = 0\n",
    "discounted_sum_rewards = 0.0\n",
    "for (s, a, r) in trajectory:\n",
    "    discounted_sum_rewards += (GAMMA ** step) * reward\n",
    "    step += 1\n",
    "\n",
    "print(\"Retorno descontado:\", discounted_sum_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Retornos intermediários a cada passo ($G_i$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Também podemos calcular um retorno parcial, de um certo episódio, a partir de um passo específico $i$ do episódio:\n",
    "\n",
    "- $ G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\gamma^3 R_4 + \\gamma^4 R_5 + \\cdots \\;\\;= G$ \n",
    "- $ G_1 = \\;\\;\\;\\;\\;\\;\\;\\;\\;  R_2 + \\;\\gamma R_3 + \\;\\gamma^2 R_4 + \\;\\gamma^3 R_5  + \\cdots $ \n",
    "- $ G_2 = \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; R_3 + \\;\\gamma R_4 + \\;\\gamma^2 R_5 + \\cdots $ \n",
    "- $\\cdots$ \n",
    "- $ G_n = 0 $\n",
    "\n",
    "Propriedade:\n",
    "- $ G_{i-1} = R_i + \\gamma G_i $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-38.76052413182959, -39.747920138767995, -40.787284356597894, -41.88135195431357, -43.03300205717218, -44.24526532333914, -45.52133191930436, -46.86455991505722, -48.27848412111287, -49.766825390645124, -41.85981620067908, -43.010332842820084, -44.221402992442194, -45.496213676254946, -46.838119659215735, -38.77696806233235, -39.765229539297216, -40.8055047782076, -41.90053134548168, -43.05319088998072, -44.26651672629549, -45.54370181715315, -46.88810717595069, -48.30327071152704, -40.3192323279232, -41.38866560834022, -42.51438485088444, -43.6993524746152, -44.946686815384425, -46.25967033198361, -47.64175824419327, -49.096587625466604, -50.62798697417538, -52.239986288605664, -44.46314346169017, -45.75067732809492, -37.632291924310444, -29.08662307822152, -29.56486639812792, -30.068280419082022, -30.598189914823184, -31.155989384024405, -31.743146720025692, -32.361207073711256, -33.01179691969606, -33.69662833652217, -34.4175035121286, -35.176319486451156, -35.9750731436328, -36.815866466981895, -37.70091207050726, -38.63253902158659, -39.61319897009115, -40.64547260009595, -41.732076421153636, -42.875869917003826, -44.079863070530344, -45.347224284768785, -46.68128872080925, -48.08556707453605, -49.56375481530111, -41.64605770031696, -42.78532389507048, -43.98455146849525, -45.24689628262658, -46.57568029750166, -47.97440031315965, -49.446737171747, -50.99656544394421, -52.62796362520444, -54.34522486863625, -56.152868282775, -48.58196661344737, -50.08628064573408, -42.1960848902464, -43.364299884469894, -44.59399987838936, -45.888420924620384, -47.25096939433725, -48.68523094140763, -50.194979938323826, -51.784189408761925, -53.45704148290729, -55.21793840306031, -47.59782989795822, -39.57666305048234, -40.60701373734983, -41.69159340773667, -33.359572008143864, -34.062707376993544, -34.80284987051952, -35.58194723212581, -36.40204971802717, -37.265315492660186, -38.17401630806336, -39.13054348217196, -40.13741419175996, -41.19727809658943, -42.3129243121994, -43.48728874968358, -35.24977763124588, -36.05239750657461, -36.897260533236434, -37.78659003498572, -38.72272635261655, -39.70813300275426, -40.74540316079396, -41.83726648504628, -42.98659630004872, -44.19641715794602, -45.46991279783792, -46.81043452403992, -48.22151002530518, -49.70685265821598, -51.27037121917472, -52.91618023071023, -54.64861076916867, -56.472221862282815, -58.39181248661349, -60.41243419643526, -53.06572020677395, -54.806021270288376, -56.63791712661935, -58.56622855433616, -60.59603005719596, -53.258979007574695, -55.00945158692073, -56.85205430202183, -49.31795189686508, -50.86100199670009, -52.485265259684304, -54.19501606282559, -46.52106953981641, -47.91691530506991, -49.386226636915694, -50.932870144121786, -52.56091594118083, -54.27464835913772, -56.07857722014497, -48.50376549488945, -50.003963678831, -51.58311966192737, -43.77170490729197, -45.02284727083365, -46.33983923245648, -47.7261465604805, -49.18541743208474, -50.721492033773416, -52.338412667129916, -44.56675017592623, -36.386052816764455, -37.24847664922574, -38.15629120971131, -39.11188548390665, -40.117774193585944, -31.702920203774674, -32.3188633723944, -32.96722460252042, -33.64971010791623, -34.368115903069715, -35.12433252954707, -35.92035003110218, -36.758263190633876, -37.6402770427725, -38.56871267660264, -39.546013343792254, -40.574750888202374, -41.65763251389724, -42.79750790936552, -43.99737674670055, -45.26039657547427, -46.58989113207818, -47.98935908639809, -39.98879903831378, -41.04084109296188, -42.14825378206513, -33.84026713901593, -34.56870225159572, -35.335476054311286, -36.14260637295925, -27.51853302416763, -27.914245288597506, -28.330784514313166, -28.769246857171755, -29.230786165443956, -29.716617016256798, -20.754333701322945, -20.794035475076786, -20.835826815870302, -20.879817700916107, -20.926123895701167, -20.97486725863281, -21.026176061718747, -21.080185328124998, -21.1370371875, -21.19688125, -21.259875, -11.8525, -1.95, -1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# calcula os retornos a cada passo (G_i, para cada i=0...n) do episódio completo\n",
    "Gt = 0.0\n",
    "all_Gs = [ Gt ]\n",
    "for (s, a, r) in reversed(trajectory):   \n",
    "    Gt = r + GAMMA*Gt\n",
    "    all_Gs.insert(0, Gt)\n",
    "\n",
    "print(all_Gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Funções de Valor\n",
    "\n",
    "São funções que não fazem parte da essência de um MDP, mas são úteis para criar algoritmos.\n",
    "\n",
    "Todas elas fazem avaliações dos retornos esperados para uma política específica! Ambas são calculadas a partir dos retornos intermediários.\n",
    "\n",
    "Veremos dois tipos de função de valor, a seguir. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Função de valor do estado V(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcula o retorno esperado a partir de cada estado $s$.\n",
    "\n",
    "De forma matemática:\n",
    "\n",
    "$V(s) = E[G_t | S_t=s]$ \n",
    "\n",
    "---\n",
    "\n",
    "Uma explicação mais algorítmica do valor de $V(s)$ para um $s$ específico, seria esta:\n",
    "1. rode infinitos episódios com a política\n",
    "2. para cada episódio:\n",
    "   - examine se o estado $s$ ocorre em algum passo $t$ (qualquer)\n",
    "   - se ocorrer, salve os possíveis valores $G_t$\n",
    "3. Tire a média dos valores salvos\n",
    "\n",
    "---\n",
    "\n",
    "Vamos implementar esta ideia rodando apenas 1 episódio, e vamos calcular o $V(s)$ para todo estado que encontrarmos. Para isso, vamos anexar cada retorno a um dicionário \"returns-history\" (um histórico dos retornos) indexado pelo $s$ onde se originou cada retorno intermediário.\n",
    "\n",
    "Esta implementação assume ambiente de *estado discreto*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.         -45.54982962   0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.         -33.68694901   0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.         -47.79925908   0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.         -31.92788963   0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.         -44.05103077   0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.        ]\n"
     ]
    }
   ],
   "source": [
    "returns_history = dict()\n",
    "V = np.zeros(env.observation_space.n)\n",
    "\n",
    "# calcula os retornos a cada passo (G_i, para cada i=0...n) do episódio completo\n",
    "G = 0.0\n",
    "for (s, a, r) in reversed(trajectory):   \n",
    "    G = r + GAMMA*G\n",
    "    \n",
    "    if s in returns_history.keys():\n",
    "        returns_history[s].append(G)\n",
    "    else:\n",
    "        returns_history[s] = [ G ]\n",
    "    \n",
    "    V[s] = np.mean( returns_history[s] )\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Função de valor do estado-ação Q(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De maneira análoga ao $V(s)$, o $Q(s,a)$ representa o retorno esperado a partir do par $(s,a)$. \n",
    "\n",
    "Em outras palavras, $Q(s,a)$ responde a esta pergunta:\n",
    "\n",
    "*\"Quando estava no estado $s$ e fez a ação $a$, qual o retorno esperado (se continuar seguindo a política no restante do episódio)?\"*\n",
    "\n",
    "A definição e a forma de calcular é análoga ao $V(s)$, mas vamos usar um \"returns_history\" indexado pelo par $(s,a)$ onde se originou cada retorno $G_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "returns_history = dict()\n",
    "Q = np.zeros(shape=(env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# calcula os retornos a cada passo (G_i, para cada i=0...n) do episódio completo\n",
    "Gt = 0.0\n",
    "for (s, a, r) in reversed(trajectory):   \n",
    "    Gt = r + GAMMA*Gt\n",
    "    \n",
    "    if (s,a) in returns_history.keys():\n",
    "        returns_history[s,a].append(Gt)\n",
    "    else:\n",
    "        returns_history[s,a] = [ Gt ]\n",
    "    \n",
    "    Q[s,a] = np.mean( returns_history[s,a] )\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o $Q$ calculado para estados discretos e ações discretas pode ser representado como uma matriz, tal como fizemos acima. E uma matriz assim pode ser vista como uma _tabela_ estado x ação. Por esse motivo, chamamos essa representação de **Q-table** (tabela Q)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Introdução aos Métodos Baseados em Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponha que você partiu de uma política qualquer (provavelmente ruim) e calculou o *Q* dessa política com uma Q-table.\n",
    "\n",
    "**De que forma você poderia melhorar a política?**\n",
    "\n",
    "**Ou, como escolher a melhor ação a cada estado, usando a Q-table?**\n",
    "\n",
    "---\n",
    "\n",
    "No próxima parte do curso, veremos um algoritmo para RL, da família Monte-Carlo, onde a política é implicitamente representada pelo $Q$. \n",
    "\n",
    "Este método repete $N$ vezes esses passos:\n",
    "1. Rode um episódio, usando a política representada pela tabela $Q$\n",
    "   - salve a trajetória\n",
    "1. Depois, calcule os valores de $G_t$ e use esses valores para atualizar $Q$\n",
    "   - ao atualizar $Q$, a política também muda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "47acfd36b4a698d100796428813311ecacef03b489c77dd1fdf080373e214244"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
