{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MDPs, Trajetórias e Retornos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pablo\\anaconda3\\envs\\rlx\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# vamos focar nesses três ambientes por serem mais simples\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "#env = gym.make(\"CartPole-v1\")\n",
    "#env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figura mostrando interação agente(política)-ambiente](figura_mdp.png \"Interação agente-ambiente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Episódio e Trajetória"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um *episódio* é uma execução da tarefa (ou do ambiente gym). \n",
    "\n",
    "E a *trajetória* é a sequência de estados (observações), ações e recompensas do episódio. Assumindo um episódio de $T$ passos (ações aplicadas):\n",
    "\n",
    "$S_0 \\rightarrow A_0 \\rightarrow R_1 \\rightarrow S_1 \\rightarrow A_1 \\rightarrow R_2 \\rightarrow S_2 \\rightarrow \\cdots S_{T-1} \\rightarrow A_{T-1} \\rightarrow R_T \\rightarrow S_T$\n",
    "\n",
    "Vamos ilustrar um episódio em um MDP usando o ambiente *\"env\"* escolhido no código acima. \n",
    "\n",
    "Estamos assumindo que o episódio encerrou de fato (chegou em um estado final) em *$n$=\"TOTAL_STEPS\"* passos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0 = [-0.4965765  0.       ]\n",
      " A0 = 1\n",
      "  R1 = -1.0\n",
      "S1 = [-4.9677894e-01 -2.0244533e-04]\n",
      " A1 = 0\n",
      "  R2 = -1.0\n",
      "S2 = [-0.4981823  -0.00140338]\n",
      " A2 = 0\n",
      "  R3 = -1.0\n",
      "S3 = [-0.5007761  -0.00259382]\n",
      " A3 = 1\n",
      "  R4 = -1.0\n",
      "S4 = [-0.503541   -0.00276485]\n",
      " A4 = 0\n",
      "  R5 = -1.0\n",
      "S5 = [-0.5074562 -0.0039152]\n"
     ]
    }
   ],
   "source": [
    "TOTAL_STEPS = 5\n",
    "\n",
    "i = 0\n",
    "obs = env.reset()\n",
    "print(f\"S0 = {obs}\")\n",
    "\n",
    "done = False\n",
    "\n",
    "# roda apenas alguns passos\n",
    "for i in range(0,TOTAL_STEPS):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    print(f\" A{i} = {action}\")\n",
    "\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    print(f\"  R{i+1} = {reward}\")\n",
    "    print(f\"S{i+1} = {next_obs}\")\n",
    "\n",
    "    obs = next_obs\n",
    "    time.sleep(0.1)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os detalhes do *episódio* que mostramos acima são chamamos de *trajectory* ou *rollout*.\n",
    "\n",
    "Dependendo do algoritmo, vamos precisar analisar essas informações em trios (S,A,R) ou quádruplas (S,A,R,S) ou até quíntuplas (S,A,R,S',A').\n",
    "\n",
    "Abaixo, vamos agrupar e guardar em trio, para preparar para o algoritmo Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajetória como sequência de trios (STATE, ACTION, REWARD):\n",
      "[(array([-0.5791818,  0.       ], dtype=float32), 0, -1.0), (array([-0.57976687, -0.00058506], dtype=float32), 2, -1.0), (array([-0.57893264,  0.00083421], dtype=float32), 2, -1.0), (array([-0.5766853 ,  0.00224731], dtype=float32), 1, -1.0), (array([-0.57404155,  0.00264378], dtype=float32), 1, -1.0)]\n"
     ]
    }
   ],
   "source": [
    "TOTAL_STEPS = 5\n",
    "\n",
    "i = 0\n",
    "obs = env.reset()\n",
    "trajectory = [] \n",
    "\n",
    "done = False\n",
    "\n",
    "for i in range(0,TOTAL_STEPS):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    trajectory.append( (obs, action, reward) )\n",
    "\n",
    "    obs = next_obs\n",
    "    time.sleep(0.1)\n",
    "\n",
    "env.close()\n",
    "#trajectory.append( (obs, None, None) )\n",
    "\n",
    "print(\"Trajetória como sequência de trios (STATE, ACTION, REWARD):\")\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2.Calcular os Retornos\n",
    "\n",
    "O *retorno (final)* $G$ é uma medida da recompensa total obtida ao longo de um episódio. \n",
    "\n",
    "Em um MDP, o objetivo é otimizar o valor médio de $G$, para infinitos episódios.\n",
    "\n",
    "Para isso, vamos assumir a política abaixo, que escolhe a ação $0$ com 50% de probabilidade, ou outra ação, caso contrário.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "\n",
    "def policy(state):\n",
    "    x = np.random.random()\n",
    "    if x <= 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.random.randint(1, num_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Retorno final do episódio ($G$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para um episódio com $n$ passos, o *retorno* (não-descontado) é calculado assim:\n",
    "\n",
    "$ G = R_1 + R_2 + R_3 + \\cdots + R_n = \\displaystyle\\sum_{i=1}^{n} R_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retorno não-descontado: -200.0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "obs = env.reset()\n",
    "trajectory = [] \n",
    "\n",
    "done = False\n",
    "sum_rewards = 0.0\n",
    "\n",
    "while not done:\n",
    "    #env.render()\n",
    "    action = policy(obs)\n",
    "\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "    sum_rewards += reward\n",
    "\n",
    "    obs = next_obs\n",
    "    time.sleep(0.1)\n",
    "\n",
    "env.close()\n",
    "#trajectory.append( (obs, None, None) )\n",
    "\n",
    "print(\"Retorno não-descontado:\", sum_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mais usado é o *retorno descontado* de um episódio.\n",
    "\n",
    "Neste caso, $G$ é uma soma que \"atenua\" recompensas mais distantes, valorizando mais as recompensas iniciais. (Você prefere receber 100 reais agora, de uma vez, ou em 100 parcelas de 1 real?)\n",
    "\n",
    "Para isso, a cada passo, a recompensa tem uma redução dada por um parâmetro $\\gamma\\;$, tal que $0 < \\gamma \\leq 1$.\n",
    "\n",
    "Para um episódio com $n$ passos, o *retorno descontado* é calculado assim:\n",
    "\n",
    "$ G = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\cdots + \\gamma^{(n-1)} R_n = \\displaystyle\\sum_{t=1}^{n} \\gamma^{(t-1)} R_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retorno descontado: -19.999298946675\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "obs = env.reset()\n",
    "trajectory = [] \n",
    "\n",
    "done = False\n",
    "discounted_sum_rewards = 0.0\n",
    "gamma = 0.95\n",
    "\n",
    "step = 0\n",
    "\n",
    "while not done:\n",
    "    #env.render()\n",
    "    action = policy(obs)\n",
    "\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "    discounted_sum_rewards += (gamma ** step) * reward\n",
    "    step += 1\n",
    "    \n",
    "    obs = next_obs\n",
    "    time.sleep(0.1)\n",
    "\n",
    "env.close()\n",
    "#trajectory.append( (obs, None, None) )\n",
    "\n",
    "print(\"Retorno descontado:\", discounted_sum_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Retornos intermediários a cada passo ($G_i$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Também podemos calcular um retorno parcial, de um certo episódio, a partir de um passo específico $i$ do episódio:\n",
    "\n",
    "- $ G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\gamma^3 R_4 + \\gamma^4 R_5 + \\cdots \\;\\;= G$ \n",
    "- $ G_1 = \\;\\;\\;\\;\\;\\;\\;\\;\\;  R_2 + \\;\\gamma R_3 + \\;\\gamma^2 R_4 + \\;\\gamma^3 R_5  + \\cdots $ \n",
    "- $ G_2 = \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; R_3 + \\;\\gamma R_4 + \\;\\gamma^2 R_5 + \\cdots $ \n",
    "- $\\cdots$ \n",
    "- $ G_n = 0 $\n",
    "\n",
    "Propriedade:\n",
    "- ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$ G_{i-1} = r_i + \\gamma G_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Funções de Valor\n",
    "\n",
    "São funções que não fazem parte da essência de um MDP, mas são úteis para criar algoritmos.\n",
    "\n",
    "Veremos dois tipos de função de valor. Ambas são calculadas a partir dos retornos intermediários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Função de valor do estado V(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcula o retorno esperado a partir de todo estado $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fica como exercício"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Função de valor do estado-ação Q(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcula o retorno esperado a partir do par $(s,a)$.\n",
    "\n",
    "*\"Quando estava no estado $s$ e fez a ação $a$, qual a recompensa futura esperada?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos fazer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Introdução aos Métodos Baseados em Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponha que você partiu de uma política qualquer (provavelmente ruim) e calculou o *Q* dessa política.\n",
    "\n",
    "De que forma poderíamos melhorá-la?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "17935451bad8d760d3ec9e03731ab7fc392b610a70f377bd5ec1362f80c504be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
