{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MDPs, Trajetórias e Retornos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# vamos focar nesses três ambientes por serem mais simples\n",
    "#env = gym.make(\"MountainCar-v0\")\n",
    "#env = gym.make(\"CartPole-v1\")\n",
    "env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figura mostrando interação agente(política)-ambiente](mdp.png \"Interação agente - ambiente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Episódio e Trajetória"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um *episódio* é uma execução da tarefa (ou do ambiente gym). \n",
    "\n",
    "E a *trajetória* é a sequência de estados (observações), ações e recompensas do episódio. Assumindo um episódio de $T$ passos (ações aplicadas):\n",
    "\n",
    "$S_0 \\rightarrow A_0 \\rightarrow R_1 \\rightarrow S_1 \\rightarrow A_1 \\rightarrow R_2 \\rightarrow S_2 \\rightarrow \\cdots S_{T-1} \\rightarrow A_{T-1} \\rightarrow R_T \\rightarrow S_T$\n",
    "\n",
    "Vamos ilustrar um episódio em um MDP usando o ambiente *\"env\"* escolhido no código acima. \n",
    "\n",
    "Estamos assumindo que o episódio encerrou de fato (chegou em um estado final) em *$n$=\"TOTAL_STEPS\"* passos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0 = 111\n",
      " A0 = 1\n",
      "  R1 = -1\n",
      "S1 = 11\n",
      " A1 = 1\n",
      "  R2 = -1\n",
      "S2 = 11\n",
      " A2 = 4\n",
      "  R3 = -10\n",
      "S3 = 11\n",
      " A3 = 2\n",
      "  R4 = -1\n",
      "S4 = 31\n",
      " A4 = 3\n",
      "  R5 = -1\n",
      "S5 = 11\n"
     ]
    }
   ],
   "source": [
    "TOTAL_STEPS = 5\n",
    "\n",
    "i = 0\n",
    "obs = env.reset()\n",
    "print(f\"S0 = {obs}\")\n",
    "\n",
    "done = False\n",
    "\n",
    "# roda apenas alguns passos\n",
    "for i in range(0,TOTAL_STEPS):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    print(f\" A{i} = {action}\")\n",
    "\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    print(f\"  R{i+1} = {reward}\")\n",
    "    print(f\"S{i+1} = {next_obs}\")\n",
    "\n",
    "    obs = next_obs\n",
    "    time.sleep(0.1)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os detalhes do *episódio* que mostramos acima são chamamos de *trajectory* ou *rollout*.\n",
    "\n",
    "Dependendo do algoritmo, vamos precisar analisar essas informações em trios (S,A,R) ou quádruplas (S,A,R,S) ou até quíntuplas (S,A,R,S',A').\n",
    "\n",
    "Abaixo, vamos agrupar e guardar em trio, para preparar para o algoritmo Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pular?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2.Calcular os Retornos\n",
    "\n",
    "O *retorno (final)* $G$ é uma medida da recompensa total obtida ao longo de um episódio. \n",
    "\n",
    "Em um MDP, o objetivo é otimizar o valor médio de $G$, para infinitos episódios.\n",
    "\n",
    "Para isso, vamos assumir a política abaixo, que escolhe a ação $0$ com 50% de probabilidade, ou outra ação, caso contrário.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "\n",
    "def policy(state):\n",
    "    x = np.random.random()\n",
    "    if x <= 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.random.randint(1, num_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Retorno final do episódio ($G$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para um episódio com $n$ passos, o *retorno* (não-descontado) é calculado assim:\n",
    "\n",
    "$ G = R_1 + R_2 + R_3 + \\cdots + R_n = \\displaystyle\\sum_{i=1}^{n} R_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mais usado é o *retorno descontado* de um episódio.\n",
    "\n",
    "Neste caso, $G$ é uma soma que \"atenua\" recompensas mais distantes, valorizando mais as recompensas iniciais. (Você prefere receber 100 reais agora, de uma vez, ou em 100 parcelas de 1 real?)\n",
    "\n",
    "Para isso, a cada passo, a recompensa tem uma redução dada por um parâmetro $\\gamma\\;$, tal que $0 < \\gamma \\leq 1$.\n",
    "\n",
    "Para um episódio com $n$ passos, o *retorno descontado* é calculado assim:\n",
    "\n",
    "$ G = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\cdots + \\gamma^{(n-1)} R_n = \\displaystyle\\sum_{i=1}^{n} R_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deixo como exercício\n",
    "# raramento vamos calcular diretamente, durante a execução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Retornos intermediários a cada passo ($G_i$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Também podemos calcular um retorno parcial, de um certo episódio, a partir de um passo específico $i$ do episódio:\n",
    "\n",
    "- $ G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\gamma^3 R_4 + \\gamma^4 R_5 \\cdots \\;\\;= G$ \n",
    "- $ G_1 = \\;\\;\\;\\;\\;\\;\\;\\;\\;  R_2 + \\;\\gamma R_3 + \\gamma^2 R_4 + \\cdots $ \n",
    "- $ G_2 = \\;\\;\\;\\;\\;\\;\\;\\;\\; \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; R_3 + \\;\\gamma R_4 + \\cdots $ \n",
    "- $\\cdots$ \n",
    "- $ G_n = 0 $\n",
    "\n",
    "Propriedade:\n",
    "- $ G_{i-1} = r_i + \\gamma G_i $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Funções de Valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Função de valor do estado V(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcula o retorno esperado a partir de todo estado $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fica como exercício"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Função de valor do estado-ação Q(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcula o retorno esperado a partir do par $(s,a)$.\n",
    "\n",
    "*\"Quando estava no estado $s$ e fez a ação $a$, qual a recompensa futura esperada?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos fazer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Introdução aos Métodos Baseados em Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponha que você partiu de uma política qualquer (provavelmente ruim) e calculou o *Q* dessa política.\n",
    "\n",
    "De que forma poderíamos melhorá-la?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "17935451bad8d760d3ec9e03731ab7fc392b610a70f377bd5ec1362f80c504be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
