{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SeJ3wCaKe2Wl"
      },
      "source": [
        "# Capítulo 6 - Tarefas Continuadas - Differential Q-Learning\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vgUyA2Di5J_n"
      },
      "source": [
        "Você pode rodar este notebook localmente ou no Colab. Para abrir diretamente no Colab, basta clicar no link abaixo.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap06/cap06-main-1.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uAHITU7VhsM7"
      },
      "source": [
        "## Configurações Iniciais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS23BU8R1vq-"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install gym==0.23.1\n",
        "    !pip install optuna\n",
        "\n",
        "    # clone repository\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "\n",
        "    clear_output()\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gzf7VhkiHxQ"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import optuna\n",
        "\n",
        "from util.experiments import repeated_exec, repeated_exec_greedy_Q\n",
        "from util.plot import plot_result, plot_multiple_results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U-xEwtye5J_r"
      },
      "source": [
        "## 1 - Tarefa Continuada (Infinita)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TwoChoice(gym.Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define the action and observation spaces\n",
        "        self.action_space = spaces.Discrete(2)  # Two discrete actions: 0 and 1\n",
        "        self.observation_space = spaces.Discrete(9)  # Nine discrete states: 0 to 8\n",
        "\n",
        "        # Set the initial state\n",
        "        self.current_state = 0\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the environment to the initial state\n",
        "        self.current_state = 0\n",
        "        return self.current_state\n",
        "\n",
        "    def step(self, action):\n",
        "        # Perform the specified action and transition to the next state\n",
        "        if action != 0 and action != 1:\n",
        "            raise ValueError(\"Invalid action!\")\n",
        "        \n",
        "        reward = 0.0\n",
        "\n",
        "        if self.current_state == 0:\n",
        "            # left\n",
        "            if action == 0:\n",
        "                reward = 1.0\n",
        "                self.current_state = 1\n",
        "            # right\n",
        "            else:\n",
        "                reward = 0.0\n",
        "                self.current_state = 5\n",
        "        elif self.current_state == 4:\n",
        "            reward = 0.0\n",
        "            self.current_state = 0\n",
        "        elif self.current_state == 8:\n",
        "            reward = 2.0\n",
        "            self.current_state = 0\n",
        "        else:\n",
        "            reward = 0.0\n",
        "            self.current_state += 1\n",
        "\n",
        "        return self.current_state, reward, False, {}\n",
        "\n",
        "    def render(self, mode=None):\n",
        "        # Display the current state (optional)\n",
        "        print(\"Current state:\", self.current_state)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 - Q-Learning (parando por passos)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O mesmo **Q-learning** que vimos antes, porém com um critério de parada dado como uma quantidade de passos (não importa a quantidade de episódios envolvida)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Esta é a política. Neste caso, escolhe uma ação com base nos valores\n",
        "# da tabela Q, usando uma estratégia epsilon-greedy.\n",
        "def epsilon_greedy(Q, state, num_actions, epsilon):\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(0, num_actions)\n",
        "    else:\n",
        "        return np.argmax(Q[state])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Algoritmo Q-learning\n",
        "def run_qlearning_step(env, total_steps, lr=0.1, gamma=0.95, epsilon=0.1):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "    \n",
        "    # inicializa a tabela Q com valores aleatórios de -1.0 a 0.0\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.random.uniform(low = -1.0, high = 0.0, \n",
        "                          size = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    sum_rewards_per_step = []\n",
        "\n",
        "    state = env.reset()\n",
        "    sum_rewards, reward = 0, 0\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(total_steps):\n",
        "        \n",
        "        # escolhe a próxima ação -- usa epsilon-greedy\n",
        "        action = epsilon_greedy(Q, state, num_actions, epsilon)\n",
        "    \n",
        "        # realiza a ação, ou seja, dá um passo no ambiente\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        if done: \n",
        "            # para estados terminais\n",
        "            V_next_state = 0\n",
        "            next_state = env.reset()\n",
        "        else:\n",
        "            # para estados não-terminais -- valor máximo (melhor ação)\n",
        "            V_next_state = np.max(Q[next_state])\n",
        "\n",
        "        # atualiza a Q-table\n",
        "        # delta = (estimativa usando a nova recompensa) - estimativa antiga\n",
        "        delta = (reward + gamma * V_next_state) - Q[state,action]\n",
        "        Q[state,action] = Q[state,action] + lr * delta\n",
        "        \n",
        "        sum_rewards += reward\n",
        "        sum_rewards_per_step.append(sum_rewards)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # a cada 1000 passos, imprime informação sobre o progresso \n",
        "        if (i+1) % 1000 == 0:\n",
        "            avg_reward = np.mean(sum_rewards_per_step[-100:])\n",
        "            print(f\"Step {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    return sum_rewards_per_step, Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x5E-zx85J_u"
      },
      "outputs": [],
      "source": [
        "TOTAL_STEPS = 100\n",
        "LR = 0.3\n",
        "GAMMA = 0.70   # só vai dar a política ótima para valores a partir de 0.85\n",
        "EPSILON = 0.1\n",
        "\n",
        "rmax = 2*TOTAL_STEPS\n",
        "env = TwoChoice()\n",
        "\n",
        "rewards, qtable = run_qlearning_step(env, TOTAL_STEPS, LR, GAMMA, EPSILON)\n",
        "print(\"Acumulado final =\", (rewards[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jfj6LvPI5J_u"
      },
      "outputs": [],
      "source": [
        "# Mostra um gráfico de episódios x retornos não descontados\n",
        "plot_result(rewards, rmax, None, window=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 - Differential Q-Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Um algoritmo específico para tarefas continuadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Algoritmo Differential Q-learning\n",
        "def run_differential_qlearning_step(env, total_steps, lr=0.1, eta=1.0, epsilon=0.1):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "    \n",
        "    # inicializa a tabela Q com valores aleatórios de -1.0 a 0.0\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.random.uniform(low = -1.0, high = 0.0, \n",
        "                          size = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    sum_rewards_per_step = []\n",
        "\n",
        "    state = env.reset()\n",
        "    sum_rewards, reward = 0, 0\n",
        "    mean_reward = 0.0\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(total_steps):\n",
        "        \n",
        "        # escolhe a próxima ação -- usa epsilon-greedy\n",
        "        action = epsilon_greedy(Q, state, num_actions, epsilon)\n",
        "    \n",
        "        # realiza a ação, ou seja, dá um passo no ambiente\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        assert not done, \"This algorithm is for continuing tasks!\"\n",
        "\n",
        "        # para estados não-terminais -- valor máximo (melhor ação)\n",
        "        V_next_state = np.max(Q[next_state])\n",
        "\n",
        "        # atualiza a Q-table\n",
        "        # delta = (estimativa usando a nova recompensa) - estimativa antiga\n",
        "        delta = (reward - mean_reward + V_next_state) - Q[state,action]\n",
        "        Q[state,action] = Q[state,action] + lr * delta\n",
        "\n",
        "        # atualiza a recompensa média\n",
        "        mean_reward += eta * lr * delta\n",
        "        \n",
        "        sum_rewards += reward\n",
        "        sum_rewards_per_step.append(sum_rewards)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # a cada 1000 passos, imprime informação sobre o progresso \n",
        "        if (i+1) % 1000 == 0:\n",
        "            avg_reward = np.mean(sum_rewards_per_step[-100:])\n",
        "            print(f\"Step {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    return sum_rewards_per_step, Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TOTAL_STEPS = 100\n",
        "LR = 0.3\n",
        "ETA = 1.0\n",
        "EPSILON = 0.1\n",
        "\n",
        "rmax = 2*TOTAL_STEPS\n",
        "env = TwoChoice()\n",
        "\n",
        "rewards, qtable = run_differential_qlearning_step(env, TOTAL_STEPS, LR, ETA, EPSILON)\n",
        "print(\"Acumulado final =\", (rewards[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostra um gráfico de episódios x retornos não descontados\n",
        "plot_result(rewards, rmax, None, window=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M8roKzCgsuCl"
      },
      "source": [
        "## 4 - Otimizando Parâmetros\n",
        "\n",
        "Vamos usar a biblioteca *Optuna* para otimizar (hiper-)parâmetros dos algoritmos de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENV   = TwoChoice()\n",
        "RUNS_PER_TRIAL = 10"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MBsJRMCstj0N"
      },
      "source": [
        "### 4.1 - Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU8DNpphvcRa"
      },
      "outputs": [],
      "source": [
        "def create_train_fn(fixed_gamma):\n",
        "    def train(trial : optuna.Trial):\n",
        "        # chama os métodos do \"trial\" (tentativa) para sugerir valores para os parâmetros\n",
        "        lr = trial.suggest_uniform('lr', 0.1, 1.0)\n",
        "        eps = trial.suggest_uniform('epsilon', 0.01, 0.2)\n",
        "\n",
        "        print(f\"\\nTRIAL #{trial.number}: lr={lr}, eps={eps}\")\n",
        "\n",
        "        # roda o algoritmo várias vezes\n",
        "        results = repeated_exec(RUNS_PER_TRIAL, \"qlearn-optuna\", run_qlearning_step, ENV, 100, lr=lr, epsilon=eps, gamma=fixed_gamma)\n",
        "        \n",
        "        # soma dos retornos não-descontado finais\n",
        "        return np.sum(results[1][:,-1])\n",
        "    return train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTVBjiirtOUP"
      },
      "outputs": [],
      "source": [
        "GAMMA = 0.70\n",
        "study = optuna.create_study(direction='maximize',\n",
        "                        storage='sqlite:///optuna_continuing.db',\n",
        "                        study_name=f'qlearning-g{GAMMA}',\n",
        "                        load_if_exists=True)\n",
        "\n",
        "study.optimize(create_train_fn(GAMMA), n_trials=100)\n",
        "clear_output()\n",
        "\n",
        "print(\"MELHORES PARÂMETROS PARA GAMMA\", GAMMA, \":\")\n",
        "print(study.best_params)\n",
        "qlearn_params_g07 = study.best_params\n",
        "qlearn_params_g07['gamma'] = GAMMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GAMMA = 0.95\n",
        "study = optuna.create_study(direction='maximize',\n",
        "                        storage='sqlite:///optuna_continuing.db',\n",
        "                        study_name=f'qlearning-g{GAMMA}',\n",
        "                        load_if_exists=True)\n",
        "\n",
        "study.optimize(create_train_fn(GAMMA), n_trials=100)\n",
        "clear_output()\n",
        "\n",
        "print(\"MELHORES PARÂMETROS PARA GAMMA\", GAMMA, \":\")\n",
        "print(study.best_params)\n",
        "qlearn_params_g09 = study.best_params\n",
        "qlearn_params_g09['gamma'] = GAMMA"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_N6eLcX7yfqd"
      },
      "source": [
        "### 4.2 - Differential Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_6xX6d_0lht"
      },
      "outputs": [],
      "source": [
        "def train_diff(trial : optuna.Trial):\n",
        "    # chama os métodos do \"trial\" (tentativa) para sugerir valores para os parâmetros\n",
        "    lr = trial.suggest_uniform('lr', 0.1, 1.0)\n",
        "    eps = trial.suggest_uniform('epsilon', 0.01, 0.20)\n",
        "    eta = trial.suggest_uniform('eta', 0.1, 2.0)\n",
        "\n",
        "    print(f\"\\nTRIAL #{trial.number}: lr={lr}, eps={eps}, eta={eta}\")\n",
        "\n",
        "    # roda o algoritmo várias vezes\n",
        "    results = repeated_exec(RUNS_PER_TRIAL, \"diff-q-optuna\", run_differential_qlearning_step, ENV, 100, lr=lr, epsilon=eps, eta=eta)\n",
        "    \n",
        "    # soma dos retornos não-descontado finais\n",
        "    return np.sum(results[1][:,-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction='maximize',\n",
        "                        storage='sqlite:///optuna_continuing.db',\n",
        "                        study_name='diff-qlearning',\n",
        "                        load_if_exists=True)\n",
        "\n",
        "study.optimize(train_diff, n_trials=100)\n",
        "clear_output()\n",
        "\n",
        "print(\"MELHORES PARÂMETROS:\")\n",
        "print(study.best_params)\n",
        "diff_qlearn_params = study.best_params"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yF9fggfZzFVV"
      },
      "source": [
        "## 5 - Experimentos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 - Desempenho no Treinamento"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparando os dois usando os parâmetros ótimos obtidos antes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYSG9xiHxBKe"
      },
      "outputs": [],
      "source": [
        "environment = TwoChoice()\n",
        "NUM_STEPS = 200\n",
        "RUNS = 200\n",
        "\n",
        "results = []\n",
        "\n",
        "results.append( repeated_exec(RUNS, f\"Q-Learning (g=0.7)\", run_qlearning_step, environment, NUM_STEPS, **qlearn_params_g07) )\n",
        "clear_output()\n",
        "\n",
        "results.append( repeated_exec(RUNS, f\"Q-Learning (g=0.9)\", run_qlearning_step, environment, NUM_STEPS, **qlearn_params_g09) )\n",
        "clear_output()\n",
        "\n",
        "results.append( repeated_exec(RUNS, f\"Diff Q-Learning\", run_differential_qlearning_step, environment, NUM_STEPS, **diff_qlearn_params) )\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgbhKstd8iNB"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results, cumulative=False, x_log_scale=False, window=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_multiple_results(results[0:1], cumulative=False, x_log_scale=False, window=1, plot_stddev=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_multiple_results(results[1:2], cumulative=False, x_log_scale=False, window=1, plot_stddev=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_multiple_results(results[2:], cumulative=False, x_log_scale=False, window=1, plot_stddev=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 - Desempenho Pós-Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "environment = TwoChoice()\n",
        "NUM_STEPS = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_, qtable1 = run_qlearning_step(environment, NUM_STEPS, **qlearn_params_g07)\n",
        "#_, qtable2 = run_qlearning_step(environment, NUM_STEPS, **qlearn_params_g09)\n",
        "_, qtable3 = run_differential_qlearning_step(environment, NUM_STEPS, **diff_qlearn_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "results.append( repeated_exec_greedy_Q(1, \"QLearn(0.7)-Greedy\", qtable1, environment, 1000) )\n",
        "#results.append( repeated_exec_greedy_Q(1, \"QLearn(0.9)-Greedy\", qtable2, environment, 1000) )\n",
        "results.append( repeated_exec_greedy_Q(1, \"Diff-QLearn-Greedy\", qtable3, environment, 1000) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_multiple_results(results, cumulative=False, x_log_scale=False, window=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "cap06-main-1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('rlx')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "27dbc9ce4cc602e4f15257b7b0018d8dff5b9ce9a7d73bc4399cb5afb1e02c4a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
