{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeJ3wCaKe2Wl"
      },
      "source": [
        "# Capítulo 6 - Reward Shaping e Curriculum Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AT7LdIRGf4I"
      },
      "source": [
        "Vamos praticar essas duas técnicas no ambiente **\"MountainCar-v0\"** do gym.\n",
        "\n",
        "Você pode rodar este notebook no Colab ou localmente. Para abrir diretamente no Colab, basta clicar no botão abaixo.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap06/cap06-other-techniques.ipynb) \n",
        "\n",
        "Para rodar localmente, primeiro, baixe todo o repositório do github: https://github.com/pablo-sampaio/rl_facil."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAHITU7VhsM7"
      },
      "source": [
        "## 1. Configurações Iniciais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG4byJmEPE3Q"
      },
      "source": [
        "### Cria Diretório para Experimentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zHXAzRPLO8-W"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "J� existe uma subpasta ou um arquivo results.\n"
          ]
        }
      ],
      "source": [
        "!mkdir results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyTpn6lxGf4J"
      },
      "source": [
        "### Configurações Dependentes do Sistema\n",
        "\n",
        "Rode a célula abaixo, mesmo sem estar no Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NS23BU8R1vq-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "..\\cap04\\util_experiments.py\n",
            "..\\cap04\\util_plot.py\n",
            "        2 arquivo(s) copiado(s).\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from IPython.display import clear_output\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    !pip install gym\n",
        "    !pip install gym[box2d]\n",
        "    !pip install optuna\n",
        "\n",
        "    # para salvar videos\n",
        "    !apt-get install -y xvfb x11-utils\n",
        "    !pip install pyvirtualdisplay==0.2.*\n",
        "    !apt-get install ffmpeg\n",
        "\n",
        "    from pyvirtualdisplay import Display\n",
        "    global display\n",
        "    display = Display(visible=False, size=(1400, 900))\n",
        "    _ = display.start()\n",
        "\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    clear_output()\n",
        "\n",
        "    !mv /content/rl_facil/cap04/* /content/\n",
        "    !mv /content/rl_facil/cap05/* /content/\n",
        "    !mv /content/rl_facil/cap06/* /content/\n",
        "\n",
        "else:\n",
        "    # atenção: para Windows apenas! \n",
        "    # se estiver no Linux, troque por \"copy\" por \"cp\" e mude a barra\n",
        "    !copy ..\\cap04\\util_*.py .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNQSmOCRGf4L"
      },
      "source": [
        "### Configurações para Exibir Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xTUzR6SNGf4M"
      },
      "outputs": [],
      "source": [
        "# ideias adaptadas de : https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google\n",
        "from base64 import b64encode\n",
        "from IPython.display import HTML\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def render_mp4(videopath: str) -> str:\n",
        "  \"\"\"\n",
        "  Gets a string containing a b4-encoded version of the MP4 video\n",
        "  at the specified path.\n",
        "  \"\"\"\n",
        "  mp4 = open(videopath, 'rb').read()\n",
        "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
        "  html_code = f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
        "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'\n",
        "  return HTML(html_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbNaGpBVGf4N"
      },
      "source": [
        "### Imports Principais\n",
        "\n",
        "Import algoritmos, ambientes e outros módulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Gzf7VhkiHxQ"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import optuna\n",
        "\n",
        "from util_experiments import repeated_exec\n",
        "from util_plot import plot_result, plot_multiple_results\n",
        "\n",
        "from models_torch import test_policy\n",
        "#from crossentropy_method_v1 import run_crossentropy_method1, PolicyModelCrossentropy\n",
        "from crossentropy_method_v2 import run_crossentropy_method2, PolicyModelCrossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztvJdbKVh20Y"
      },
      "source": [
        "## 2. Modelagem de Recomepensas (Reward Shaping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Na **modelagem de recompensas**, nós alteramos o modelo de recompensa do ambiente original para dar recompensas mais *informativa*.\n",
        "\n",
        "Primeiro, criamos um wrapper do ambiente, em que damos recompensas adicionais a cada novo avanço conseguido pelo algoritmo na direção `x`.\n",
        "\n",
        "Veja os detalhes do estado do MountainCar em https://www.gymlibrary.ml/environments/classic_control/mountain_car/ .\n",
        "\n",
        "Ideia:\n",
        "- Calcular o deslocamento `delta` em relação ao `x` inicial (pode ser para a esquerda ou direita)\n",
        "- Toda vez que o agente ultrapassa o `delta` máximo obtido no episódio, ele recebe uma recompensa extra.\n",
        "- Se atingir o alvo, recebe uma recompensa alta\n",
        "- Atenção: pode estar violando a especificação de MDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FrequentRewardsMC(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        self.dist = 0\n",
        "        self.initial_pos = obs[0]\n",
        "        self.max_delta = 0\n",
        "        return obs\n",
        "    \n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        delta = abs(obs[0] - self.initial_pos) # quão distante está da posição inicial\n",
        "        if delta > self.max_delta:\n",
        "            reward = delta\n",
        "        if obs[0] >= 0.6:\n",
        "            reward = 100.0\n",
        "        return obs, reward, done, info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "8OACm0r-iuh2",
        "outputId": "a22a8644-6df5-4df4-ff2f-ded808c1c1bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Últimos episódios do treinamento: media = 21.816432578861715 , desvio padrao = 10.571414522501259\n"
          ]
        }
      ],
      "source": [
        "rmax = 50.0\n",
        "ENV = gym.make(\"MountainCar-v0\")\n",
        "ENV = FrequentRewardsMC(ENV)\n",
        "\n",
        "EPISODES   = 2000    # total de episódios\n",
        "BATCH_SIZE = 50      # quantidade de episódios executados por época de treinamento\n",
        "PROPORTION = 0.1     # percentual dos episódios (do batch) que serão selecionados\n",
        "\n",
        "policy_model = PolicyModelCrossentropy(ENV.observation_space.shape[0], [512], ENV.action_space.n, lr=0.005)\n",
        "returns, policy = run_crossentropy_method2(ENV, EPISODES, BATCH_SIZE, PROPORTION)\n",
        "clear_output()\n",
        "\n",
        "print(\"Últimos episódios do treinamento: media =\", np.mean(returns[-20:]), \", desvio padrao =\", np.std(returns[-20:]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exibe um gráfico episódios x retornos (não descontados)\n",
        "plot_result(returns, rmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-NXLvxYGf4P",
        "outputId": "318bd83e-4d2f-49a3-dd33-497c93b986a0"
      },
      "outputs": [],
      "source": [
        "# Faz alguns testes com o modelo de forma DETERMINÍSTICA e salva o vídeo em arquivo\n",
        "video = VideoRecorder(ENV, \"politica-treinada.mp4\")\n",
        "test_policy(ENV, policy, True, 5, render=False, videorec=video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "oVa15jmMIDrI",
        "outputId": "4802ab0f-e868-4dde-ddf4-d9ea866091ce"
      },
      "outputs": [],
      "source": [
        "render_mp4(\"politica-treinada.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8roKzCgsuCl"
      },
      "source": [
        "## 3. Aprendizagem por Currículo (Curriculum Learning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Na *aprendizagem por currículo* aplicada em RL:\n",
        "- o agente é treinado sequencialmente em *vários* ambientes (parecidos)\n",
        "- os ambientes têm níveis crescentes de dificuldade\n",
        "- ele deve aprender bem um deles antes de passar para o próximo\n",
        "\n",
        "Abaixo, criamos um wrapper que permite instanciar **versões simplificadas do MountainCar**.\n",
        "- a nova versão é parametrizada por um valor `goal_x`\n",
        "- basta o agente ultrapassar essa posição com velocidade positiva para terminar o episódio\n",
        "- a recompensa final é a velocidade (quanto mais rápido ele passar, melhor)\n",
        "- se `goal_x > 0.6`, ele funciona idêntico ao ambiente original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimplifiedMC(gym.Wrapper):\n",
        "    def __init__(self, env, goal_x):\n",
        "        super().__init__(env)\n",
        "        self.goal_x = goal_x\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        return obs\n",
        "    \n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        if obs[0] >= self.goal_x and obs[1] > 0:\n",
        "            reward = obs[1]\n",
        "            done = True\n",
        "        return obs, reward, done, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para treinar, vamos usar uma rede com DUAS camadas intermediárias: 128 e 256.\n",
        "\n",
        "E vamos treinar em diferentes instâncais do ambiente acima, com valores de `goal_x` crescentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rmax = 50.0\n",
        "ENV = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "BATCH_SIZE   = 50     # quantidade de episódios executados por época de treinamento\n",
        "PROPORTION = 0.1      # percentual dos episódios (do batch) que serão selecionados\n",
        "\n",
        "policy = PolicyModelCrossentropy(ENV.observation_space.shape[0], [128, 256], ENV.action_space.n, lr=0.01)\n",
        "\n",
        "all_returns = []\n",
        "\n",
        "curriculum_params = [(-0.30, 5000, -90.0), (-0.25, 5000, -90.0), (-0.20, 5000, -100.0), (-0.15, 5000, -100.0), (-0.10, 5000, -110.0), (-0.05, 5000, -120.0), (0.00, 5000, -120.0), (0.7, 5000, -80.0)]\n",
        "\n",
        "for goal_x, episodes, target_return in curriculum_params:\n",
        "    print(f\"TREINANDO COM goal_x = {goal_x}:\")\n",
        "    wrapped_env = SimplifiedMC(ENV, goal_x)\n",
        "    \n",
        "    returns, policy = run_crossentropy_method2(wrapped_env, episodes, BATCH_SIZE, PROPORTION, target_return=target_return, initial_policy=policy, render=False)\n",
        "    print(\"Últimos resultados: media =\", np.mean(returns[-20:]), \", desvio padrao =\", np.std(returns[-20:]))        \n",
        "    plot_result(returns, rmax)\n",
        "    all_returns.extend(returns)\n",
        "\n",
        "plot_result(all_returns, rmax)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Executa alguns episódios de forma NÃO-determinística e imprime um sumário\n",
        "test_policy(ENV, policy, False, 5, render=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "cap06-main.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('rlx')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "27dbc9ce4cc602e4f15257b7b0018d8dff5b9ce9a7d73bc4399cb5afb1e02c4a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
