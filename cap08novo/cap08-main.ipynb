{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeJ3wCaKe2Wl"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap08/cap08-main.ipynb)\n",
        "\n",
        "# Capítulo 8 - DQN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAHITU7VhsM7"
      },
      "source": [
        "## 1 - Configurações Iniciais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk1160jwoH9y"
      },
      "source": [
        "Instalação de pacotes e atribuição do caminho para o código do projeto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS23BU8R1vq-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from IPython.display import clear_output\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    #!pip install gym==0.23.1  # also works with 0.25.2\n",
        "    #!pip install gym[box2d]\n",
        "    !pip install opencv-python\n",
        "    !pip install gymnasium[all]\n",
        "    !pip install gym[atari,accept-rom-license]\n",
        "    !pip install tensorboard\n",
        "\n",
        "    # para salvar videos\n",
        "    !apt-get install -y ffmpeg xvfb x11-utils\n",
        "\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "\n",
        "    clear_output()\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cria um falso dispositivo de vídeo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It9SF9Iw2zXq"
      },
      "outputs": [],
      "source": [
        "'''if IN_COLAB:\n",
        "    import os\n",
        "    os.system(\"Xvfb :1 -screen 0 1400x900x24 &\")\n",
        "    os.environ['DISPLAY'] = ':1'\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Carrega a extensão para visualizar o `tensorboard` em um notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N903Nh7fRL0S"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importa pacotes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gzf7VhkiHxQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#from tensorboardX import SummaryWriter\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import tensorboard\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__2WQEWRk2a4"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "#from gym.wrappers.monitoring.video_recorder import VideoRecorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jXaW5bSkf-_"
      },
      "outputs": [],
      "source": [
        "from cap08novo import dqn_models\n",
        "from cap08novo.atari_wrappers import *\n",
        "\n",
        "from util.notebook import display_videos_from_path, display_videos_from_path_widgets, display_video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztvJdbKVh20Y"
      },
      "source": [
        "## 2 - DQN - Definições Auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adaptado do código explicado no livro **\"Deep Reinforcement Learning Hands On\"** (Maxim Lapan), Chapter 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_79PzMRmndhC"
      },
      "source": [
        "### Classes Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OACm0r-iuh2"
      },
      "outputs": [],
      "source": [
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class DQNExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, s1, a, r, done, s2):\n",
        "        experience = Experience(s1, a, r, done, s2)\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzwVdZIf3xi6"
      },
      "source": [
        "### Funções Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGMkPSqg3wNx"
      },
      "outputs": [],
      "source": [
        "# Faz uma escolha epsilon-greedy\n",
        "def choose_action(qnet, env, state, epsilon, device):\n",
        "    done_reward = None\n",
        "    if np.random.random() < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        state_a = np.array([state], copy=False)\n",
        "        state_v = torch.tensor(state_a, dtype=torch.float32).to(device)\n",
        "        q_vals_v = qnet(state_v)\n",
        "        _, act_v = torch.max(q_vals_v, dim=1)\n",
        "        action = int(act_v.item())\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PH-mJQ6ndhD"
      },
      "outputs": [],
      "source": [
        "# loss function, para treinamento da rede no DQN\n",
        "def calc_loss(batch, net, tgt_net, gamma, device=\"cpu\"):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "    states_v = torch.tensor(states, dtype=torch.float32).to(device)\n",
        "    next_states_v = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
        "    actions_v = torch.tensor(actions, dtype=torch.int64).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.tensor(dones, dtype=torch.bool).to(device)\n",
        "\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    next_state_values = tgt_net(next_states_v).max(dim=1)[0]\n",
        "    next_state_values[done_mask] = 0.0\n",
        "    next_state_values = next_state_values.detach()\n",
        "\n",
        "    target_state_action_values = rewards_v + gamma * next_state_values\n",
        "    return nn.MSELoss()(state_action_values, target_state_action_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFI9Q2h4yfhw"
      },
      "outputs": [],
      "source": [
        "# Função para testar a política, rodando alguns episódios. Pode, também, renderizar ou gravar um vídeo.\n",
        "def test_Qpolicy(env, Qpolicy, epsilon=0.0, num_episodes=5, render=False, videorec=None):\n",
        "    episodes_returns = []\n",
        "    total_steps = 0\n",
        "    #num_actions = env.action_space.n\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    for i in range(num_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        if render:\n",
        "            env.render()\n",
        "        if videorec is not None:\n",
        "            videorec.capture_frame()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        episodes_returns.append(0.0)\n",
        "        while not done:\n",
        "            action = choose_action(Qpolicy, env, obs, epsilon, device)\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            if render:\n",
        "                env.render()\n",
        "            if videorec is not None:\n",
        "                videorec.capture_frame()\n",
        "            total_steps += 1\n",
        "            episodes_returns[-1] += reward\n",
        "            steps += 1\n",
        "        print(f\"EPISODE {i+1}\")\n",
        "        print(\"- steps:\", steps)\n",
        "        print(\"- return:\", episodes_returns[-1])\n",
        "    mean_return = round(np.mean(episodes_returns), 1)\n",
        "    print(\"RESULTADO FINAL: média (por episódio):\", mean_return, end=\"\")\n",
        "    print(\", episódios:\", len(episodes_returns), end=\"\")\n",
        "    print(\", total de passos:\", total_steps)\n",
        "    if videorec is not None:\n",
        "        videorec.close()\n",
        "    return mean_return, episodes_returns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c241EVuundhE"
      },
      "source": [
        "## 3 - DQN - Função Principal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsqgk9RNndhE"
      },
      "source": [
        "Esta é a função que faz o aprendizado. (Porém, o DQN é uma solução maior, pensada para jogos de Atari, e que inclui também os wrappers, que serão usados na seção 6.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO3eR_sfndhE"
      },
      "outputs": [],
      "source": [
        "def DQN_TRAIN(env, env_name, gamma, qnet, qnet_lr, target_qnet, target_update_freq, replay_size, batch_size, epsilon_f, epsilon_decay_period, GOAL_REWARD):\n",
        "    # Passa as redes neurais para GPU, se tiver\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    qnet.to(device)\n",
        "    target_qnet.to(device)\n",
        "    print(qnet)\n",
        "\n",
        "    # Cria o otimizador, que vai fazer o ajuste dos pesos da 'qnet', \n",
        "    # Usa uma técnica de gradiente descendente de destaque, chamada ADAM\n",
        "    optimizer = optim.Adam(qnet.parameters(), lr=qnet_lr)\n",
        "\n",
        "    # Para o logging de dados, para serem exibidos no tensorboard\n",
        "    writer = SummaryWriter(comment=\"-\" + env_name)\n",
        "\n",
        "    buffer = DQNExperienceBuffer(replay_size)\n",
        "\n",
        "    start_time_str = datetime.now().strftime(\"%Y-%m-%d,%H-%M-%S\")\n",
        "    episode_reward_list = []\n",
        "    step = 0\n",
        "    epsilon = 1.0\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    episode_reward = 0.0\n",
        "    episode_start_step = 0\n",
        "    episode_start_time = time.time()\n",
        "\n",
        "    while True:\n",
        "        step += 1\n",
        "        \n",
        "        # Decaimento linear do epsilon\n",
        "        epsilon = max(epsilon_f, 1.0 - step / epsilon_decay_period)\n",
        "\n",
        "        action = choose_action(qnet, env, state, epsilon, device)\n",
        "\n",
        "        # Faz um passo / Aplica uma ação no ambiente\n",
        "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Adiciona no buffer\n",
        "        buffer.append(state, action, reward, terminated, new_state)\n",
        "        state = new_state\n",
        "\n",
        "        if done:\n",
        "            episode_reward_list.append(episode_reward)\n",
        "            speed = (step - episode_start_step) / (time.time() - episode_start_time)\n",
        "            \n",
        "            state, _ = env.reset()\n",
        "            episode_reward = 0.0\n",
        "            episode_start_step = step\n",
        "            episode_start_time = time.time()\n",
        "\n",
        "            # Abaixo, faz vários loggings de dados\n",
        "            mean_reward = np.mean(episode_reward_list[-100:])\n",
        "            print(f\"{step}: finished {len(episode_reward_list)} episodes, mean reward {mean_reward:.3f}, eps {epsilon:.2f}, speed {speed:.2f} steps/s\")\n",
        "            writer.add_scalar(\"epsilon\", epsilon, step)\n",
        "            writer.add_scalar(\"epi_reward_100\", mean_reward, step)\n",
        "            writer.add_scalar(\"epi_reward\", episode_reward, step)\n",
        "\n",
        "            # Testa se \"resolveu\" o ambiente\n",
        "            if mean_reward > GOAL_REWARD:\n",
        "                print(f\"Solved in {step} steps with mean reward {mean_reward:.3f}\")\n",
        "                filename = env_name + \"-\" + start_time_str + \".dat\"\n",
        "                torch.save(qnet.state_dict(), filename)\n",
        "                print(f\"Model saved as {filename}\")\n",
        "                break\n",
        "\n",
        "        if len(buffer) < replay_size:\n",
        "            continue\n",
        "\n",
        "        # Faz a 'tgt_net' receber os mesmos valores de pesos da 'qnet', na frequência indicada\n",
        "        if step % target_update_freq == 0:\n",
        "            target_qnet.load_state_dict(qnet.state_dict())\n",
        "\n",
        "        if step % 10000 == 0:\n",
        "            clear_output()\n",
        "\n",
        "        # Escolhendo amostras aleatórios do buffer e faz uma atualização dos pesos da rede\n",
        "        optimizer.zero_grad()\n",
        "        batch = buffer.sample(batch_size)\n",
        "        loss_t = calc_loss(batch, qnet, target_qnet, gamma, device=device)\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZq9U2FGndhF"
      },
      "source": [
        "## 4 - Treinando em um Ambientes Simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SigPdPRcndhF"
      },
      "outputs": [],
      "source": [
        "SIMPLE_ENV_NAME = \"MountainCar-v0\"\n",
        "GOAL_REWARD = -120\n",
        "\n",
        "#ENV_NAME = \"CartPole-v0\"\n",
        "#GOAL_REWARD = 200\n",
        "\n",
        "GAMMA = 0.999\n",
        "REPLAY_SIZE = 2_000\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "SYNC_TARGET_FRAMES = 250\n",
        "\n",
        "EPSILON_DECAY_PERIOD = 60_000\n",
        "EPSILON_FINAL = 0.02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0XKgIXSKdQV"
      },
      "outputs": [],
      "source": [
        "# Cria o ambiente\n",
        "env1 = gym.make(SIMPLE_ENV_NAME)\n",
        "\n",
        "# Cria as redes neurais\n",
        "qnet1 = dqn_models.MLP(env1.observation_space.shape[0], [128,256], env1.action_space.n)\n",
        "qtarget1 = dqn_models.MLP(env1.observation_space.shape[0], [128,256], env1.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY2pju8dymgV"
      },
      "outputs": [],
      "source": [
        "DQN_TRAIN(\n",
        "    env = env1,\n",
        "    env_name = SIMPLE_ENV_NAME,\n",
        "    gamma = GAMMA,\n",
        "    qnet = qnet1,\n",
        "    qnet_lr = LEARNING_RATE,\n",
        "    target_qnet = qtarget1,\n",
        "    target_update_freq = SYNC_TARGET_FRAMES,\n",
        "    replay_size = REPLAY_SIZE,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    epsilon_f = EPSILON_FINAL,\n",
        "    epsilon_decay_period = EPSILON_DECAY_PERIOD,\n",
        "    GOAL_REWARD = GOAL_REWARD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ2hasvindhG"
      },
      "outputs": [],
      "source": [
        "# Para carregar uma rede salva de arquivo\n",
        "#qnet1.load_state_dict(torch.load(\"/content/MountainCar-v0-XXXXXX.dat\", map_location=lambda storage,loc: storage))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFZ6eQeFIBA-"
      },
      "outputs": [],
      "source": [
        "# Roda alguns episódigos com o modelo e salva os vídeos em arquivos\n",
        "env1 = gym.make(SIMPLE_ENV_NAME)\n",
        "video_env=gym.wrappers.RecordVideo(env1, \"./dqn-simple\", episode_trigger=(lambda ep : True))\n",
        "test_Qpolicy(video_env, qnet1, 0.0, 3, render=False)\n",
        "video_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cijxLQTqt0Ja"
      },
      "outputs": [],
      "source": [
        "display_videos_from_path('./dqn-simple')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fttzaanEMzvF"
      },
      "source": [
        "## 5 - Visualização dos Resultados dos Treinamentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_iUaT7nM4ZV"
      },
      "source": [
        "Estamos, aqui, usando o **Tensorboard** para acompanhar os dados do treinamento em tempo real. Este módulo foi criado para o Tensorflow, mas é compatível com Pytorch.\n",
        "\n",
        "Basta rodar uma vez e acompanhar. Ele pode também ser executado antes da seção 4, para acompanhar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk0UohtUjSnb"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    %tensorboard --logdir runs\n",
        "else:\n",
        "    %tensorboard --logdir cap08novo/runs\n",
        "    # Mostra o Tensorboard com a altura indicada\n",
        "    #tensorboard.notebook.display(height=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP1Xyu4wndhH"
      },
      "source": [
        "## 6 - Treinando no Jogo Pong (Atari)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvEJ8CNvndhH"
      },
      "outputs": [],
      "source": [
        "# Veja outros em: https://www.gymlibrary.dev/environments/atari/complete_list/\n",
        "# Se mudar o jogo, lembre-se de alterar também o GOAL_REWARD abaixo!\n",
        "ATARI_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "\n",
        "# Recompensa alvo; no Pong, esta é a diferença de pontos do player para a \"cpu\", sendo +21.0 o máximo e -21.0 o mínimo\n",
        "# Tente com algum valor negativo (e.g. -15.0) para um treinamento mais rápido, ou algum valor positivo (+15.0) para ver o agent ganhar da \"cpu\"\n",
        "GOAL_REWARD = 0.0\n",
        "\n",
        "# Parâmetros do DQN\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 20_000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1_000\n",
        "\n",
        "EPSILON_DECAY_PERIOD = 100_000\n",
        "EPSILON_FINAL = 0.02\n",
        "\n",
        "env2 = gym.make(ATARI_ENV_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxNw6F_hywXV"
      },
      "outputs": [],
      "source": [
        "# Aplica os wrappers do DQN para ambientes Atari\n",
        "env2a = MaxAndSkipEnv(env2)\n",
        "env2b = FireResetEnv(env2a)\n",
        "env2c = ProcessFrame84(env2b)\n",
        "env2d = ImageToPyTorch(env2c)\n",
        "env2e = BufferWrapper(env2d, 4)\n",
        "env2f = ScaledFloatFrame(env2e)\n",
        "\n",
        "# Cria as redes neurais\n",
        "qnet2 = dqn_models.DQNNet(env2f.observation_space.shape, env2f.action_space.n)\n",
        "qtarget2 = dqn_models.DQNNet(env2f.observation_space.shape, env2f.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CGUxgf_iiOU"
      },
      "outputs": [],
      "source": [
        "# Mostrando a tela do ambiente original, e outras telas após passar por alguns wrappers\n",
        "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12,5))\n",
        "\n",
        "s = env2.reset()\n",
        "ax1.imshow(s)\n",
        "ax1.set_title('Tela antes de iniciar')\n",
        "\n",
        "s = env2b.reset()\n",
        "ax2.imshow(s)\n",
        "ax2.set_title('Tela da partida iniciada automaticamente')\n",
        "\n",
        "s = env2d.reset()\n",
        "ax3.imshow(s[0], cmap='gray', vmin=0, vmax=255)  # exibe em escala de cinza\n",
        "ax3.set_title('Imagem processada');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umDKxHFVndhK"
      },
      "outputs": [],
      "source": [
        "DQN_TRAIN(\n",
        "    env = env2f,\n",
        "    env_name = ATARI_ENV_NAME,\n",
        "    gamma = GAMMA,\n",
        "    qnet = qnet2,\n",
        "    qnet_lr = LEARNING_RATE,\n",
        "    target_qnet = qtarget2,\n",
        "    target_update_freq = SYNC_TARGET_FRAMES,\n",
        "    replay_size = REPLAY_SIZE,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    epsilon_f = EPSILON_FINAL,\n",
        "    epsilon_decay_period = EPSILON_DECAY_PERIOD,\n",
        "    GOAL_REWARD = GOAL_REWARD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtPoS4t5b3d5"
      },
      "outputs": [],
      "source": [
        "# Para carregar uma rede salva de arquivo\n",
        "# Permite continuar um treinamento, ou permite carregar para salvar o vídeo\n",
        "#filename = \"/content/PongNoFrameskip-v4-agente-treinado.net\"\n",
        "#dqn_net2.load_state_dict(torch.load(filename, map_location=lambda storage,loc: storage))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzVZv90bvG1x"
      },
      "outputs": [],
      "source": [
        "# Roda alguns episódigos com o modelo e salva os vídeos em arquivos\n",
        "# Atenção: precisa rodar com os wrappers aplicados!\n",
        "env2 = gym.make(ATARI_ENV_NAME)\n",
        "env2a = MaxAndSkipEnv(env2)\n",
        "env2b = FireResetEnv(env2a)\n",
        "env2c = ProcessFrame84(env2b)\n",
        "env2d = ImageToPyTorch(env2c)\n",
        "env2e = BufferWrapper(env2d, 4)\n",
        "env2f = ScaledFloatFrame(env2e)\n",
        "\n",
        "video_env=gym.wrappers.RecordVideo(env2f, \"./dqn-atari\", episode_trigger=(lambda ep : True))\n",
        "test_Qpolicy(video_env, qnet2, 0.0, 2, render=False)\n",
        "video_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lYCL7Plv3RY"
      },
      "outputs": [],
      "source": [
        "#display_videos_from_path('./dqn-atari')\n",
        "display_videos_from_path_widgets('./dqn-atari')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cap08-main.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('rlx')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "27dbc9ce4cc602e4f15257b7b0018d8dff5b9ce9a7d73bc4399cb5afb1e02c4a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
