{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7HLPQPeV94m"
      },
      "source": [
        "Você pode rodar este notebook localmente ou no Colab. Para abrir diretamente no Colab, basta clicar no botão abaixo.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap02/cap02-main.ipynb) \n",
        "\n",
        "# Capítulo 2 - Bandidos Multibraços (Multi-Armed Bandits)\n",
        "\n",
        "Neste notebook, apresentamos resultados de testes (mais simples e mais elaborados) envolvendo os principais algoritmos para este problema. \n",
        "\n",
        "Para melhor entender os algoritmos, é recomendado acessar os scripts da pasta \"cap02\", onde os códigos foram definidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYKcHICW2HxW"
      },
      "source": [
        "## Configurações Iniciais\n",
        "\n",
        "Para instalar e importar pacotes e configurar algumas coisas..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twlMXZDa0DFY"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    # clone repository, to use \"util\" module \n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "    clear_output()\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibr-xyFVvZ44"
      },
      "outputs": [],
      "source": [
        "from util.bandit_envs import MultiArmedBanditEnv, GaussianMultiArmedBanditEnv\n",
        "from util.experiments import repeated_exec\n",
        "from util.plot import plot_multiple_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2smPhi-_r8JY"
      },
      "source": [
        "## 1 - Testando os ambientes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr5d0d8OZB7S"
      },
      "source": [
        "Existem dois ambientes, no modulo **`util.bandit_envs`**. Ambos recebem uma ação representada como um número inteiro a partir de 0 e retorna uma recompensa numérica `float`.\n",
        "- **MultiArmedBanditEnv**: Retorna recompensas 0 ou 1, de acordo com a ação. Cada ação tem uma probabilidade, passada no construtor (e desconhecida dos algoritmos).\n",
        "- **GaussianMultiArmedBanditEnv**: retorna recompensas com distribuição normal (gaussiana), cujos valores são contínuos e ocorrem com maior probabilidade perto da média (mas podem ir de `-inf` a `+inf`). Cada ação tem uma média passada no construtor (e desconhecida dos algoritmos).\n",
        "\n",
        "Em cada ambiente, vamos testar aplicar a sequência de ações abaixo, que apenas alterna ação **0** e ação **1**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv0h0fXwY0lQ"
      },
      "outputs": [],
      "source": [
        "STEPS = 12\n",
        "actions = [ 0, 1 ] * (STEPS//2)\n",
        "actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNgMWddHr7ih"
      },
      "outputs": [],
      "source": [
        "env1 = MultiArmedBanditEnv()\n",
        "\n",
        "env1.reset()\n",
        "for i in range(STEPS):\n",
        "    a = actions[i]\n",
        "    r = env1.step(a)\n",
        "    print(\" - ação\", a, \", recompensa\", r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okw6fusatC3A"
      },
      "outputs": [],
      "source": [
        "env2 = GaussianMultiArmedBanditEnv()\n",
        "\n",
        "env2.reset()\n",
        "for i in range(STEPS):\n",
        "    a = actions[i]\n",
        "    r = env2.step(a)\n",
        "    print(\" - ação\", a, \", recompensa\", r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "659YAN1F2VMu"
      },
      "source": [
        "## 2 - Soluções Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm9E_ydgrrY5"
      },
      "outputs": [],
      "source": [
        "from cap02.baseline_algorithms import run_greedy, run_random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7RVzsGP4UEE"
      },
      "source": [
        "Abaixo, vamos testar duas soluções simples usadas como base para comparação (ou seja, soluções de *baseline*):\n",
        "- **greedy**: depois de fazer 1 vez cada ação, escolhe a melhor e repete para sempre\n",
        "- **random**: escolher qualquer ação aleatoriamente\n",
        "\n",
        "Vamos apresentar testes no ambiente abaixo, com três ações de probabilidades 20%, 50% e 75% de retornar recompensa 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0hGjaFFbex7"
      },
      "outputs": [],
      "source": [
        "BANDIT_PROBABILITIES = [0.2, 0.5, 0.75]\n",
        "env = MultiArmedBanditEnv(BANDIT_PROBABILITIES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I28ockB2biKp"
      },
      "source": [
        "***Qual dos dois algoritmos você acha que vai se sair melhor?***\n",
        "\n",
        "Abaixo, testamos os dois algoritmos rodando por 10 mil passos. Execute mais de uma vez a célula e veja como o resultado varia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QM99FoU6eXU"
      },
      "outputs": [],
      "source": [
        "rewards, _ = run_greedy(env, total_steps=10000)\n",
        "print(\"Greedy - soma de recompensas:\", sum(rewards))\n",
        "\n",
        "rewards, _ = run_random(env, total_steps=10000)\n",
        "print(\"Random - soma de recompensas:\", sum(rewards))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfmyzkJocH3w"
      },
      "source": [
        "Depois de fazer alguns testes, você deve perceber que um deles varia muito nos resultados. A seguir, vamos fazer experimentos executando ambos várias vezes e plotamos a recompensa média recebida até cada passo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3w3jZWb3cgYw"
      },
      "outputs": [],
      "source": [
        "RUNS  = 50\n",
        "STEPS = 10000\n",
        "\n",
        "result_random = repeated_exec(RUNS, \"RANDOM\", run_random, env, STEPS)\n",
        "result_greedy = repeated_exec(RUNS, \"GREEDY\", run_greedy, env, STEPS)\n",
        "result_random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pfF2_xK7ml"
      },
      "outputs": [],
      "source": [
        "for (alg_name, rewards) in [result_greedy, result_random]:\n",
        "    print()\n",
        "    print(\"Summary for \" + alg_name)\n",
        "    print(\" - total reward:\", rewards.sum())\n",
        "    print(\" - avg reward (win rate):\", rewards.sum() / STEPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRiqWgrqc6Tp"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results([result_greedy, result_random], cumulative=True, x_log_scale=True, yreference=env.get_max_mean_reward())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70W0dx6CLavY"
      },
      "source": [
        "O resultado acima deve mostrar resultados próximos, no longo prazo. Na verdade, se for executado por tempo suficiente, eles tendem a ficar iguais. Porém, um deles apresenta maior variação. \n",
        "\n",
        "Nos gráficos abaixo, mostramos o gráfico individualizado para cada algoritmo, junto com uma área sombreada que mostra 1 desvio padrão da média para cima e para baixo. Assim, a área sombreada dá uma ideia de quanto variam os resultados entre as várias execuções (indicada por `RUNS`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DeA7yHufpnA"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results([result_greedy], cumulative=True, x_log_scale=True, yreference=env.get_max_mean_reward(), plot_stddev=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWeKd2BCLYUj"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results([result_random], cumulative=True, x_log_scale=True, yreference=env.get_max_mean_reward(), plot_stddev=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5YgHc745SKc"
      },
      "source": [
        "## 3 - Epsilon-Greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81eb3_ASgbuH"
      },
      "outputs": [],
      "source": [
        "from cap02.epsilon_greedy import run_epsilon_greedy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z_OK4bVi3Dz"
      },
      "source": [
        "Nesta solução, um parâmetro **`epsilon`** controla o grau de exploração (*exploration*) do algoritmo, que escolhe ações assim:\n",
        "- com probabilidade *epsilon*: faz uma ação qualquer\n",
        "- com probabilidade *(1.0-epsilon)*: faz a ação de melhor média (até o momento)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21zfubHrgnAo"
      },
      "source": [
        "Vamos fazer alguns experimentos, com diferentes valores de *epsilon*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBbk9l1XhF5w"
      },
      "outputs": [],
      "source": [
        "BANDIT_PROBABILITIES = [0.2, 0.5, 0.75]\n",
        "env = MultiArmedBanditEnv(BANDIT_PROBABILITIES)\n",
        "\n",
        "rewards, _ = run_epsilon_greedy(env, total_steps=10000, epsilon=0.2)\n",
        "print(f\"Eps-greedy (0.2) - soma de recompensas:\", sum(rewards))\n",
        "\n",
        "rewards, _ = run_epsilon_greedy(env, total_steps=10000, epsilon=0.1)\n",
        "print(f\"Eps-greedy (0.1) - soma de recompensas:\", sum(rewards))\n",
        "\n",
        "rewards, _ = run_epsilon_greedy(env, total_steps=10000, epsilon=0.01)\n",
        "print(f\"Eps-greedy (0.01) - soma de recompensas:\", sum(rewards))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EPnnpOchQAg"
      },
      "source": [
        "A seguir, vamos comparações entre duas versões do *epsilon-greedy* e o *random*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VVRaDTH5T67"
      },
      "outputs": [],
      "source": [
        "RUNS  = 50\n",
        "STEPS = 10000\n",
        "\n",
        "results = []\n",
        "results.append( repeated_exec(RUNS, \"RANDOM\", run_random, env, STEPS) )\n",
        "\n",
        "for epsilon in [0.02, 0.10, 0.30]:\n",
        "    results.append( repeated_exec(RUNS, f\"EPS({epsilon})-GREEDY\", run_epsilon_greedy, env, STEPS, epsilon) )\n",
        "\n",
        "for (alg_name, rewards) in results:\n",
        "    print()\n",
        "    print(\"Summary for \" + alg_name)\n",
        "    print(\" - total reward:\", rewards.sum())\n",
        "    print(\" - avg reward (win rate):\", rewards.sum() / STEPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgRPwH5XQOWS"
      },
      "outputs": [],
      "source": [
        "results[1]#[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d7h-OO5jw-e"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results, cumulative=True, x_log_scale=True, yreference=env.get_max_mean_reward())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZIBTMdUWKpW"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results, cumulative=True, x_log_scale=True, yreference=env.get_max_mean_reward(), plot_stddev=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QelmO29Ij5_v"
      },
      "source": [
        "Uma deficiência do **epsilon-greedy** padrão é que ele *sempre* vai explorar, e sempre na mesma proporção (dada pelo *epsilon*).\n",
        "\n",
        "Uma solução melhor consiste em fazer o epsilon diminuir com o tempo. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar8290S6sWgd"
      },
      "source": [
        "## 4 - Decaying Epsilon-Greedy\n",
        "\n",
        "Nesta variante, o epsilon decai ao longo do tempo. Não daremos a implementação pronta do **decaying epsilon-greedy**. Mas, aqui, discutimos como fazer o decaimento do valor do epsilon.\n",
        "\n",
        "Uma ideia para o decaimento é fazer o valor de epsilon se comportar assim:\n",
        "- inicia com um valor inicial *`initial_epsilon`*, que pode ser sempre 1.0 (100% de exploração)\n",
        "- cai até um valor mínimo dado pelo parâmetro *`minimum_epsilon`*\n",
        "- esse valor mínimo é atingido no passo dado pelo parâmetro *`target_step`*\n",
        "\n",
        "Esse decaimento pode ser feito de forma *linear* ou *exponencial* (mas existem outros). Abaixo, mostramos como o valor do epsilon varia nos dois esquemas citados para `minimun_epsilon=0.01` e `target_step=7000`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWmAZ8MiqtpV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from cap02.decaying_schemes import exponential_decay, linear_decay\n",
        "\n",
        "mininimu_epsilon = 0.01\n",
        "target_step = 7000\n",
        "step_sequence = range(1, 10000 + 1)\n",
        "\n",
        "for label, decay_scheme in [(\"exponential\", exponential_decay), (\"linear\", linear_decay)]:\n",
        "    get_epsilon_fn = decay_scheme(mininimu_epsilon, target_step)\n",
        "    plt.plot(step_sequence, [get_epsilon_fn(t) for t in step_sequence], label=label)\n",
        "\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1BskBxGuDP1"
      },
      "source": [
        "\n",
        "Note que tanto `exponential_decay()` como `linear_decay()` retornam funções. A função retornada recebe o passo atual e retorna o valor correspondente desejado (para *epsilon*). Veja o arquivo `decaying_schemes.py` para entender melhor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndm1mcbRs8RN"
      },
      "source": [
        "Agora é com você: implemente o **decaying epsilon-greedy** simplesmente integrando alguma dessas formas de decaimento (ou ambas) ao código do epsilon-greedy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtRUb9kztFi_"
      },
      "outputs": [],
      "source": [
        "# faça aqui\n",
        "def run_decaying_epsilon_greedy():\n",
        "  pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_F2AShc4K_K"
      },
      "source": [
        "## 5 - UCB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST_5BJavuYBA"
      },
      "outputs": [],
      "source": [
        "from cap02.ucb import run_ucb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swPxD3Cth1PM"
      },
      "source": [
        "Explicar..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9NYqAmPztCH"
      },
      "outputs": [],
      "source": [
        "BANDIT_PROBABILITIES = [0.2, 0.5, 0.75]\n",
        "env = MultiArmedBanditEnv(BANDIT_PROBABILITIES)\n",
        "\n",
        "rewards, _ = run_ucb(env, 10000, c=2.0)\n",
        "print(f\"UCB(c=2.0) - soma de recompensas:\", sum(rewards))\n",
        "\n",
        "rewards, _ = run_ucb(env, 10000, c=1.0)\n",
        "print(f\"UCB(c=1.0) - soma de recompensas:\", sum(rewards))\n",
        "\n",
        "rewards, _ = run_ucb(env, 10000, c=0.5)\n",
        "print(f\"UCB(c=0.5) - soma de recompensas:\", sum(rewards))\n",
        "\n",
        "rewards, _ = run_random(env, total_steps=10000)\n",
        "print(\"Random - soma de recompensas:\", sum(rewards))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R-mAkwkv4bX"
      },
      "outputs": [],
      "source": [
        "BANDIT_PROBABILITIES = [0.2, 0.5, 0.75]\n",
        "env = MultiArmedBanditEnv(BANDIT_PROBABILITIES)\n",
        "\n",
        "results = []\n",
        "results.append( repeated_exec(RUNS, \"RANDOM\", run_random, env, STEPS) )\n",
        "\n",
        "for param_c in [0.5, 1.0, 2.0]:\n",
        "  results.append( repeated_exec(RUNS, f\"UCB(c={param_c})\", run_ucb, env, STEPS, param_c) )\n",
        "\n",
        "plot_multiple_results(results, cumulative=True, x_log_scale=True, yreference=env.get_max_mean_reward())\n",
        "#plot_multiple_results(results, cumulative=True, x_log_scale=True, yreference=env.get_max_mean_reward(), plot_stddev=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6lhGvVCQCgd"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results[1:2], cumulative=True, x_log_scale=True, yreference=env.get_max_mean_reward(), plot_stddev=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPRX6WhuycR-"
      },
      "source": [
        "Falar do parâmetro c ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_BCBC_bvqX3"
      },
      "source": [
        "## 4 - Comparação Geral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HekmNjUx5V1"
      },
      "source": [
        "Vamos fazer experimentos comparando:\n",
        "- **random**: serve de comparação, como solução \"ruim\"\n",
        "- **epsilon-greedy** com o melhor valor de epsilon encontramos antes\n",
        "- **ucb** com o melhor valor de c que encontramos\n",
        "\n",
        "Vamos rodar (reiniciando) cada algoritmo 70 vezes. Cada execução terá 14 mil passos. Vamos fazer experimentos separados para as duas versões de ambiente que apresentamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cNED7EcxusV"
      },
      "outputs": [],
      "source": [
        "# parâmetros dos algoritmos\n",
        "epsilon = 0.02\n",
        "param_c = 0.5\n",
        "\n",
        "# parâmetros que controlam a repetição e duração dos experimentos\n",
        "RUNS  = 70\n",
        "STEPS = 140000\n",
        "\n",
        "# esta variável é para carregar automaticamente resultados de simulações já executadas\n",
        "auto_load = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eTe3xicwuQe"
      },
      "source": [
        "### 4.1 - Com Multi-Armed Bandit de recompensas binárias"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, usamos o ambiente padrão, com probabilidades bem próximas, para tentar \"confundir\" os algoritmos no início de cada execução."
      ],
      "metadata": {
        "id": "g0lejn0yc3QK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyDu-ZKRxhEd"
      },
      "outputs": [],
      "source": [
        "BANDIT_PROBABILITIES = [0.4, 0.5, 0.55]\n",
        "env = MultiArmedBanditEnv(BANDIT_PROBABILITIES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhglpsL4uUuA"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "results.append( repeated_exec(RUNS, \"RANDOM\", run_random, env, STEPS, auto_load=auto_load) )\n",
        "results.append( repeated_exec(RUNS, f\"EPS({epsilon})-GREEDY\", run_epsilon_greedy, env, STEPS, epsilon, auto_load=auto_load) )\n",
        "results.append( repeated_exec(RUNS, f\"UCB(c={param_c})\", run_ucb, env, STEPS, param_c, auto_load=auto_load) )\n",
        "\n",
        "plot_multiple_results(results, cumulative=True, x_log_scale=True, yreference=env.get_max_mean_reward())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_results(results, cumulative=True, x_log_scale=True, yreference=env.get_max_mean_reward(), plot_stddev=True)"
      ],
      "metadata": {
        "id": "uQ6HjNoyeBsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tc5dpYswzV-"
      },
      "source": [
        "### 4.2 - Com Multi-Armed Bandit de recompensas normais (gaussianas)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, usamos o ambiente com recompensas gaussianas (distribuição normal), com médias bem próximas, para tentar \"confundir\" os algoritmos no início. "
      ],
      "metadata": {
        "id": "rbTIm0n1dxl5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrUE2WOTxbgG"
      },
      "outputs": [],
      "source": [
        "BANDIT_MEANS = [0.4, 0.5, 0.55]\n",
        "env_gauss = GaussianMultiArmedBanditEnv(BANDIT_MEANS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EThDhfbPwzus"
      },
      "outputs": [],
      "source": [
        "results_gauss = []\n",
        "\n",
        "results_gauss.append( repeated_exec(RUNS, \"RANDOM\", run_random, env_gauss, STEPS, auto_load=auto_load) )\n",
        "results_gauss.append( repeated_exec(RUNS, f\"EPS({epsilon})-GREEDY\", run_epsilon_greedy, env_gauss, STEPS, epsilon, auto_load=auto_load) )\n",
        "results_gauss.append( repeated_exec(RUNS, f\"UCB(c={param_c})\", run_ucb, env_gauss, STEPS, param_c, auto_load=auto_load) )\n",
        "\n",
        "plot_multiple_results(results_gauss, cumulative=True, x_log_scale=True, yreference=env.get_max_mean_reward())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQFRTOVgxpXv"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results_gauss, cumulative=True, x_log_scale=True, yreference=env.get_max_mean_reward(), plot_stddev=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}