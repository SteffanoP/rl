{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F7HLPQPeV94m"
      },
      "source": [
        "Você pode rodar este notebook localmente ou no Colab. Para abrir diretamente no Colab, basta clicar no botão abaixo.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap01/cap01-main.ipynb)\n",
        "\n",
        "# Capítulo 1 - Bandidos Multibraços (Multi-Armed Bandits)\n",
        "\n",
        "Neste notebook, apresentamos resultados de testes (mais simples e mais elaborados) envolvendo os principais algoritmos para este problema.\n",
        "\n",
        "Para melhor entender os algoritmos, é recomendado acessar os scripts da pasta `cap01` onde os códigos foram definidos."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MYKcHICW2HxW"
      },
      "source": [
        "## Configurações Iniciais\n",
        "\n",
        "Para instalar e importar pacotes e configurar algumas coisas..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "twlMXZDa0DFY"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    !pip install gymnasium\n",
        "    # clone repository, to use \"util\" module\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "    clear_output()\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ibr-xyFVvZ44"
      },
      "outputs": [],
      "source": [
        "from envs.bandits import MultiArmedBanditEnv, GaussianMultiArmedBanditEnv\n",
        "from util.experiments import repeated_exec\n",
        "from util.plot import plot_multiple_results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2smPhi-_r8JY"
      },
      "source": [
        "## 1 - Testando os ambientes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vr5d0d8OZB7S"
      },
      "source": [
        "Existem dois ambientes, no modulo **`envs.bandits`**. Ambos recebem uma ação representada como um número inteiro a partir de 0 e retorna uma recompensa numérica `float`.\n",
        "- **MultiArmedBanditEnv**: \n",
        "  - retorna recompensas 0 ou 1, de acordo com a ação\n",
        "  - cada ação tem uma probabilidade distinta passada no construtor (e desconhecida dos algoritmos).\n",
        "- **GaussianMultiArmedBanditEnv**: \n",
        "    - retorna recompensas com distribuição normal (gaussiana), cujos valores são contínuos e ocorrem com maior probabilidade perto da média\n",
        "    - cada ação tem uma média distinta passada no construtor (e desconhecida dos algoritmos).\n",
        "\n",
        "Em cada ambiente, vamos testar aplicar a sequência de ações abaixo, que apenas alterna ciclicamente as ações **0**, **1** e **2**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv0h0fXwY0lQ"
      },
      "outputs": [],
      "source": [
        "STEPS = 15\n",
        "action_list = [ 0, 1, 2 ] * (STEPS//3)\n",
        "action_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNgMWddHr7ih"
      },
      "outputs": [],
      "source": [
        "env1 = MultiArmedBanditEnv([0.01, 0.80, 0.20])\n",
        "\n",
        "env1.reset()\n",
        "for i in range(STEPS):\n",
        "    a = action_list[i]\n",
        "    r = env1.step(a)\n",
        "    print(\" - ação\", a, \", recompensa\", r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okw6fusatC3A"
      },
      "outputs": [],
      "source": [
        "env2 = GaussianMultiArmedBanditEnv([0.01, 5.0, 1.0])\n",
        "\n",
        "env2.reset()\n",
        "for i in range(STEPS):\n",
        "    a = action_list[i]\n",
        "    r = env2.step(a)\n",
        "    print(\" - ação\", a, \", recompensa\", r)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "659YAN1F2VMu"
      },
      "source": [
        "## 2 - Soluções Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Xm9E_ydgrrY5"
      },
      "outputs": [],
      "source": [
        "from cap01.baseline_algorithms import run_greedy, run_random"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-7RVzsGP4UEE"
      },
      "source": [
        "Abaixo, vamos testar duas soluções simples usadas como base para comparação (ou seja, soluções de *baseline*):\n",
        "- **greedy**: depois de fazer 1 vez cada ação, escolhe a melhor e repete para sempre\n",
        "- **random**: escolher qualquer ação aleatoriamente\n",
        "\n",
        "Vamos apresentar testes no ambiente abaixo, com três ações de probabilidades 20%, 50% e 75% de retornar recompensa.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "k0hGjaFFbex7"
      },
      "outputs": [],
      "source": [
        "               # Ações:   0    1    2\n",
        "BANDITS_PROBABILITIES = [0.2, 0.5, 0.75]\n",
        "env = MultiArmedBanditEnv(BANDITS_PROBABILITIES)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "I28ockB2biKp"
      },
      "source": [
        "***Qual dos dois algoritmos você acha que vai se sair melhor?***\n",
        "\n",
        "Abaixo, testamos os dois algoritmos rodando por 10 mil passos. Execute mais de uma vez a célula e veja como o resultado varia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QM99FoU6eXU"
      },
      "outputs": [],
      "source": [
        "rewards, _ = run_greedy(env, total_steps=10000)\n",
        "print(\"Greedy - soma de recompensas:\", sum(rewards))\n",
        "\n",
        "rewards, _ = run_random(env, total_steps=10000)\n",
        "print(\"Random - soma de recompensas:\", sum(rewards))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TfmyzkJocH3w"
      },
      "source": [
        "Depois de fazer alguns testes, você deve perceber que um deles varia muito nos resultados. A seguir, vamos fazer experimentos executando ambos várias vezes e plotamos a recompensa média recebida até cada passo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3w3jZWb3cgYw"
      },
      "outputs": [],
      "source": [
        "RUNS  = 50\n",
        "STEPS = 10000\n",
        "\n",
        "result_random = repeated_exec(RUNS, \"RANDOM\", run_random, env, STEPS)\n",
        "result_greedy = repeated_exec(RUNS, \"GREEDY\", run_greedy, env, STEPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pfF2_xK7ml"
      },
      "outputs": [],
      "source": [
        "for (alg_name, rewards) in [result_greedy, result_random]:\n",
        "    print()\n",
        "    print(\"Summary for \" + alg_name)\n",
        "    print(\" - total reward:\", rewards.sum())\n",
        "    print(\" - avg reward (win rate):\", rewards.sum() / STEPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRiqWgrqc6Tp"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results([result_greedy, result_random], cumulative='avg', x_log_scale=True, yreference=env.get_max_mean_reward())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "70W0dx6CLavY"
      },
      "source": [
        "O resultado acima deve mostrar resultados próximos, no longo prazo. Porém, um deles:\n",
        "- tende a ser levemente melhor,\n",
        "- porém apresenta maior variação.\n",
        "\n",
        "Abaixo, mostramos os gráficos individualizados de cada algoritmo, junto com uma área sombreada que mostra 1 desvio padrão da média para cima e para baixo. Assim, a área sombreada dá uma ideia de quanto variam os resultados entre as várias execuções."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DeA7yHufpnA"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results([result_greedy], cumulative='avg', x_log_scale=True, yreference=env.get_max_mean_reward(), plot_stddev=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWeKd2BCLYUj"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results([result_random], cumulative='avg', x_log_scale=True, yreference=env.get_max_mean_reward(), plot_stddev=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A5YgHc745SKc"
      },
      "source": [
        "## 3 - Epsilon-Greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "81eb3_ASgbuH"
      },
      "outputs": [],
      "source": [
        "from cap01.epsilon_greedy import run_epsilon_greedy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z_OK4bVi3Dz"
      },
      "source": [
        "Nesta solução, um parâmetro **`epsilon`** controla o grau de exploração (*exploration*) do algoritmo, que escolhe ações assim:\n",
        "- com probabilidade *epsilon*: faz uma ação qualquer\n",
        "- com probabilidade *(1.0-epsilon)*: faz a ação de melhor média (até o momento)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "21zfubHrgnAo"
      },
      "source": [
        "Vamos fazer alguns experimentos, com diferentes valores de *epsilon*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBbk9l1XhF5w"
      },
      "outputs": [],
      "source": [
        "BANDIT_PROBABILITIES = [0.2, 0.5, 0.75]\n",
        "env = MultiArmedBanditEnv(BANDIT_PROBABILITIES)\n",
        "\n",
        "rewards, _ = run_epsilon_greedy(env, total_steps=10000, epsilon=0.2)\n",
        "print(f\"Eps-greedy (0.2) - soma de recompensas:\", sum(rewards))\n",
        "\n",
        "rewards, _ = run_epsilon_greedy(env, total_steps=10000, epsilon=0.1)\n",
        "print(f\"Eps-greedy (0.1) - soma de recompensas:\", sum(rewards))\n",
        "\n",
        "rewards, _ = run_epsilon_greedy(env, total_steps=10000, epsilon=0.01)\n",
        "print(f\"Eps-greedy (0.01) - soma de recompensas:\", sum(rewards))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3EPnnpOchQAg"
      },
      "source": [
        "A seguir, vamos comparações entre duas versões do *epsilon-greedy* e o *random*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VVRaDTH5T67"
      },
      "outputs": [],
      "source": [
        "RUNS  = 50\n",
        "STEPS = 10000\n",
        "\n",
        "results = []\n",
        "results.append( repeated_exec(RUNS, \"RANDOM\", run_random, env, STEPS) )\n",
        "\n",
        "for epsilon in [0.02, 0.10, 0.30]:\n",
        "    results.append( repeated_exec(RUNS, f\"EPS({epsilon})-GREEDY\", run_epsilon_greedy, env, STEPS, epsilon) )\n",
        "\n",
        "for (alg_name, rewards) in results:\n",
        "    print()\n",
        "    print(\"Summary for \" + alg_name)\n",
        "    print(\" - total reward:\", rewards.sum())\n",
        "    print(\" - avg reward (win rate):\", rewards.sum() / STEPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d7h-OO5jw-e"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results, cumulative='avg', x_log_scale=True, yreference=env.get_max_mean_reward())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZIBTMdUWKpW"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results, cumulative='avg', x_log_scale=True, yreference=env.get_max_mean_reward(), plot_stddev=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QelmO29Ij5_v"
      },
      "source": [
        "Uma deficiência do **epsilon-greedy** padrão é que ele *sempre* vai explorar, e sempre na mesma proporção (dada pelo *epsilon*).\n",
        "\n",
        "Uma solução melhor consiste em fazer o epsilon diminuir com o tempo.\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ar8290S6sWgd"
      },
      "source": [
        "## 4 - Decaying Epsilon-Greedy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "auyhx7S9l--M"
      },
      "source": [
        "Nesta variante do *epsilon-greedy*, o epsilon decai ao longo do tempo.\n",
        "\n",
        "Não daremos a implementação pronta do **decaying epsilon-greedy**. Mas, aqui, discutimos como fazer o decaimento do valor do epsilon.\n",
        "\n",
        "Uma ideia para o decaimento é fazer o valor de epsilon se comportar assim:\n",
        "- inicia com um valor inicial *`initial_epsilon`*, que pode ser sempre `1.0` (ou seja, inicia com 100% de exploração)\n",
        "- cai até um valor mínimo dado pelo parâmetro *`minimum_epsilon`*\n",
        "- esse valor mínimo é atingido no passo dado pelo parâmetro *`target_step`*\n",
        "\n",
        "Esse decaimento pode ser feito de forma *linear* ou *exponencial* (mas existem outros). Abaixo, mostramos como o valor do epsilon varia nos dois esquemas citados para `minimun_epsilon=0.01` e `target_step=7000`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWmAZ8MiqtpV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from cap01.decaying_schemes import exponential_decay, linear_decay\n",
        "\n",
        "mininimum_epsilon = 0.01\n",
        "target_step = 7000\n",
        "step_sequence = range(1, 10000 + 1)\n",
        "\n",
        "for label, decay_scheme in [(\"exponential\", exponential_decay), (\"linear\", linear_decay)]:\n",
        "    get_epsilon_fn = decay_scheme(mininimum_epsilon, target_step)\n",
        "    plt.plot(step_sequence, [get_epsilon_fn(t) for t in step_sequence], label=label)\n",
        "\n",
        "plt.legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y1BskBxGuDP1"
      },
      "source": [
        "\n",
        "Note que tanto `exponential_decay()` como `linear_decay()` retornam funções. A função retornada recebe o passo atual e retorna o valor correspondente desejado (para *epsilon*). Veja o arquivo `decaying_schemes.py` para entender melhor."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndm1mcbRs8RN"
      },
      "source": [
        "Agora é com você: implemente o **decaying epsilon-greedy** simplesmente integrando alguma dessas formas de decaimento (ou ambas) ao código do epsilon-greedy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZtRUb9kztFi_"
      },
      "outputs": [],
      "source": [
        "# faça aqui\n",
        "def run_decaying_epsilon_greedy():\n",
        "  pass\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B_F2AShc4K_K"
      },
      "source": [
        "## 5 - UCB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ST_5BJavuYBA"
      },
      "outputs": [],
      "source": [
        "from cap01.ucb import run_ucb"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "swPxD3Cth1PM"
      },
      "source": [
        "O **UCB** usa uma única fórmula para avaliar cada possível ação. A fórmula tem dois termos:\n",
        "1. O **Q** da ação, que é a estimativa da recompensa média. Serve como termo de \"exploitação\".\n",
        "1. E um termo que mede a *incerteza* de **Q** em relação ao valor real da média. Serve como fator de exploração.\n",
        "  - A incerteza é baixa para ações escolhidas \"muitas\" vezes, pois o valor de **Q** provavelmente está próximo da média real.\n",
        "  - E é alta para ações escolhidas relativamente \"poucas\" vezes, pois o valor de **Q** tem mais chance de estar impreciso.\n",
        "\n",
        "O UCB escolhe sempre a ação que dê a melhor soma dos dois termos acima.\n",
        "\n",
        "Um parâmetro **c** controla o peso do termo de incerteza. O valor padrão é `c=2.0`.\n",
        "\n",
        "Abaixo, testamos o UCB com diferentes valores do `c`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9NYqAmPztCH"
      },
      "outputs": [],
      "source": [
        "BANDIT_PROBABILITIES = [0.2, 0.5, 0.75]\n",
        "env = MultiArmedBanditEnv(BANDIT_PROBABILITIES)\n",
        "#env = GaussianMultiArmedBanditEnv(BANDIT_PROBABILITIES)\n",
        "\n",
        "rewards, _ = run_ucb(env, 10000, c=2.0)\n",
        "print(f\"UCB(c=2.0) - soma de recompensas:\", sum(rewards))\n",
        "\n",
        "rewards, _ = run_ucb(env, 10000, c=1.0)\n",
        "print(f\"UCB(c=1.0) - soma de recompensas:\", sum(rewards))\n",
        "\n",
        "rewards, _ = run_ucb(env, 10000, c=0.5)\n",
        "print(f\"UCB(c=0.5) - soma de recompensas:\", sum(rewards))\n",
        "\n",
        "rewards, _ = run_random(env, total_steps=10000)\n",
        "print(\"Random - soma de recompensas:\", sum(rewards))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R-mAkwkv4bX"
      },
      "outputs": [],
      "source": [
        "RUNS  = 50\n",
        "STEPS = 10000\n",
        "\n",
        "results = []\n",
        "results.append( repeated_exec(RUNS, \"RANDOM\", run_random, env, STEPS) )\n",
        "\n",
        "for param_c in [0.5, 1.0, 2.0]:\n",
        "  results.append( repeated_exec(RUNS, f\"UCB(c={param_c})\", run_ucb, env, STEPS, param_c) )\n",
        "\n",
        "plot_multiple_results(results, cumulative='avg', x_log_scale=True, yreference=env.get_max_mean_reward())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yxk29NaSvJHL"
      },
      "source": [
        "Vemos, no gráfico acima, que os valores menores de *c* tiveram melhor desempenho.\n",
        "\n",
        "Abaixo, mostramos especificamente o gráfico do UCB com `c=0.5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6lhGvVCQCgd"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results([results[1]], cumulative='avg', x_log_scale=True, yreference=env.get_max_mean_reward(), plot_stddev=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r_BCBC_bvqX3"
      },
      "source": [
        "## 6 - Comparação Geral"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_HekmNjUx5V1"
      },
      "source": [
        "Vamos fazer experimentos comparando:\n",
        "- **random** para servir de comparação, como solução \"ruim\"\n",
        "- **epsilon-greedy** com o melhor valor de *epsilon* que encontramos antes\n",
        "- **ucb** com o melhor valor de *c* que encontramos\n",
        "\n",
        "Cada experimento vai executar (reiniciando) cada algoritmo 70 vezes. Cada execução terá 100 mil passos. (Fique à vontade se quiser alterar).\n",
        "\n",
        "Faremos experimentos separados para as duas versões de ambiente, nas duas subseções a seguir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6cNED7EcxusV"
      },
      "outputs": [],
      "source": [
        "# parâmetros dos algoritmos\n",
        "epsilon = 0.02\n",
        "c = 0.5\n",
        "\n",
        "# parâmetros que controlam a repetição e duração dos experimentos\n",
        "RUNS  = 70\n",
        "STEPS = 100_000\n",
        "\n",
        "# para carregar automaticamente os resultados de simulações já executadas\n",
        "auto_load = True"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4eTe3xicwuQe"
      },
      "source": [
        "### 6.1 - Com Multi-Armed Bandit de recompensas binárias"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g0lejn0yc3QK"
      },
      "source": [
        "Aqui, usamos o ambiente padrão, com probabilidades bem próximas, para tentar \"confundir\" os algoritmos no início de cada execução."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uyDu-ZKRxhEd"
      },
      "outputs": [],
      "source": [
        "BANDIT_PROBABILITIES = [0.4, 0.5, 0.55]\n",
        "env = MultiArmedBanditEnv(BANDIT_PROBABILITIES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhglpsL4uUuA"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "results.append( repeated_exec(RUNS, \"RANDOM\", run_random, env, STEPS, auto_load=auto_load) )\n",
        "results.append( repeated_exec(RUNS, f\"EPS({epsilon})-GREEDY\", run_epsilon_greedy, env, STEPS, epsilon, auto_load=auto_load) )\n",
        "results.append( repeated_exec(RUNS, f\"UCB(c={c})\", run_ucb, env, STEPS, c, auto_load=auto_load) )\n",
        "\n",
        "plot_multiple_results(results, cumulative='avg', x_log_scale=True, yreference=env.get_max_mean_reward())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ6HjNoyeBsK"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results, cumulative='avg', x_log_scale=True, yreference=env.get_max_mean_reward(), plot_stddev=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8tc5dpYswzV-"
      },
      "source": [
        "### 6.2 - Com Multi-Armed Bandit de recompensas gaussianas"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rbTIm0n1dxl5"
      },
      "source": [
        "Aqui, usamos o ambiente com recompensas gaussianas (distribuição normal), com médias bem próximas, para tentar \"confundir\" os algoritmos no início.\n",
        "\n",
        "*Atenção: Talvez os valores de **c** e **epsilon** que definimos antes não sejam tão bons nesta versão, porque eles foram escolhidos com experimentos na versão padrão.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "CrUE2WOTxbgG"
      },
      "outputs": [],
      "source": [
        "BANDIT_MEANS = [0.4, 0.5, 0.55]\n",
        "env_gauss = GaussianMultiArmedBanditEnv(BANDIT_MEANS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EThDhfbPwzus"
      },
      "outputs": [],
      "source": [
        "results_gauss = []\n",
        "\n",
        "results_gauss.append( repeated_exec(RUNS, \"RANDOM\", run_random, env_gauss, STEPS, auto_load=auto_load) )\n",
        "results_gauss.append( repeated_exec(RUNS, f\"EPS({epsilon})-GREEDY\", run_epsilon_greedy, env_gauss, STEPS, epsilon, auto_load=auto_load) )\n",
        "results_gauss.append( repeated_exec(RUNS, f\"UCB(c={c})\", run_ucb, env_gauss, STEPS, c, auto_load=auto_load) )\n",
        "\n",
        "plot_multiple_results(results_gauss, cumulative='avg', x_log_scale=True, yreference=env_gauss.get_max_mean_reward())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQFRTOVgxpXv"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results_gauss, cumulative='avg', x_log_scale=True, yreference=env_gauss.get_max_mean_reward(), plot_stddev=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W48yvhviFUEk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "rl23",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
