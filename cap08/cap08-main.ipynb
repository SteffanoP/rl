{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeJ3wCaKe2Wl"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap08/cap08-main.ipynb)\n",
        "\n",
        "# Capítulo 8 - DQN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAHITU7VhsM7"
      },
      "source": [
        "## 1 - Configurações Iniciais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk1160jwoH9y"
      },
      "source": [
        "Instalação de pacotes e atribuição do caminho para o código do projeto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NS23BU8R1vq-"
      },
      "outputs": [],
      "source": [
        "import pygame # this import here is just to prevent a strange bug (with some dynamic library used by pygame and other packages)\n",
        "\n",
        "import sys\n",
        "from IPython.display import clear_output\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install opencv-python\n",
        "    !pip install swig\n",
        "    !pip install gymnasium[all]   # roda no 1.0.0\n",
        "    !pip install ale-py           # para os jogos de Atari\n",
        "    #!pip install autorom[accept-rom-license]\n",
        "    !pip install tensorboard\n",
        "\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j09GCBHi_83"
      },
      "source": [
        "Carrega a extensão para visualizar o `tensorboard` em um notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N903Nh7fRL0S"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akq_mR_ii_84"
      },
      "source": [
        "Importa pacotes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0Gzf7VhkiHxQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import tensorboard\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "__2WQEWRk2a4"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import ale_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0jXaW5bSkf-_"
      },
      "outputs": [],
      "source": [
        "from cap08 import dqn_models\n",
        "from cap08.atari_wrappers import *\n",
        "from cap08.qnet_helper import record_video_qnet\n",
        "\n",
        "from util.notebook import display_videos_from_path, display_videos_from_path_widgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G2vNfXbwi_86"
      },
      "outputs": [],
      "source": [
        "# usar GPU compatível com CUDA costuma ser mais rápido\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "torch.set_default_device(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztvJdbKVh20Y"
      },
      "source": [
        "## 2 - DQN - Definições Auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxJqaLxPi_86"
      },
      "source": [
        "Adaptado do código explicado no livro **\"Deep Reinforcement Learning Hands On\"** (Maxim Lapan), Chapter 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_79PzMRmndhC"
      },
      "source": [
        "### Classes Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8OACm0r-iuh2"
      },
      "outputs": [],
      "source": [
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class DQNExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, s1, a, r, done, s2):\n",
        "        experience = Experience(s1, a, r, done, s2)\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzwVdZIf3xi6"
      },
      "source": [
        "### Funções Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tGMkPSqg3wNx"
      },
      "outputs": [],
      "source": [
        "# Faz uma escolha epsilon-greedy\n",
        "def epsilon_greedy_qnet(qnet, env, state, epsilon):\n",
        "    if np.random.random() < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        state_v = torch.tensor(state, dtype=torch.float32)\n",
        "        state_v = state_v.unsqueeze(0)  # Adiciona dimensão de batch como eixo 0 (e.g. transforma uma lista [a,b,c] em [[a,b,c]])\n",
        "        q_vals_v = qnet(state_v)\n",
        "        _, act_v = torch.max(q_vals_v, dim=1)\n",
        "        action = int(act_v.item())\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3PH-mJQ6ndhD"
      },
      "outputs": [],
      "source": [
        "# loss function, para treinamento da rede no DQN\n",
        "def calc_loss(batch, net, tgt_net, gamma):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "    states_v = torch.tensor(states, dtype=torch.float32)\n",
        "    next_states_v = torch.tensor(next_states, dtype=torch.float32)\n",
        "    actions_v = torch.tensor(actions, dtype=torch.int64)\n",
        "    rewards_v = torch.tensor(rewards)\n",
        "    done_mask = torch.tensor(dones, dtype=torch.bool)\n",
        "\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    next_state_values = tgt_net(next_states_v).max(dim=1)[0]\n",
        "    next_state_values[done_mask] = 0.0\n",
        "    next_state_values = next_state_values.detach()\n",
        "\n",
        "    target_state_action_values = rewards_v + gamma * next_state_values\n",
        "    return nn.MSELoss()(state_action_values, target_state_action_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c241EVuundhE"
      },
      "source": [
        "## 3 - DQN - Função Principal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsqgk9RNndhE"
      },
      "source": [
        "Esta é a função que faz o aprendizado. (Porém, o DQN é uma solução maior, pensada para jogos de Atari, e que inclui também os wrappers, que serão usados na seção 6.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JO3eR_sfndhE"
      },
      "outputs": [],
      "source": [
        "def DQN_TRAIN(env, env_name, gamma, qnet, qnet_lr, target_qnet, target_update_freq, replay_size, batch_size, epsilon_f, epsilon_decay_period, GOAL_REWARD):\n",
        "    print(qnet)\n",
        "\n",
        "    # Cria o otimizador, que vai fazer o ajuste dos pesos da 'qnet',\n",
        "    # Usa uma técnica de gradiente descendente de destaque, chamada ADAM\n",
        "    optimizer = optim.Adam(qnet.parameters(), lr=qnet_lr)\n",
        "\n",
        "    # Para o logging de dados, para serem exibidos no tensorboard\n",
        "    writer = SummaryWriter(comment=\"-\" + env_name)\n",
        "\n",
        "    buffer = DQNExperienceBuffer(replay_size)\n",
        "\n",
        "    start_time_str = datetime.now().strftime(\"%Y-%m-%d,%H-%M-%S\")\n",
        "    episode_reward_list = []\n",
        "    step = 0\n",
        "    epsilon = 1.0\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    episode_reward = 0.0\n",
        "    episode_start_step = 0\n",
        "    episode_start_time = time.time()\n",
        "\n",
        "    while True:\n",
        "        # Decaimento linear do epsilon\n",
        "        epsilon = max(epsilon_f, 1.0 - step / epsilon_decay_period)\n",
        "\n",
        "        action = epsilon_greedy_qnet(qnet, env, state, epsilon)\n",
        "\n",
        "        # Faz um passo / Aplica uma ação no ambiente\n",
        "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        step += 1\n",
        "        done = terminated or truncated\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Adiciona no buffer\n",
        "        buffer.append(state, action, reward, terminated, new_state)\n",
        "        state = new_state\n",
        "\n",
        "        if step % 10_000 == 0:\n",
        "            clear_output()\n",
        "\n",
        "        if done:\n",
        "            episode_reward_list.append(episode_reward)\n",
        "            speed = (step - episode_start_step) / (time.time() - episode_start_time + 0.00001)\n",
        "\n",
        "            state, _ = env.reset()\n",
        "            episode_reward = 0.0\n",
        "            episode_start_step = step\n",
        "            episode_start_time = time.time()\n",
        "\n",
        "            # Abaixo, faz vários loggings de dados\n",
        "            mean_reward = np.mean(episode_reward_list[-100:])\n",
        "            print(f\"{step}: finished {len(episode_reward_list)} episodes, mean reward {mean_reward:.3f}, eps {epsilon:.2f}, speed {speed:.2f} steps/s\")\n",
        "            writer.add_scalar(\"epsilon\", epsilon, step)\n",
        "            writer.add_scalar(\"epi_reward_100\", mean_reward, step)\n",
        "            writer.add_scalar(\"epi_reward\", episode_reward, step)\n",
        "\n",
        "            # Testa se \"resolveu\" o ambiente\n",
        "            if mean_reward > GOAL_REWARD:\n",
        "                print(f\"Solved in {step} steps with mean reward {mean_reward:.3f}\")\n",
        "                filename = env_name + \"-\" + start_time_str + \".dat\"\n",
        "                torch.save(qnet.state_dict(), filename)\n",
        "                print(f\"Model saved as {filename}\")\n",
        "                break\n",
        "\n",
        "        if len(buffer) >= replay_size:\n",
        "            # Faz a 'tgt_net' receber os mesmos valores de pesos da 'qnet', na frequência indicada\n",
        "            if step % target_update_freq == 0:\n",
        "                target_qnet.load_state_dict(qnet.state_dict())\n",
        "\n",
        "            # Escolhe amostras aleatórios do buffer e faz uma atualização dos pesos da rede\n",
        "            optimizer.zero_grad()\n",
        "            batch = buffer.sample(batch_size)\n",
        "            loss_t = calc_loss(batch, qnet, target_qnet, gamma)\n",
        "            loss_t.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZq9U2FGndhF"
      },
      "source": [
        "## 4 - Treinando em um Ambientes Simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SigPdPRcndhF"
      },
      "outputs": [],
      "source": [
        "SIMPLE_ENV_NAME = \"CartPole-v1\"\n",
        "GOAL_REWARD = 200\n",
        "\n",
        "#SIMPLE_ENV_NAME = \"MountainCar-v0\"\n",
        "#GOAL_REWARD = -120\n",
        "\n",
        "GAMMA = 0.999\n",
        "REPLAY_SIZE = 5_000\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "SYNC_TARGET_FRAMES = 1_000\n",
        "\n",
        "EPSILON_DECAY_PERIOD = 40_000\n",
        "EPSILON_FINAL = 0.02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P0XKgIXSKdQV"
      },
      "outputs": [],
      "source": [
        "# Cria o ambiente\n",
        "env1 = gym.make(SIMPLE_ENV_NAME)\n",
        "\n",
        "# Cria as redes neurais\n",
        "qnet1 = dqn_models.MLP(env1.observation_space.shape[0], [128,128], env1.action_space.n)\n",
        "qtarget1 = dqn_models.MLP(env1.observation_space.shape[0], [128,128], env1.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1Ek-0R7ui_89"
      },
      "outputs": [],
      "source": [
        "# Para carregar uma rede salva de arquivo, descomente o bloco abaixo\n",
        "# Essa rede salva pode ser testada ou treinada por mais alguns passos\n",
        "'''\n",
        "filename = filename + \"<Nome do arquivo>.dat\"\n",
        "qnet1.load_state_dict(torch.load(filename, map_location=lambda storage,loc: storage))\n",
        "#''';"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY2pju8dymgV"
      },
      "outputs": [],
      "source": [
        "# Para treinar o agente, rode o código abaixo\n",
        "#'''\n",
        "\n",
        "DQN_TRAIN(\n",
        "    env = env1,\n",
        "    env_name = SIMPLE_ENV_NAME,\n",
        "    gamma = GAMMA,\n",
        "    qnet = qnet1,\n",
        "    qnet_lr = LEARNING_RATE,\n",
        "    target_qnet = qtarget1,\n",
        "    target_update_freq = SYNC_TARGET_FRAMES,\n",
        "    replay_size = REPLAY_SIZE,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    epsilon_f = EPSILON_FINAL,\n",
        "    epsilon_decay_period = EPSILON_DECAY_PERIOD,\n",
        "    GOAL_REWARD = GOAL_REWARD)\n",
        "\n",
        "#'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzC7IWB4i_89"
      },
      "outputs": [],
      "source": [
        "record_video_qnet(SIMPLE_ENV_NAME, qnet1, episodes=2, folder=\"./dqn-simple\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cijxLQTqt0Ja"
      },
      "outputs": [],
      "source": [
        "display_videos_from_path('./dqn-simple')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fttzaanEMzvF"
      },
      "source": [
        "## 5 - Visualização dos Resultados dos Treinamentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_iUaT7nM4ZV"
      },
      "source": [
        "Estamos, aqui, usando o **Tensorboard** para acompanhar os dados do treinamento em tempo real. Este módulo foi criado para o Tensorflow, mas é compatível com Pytorch.\n",
        "\n",
        "Basta rodar uma vez e acompanhar. Ele pode também ser executado antes da seção 4, para acompanhar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk0UohtUjSnb"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    %tensorboard --logdir runs\n",
        "else:\n",
        "    %tensorboard --logdir cap08/runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qa55xn4gi_8-"
      },
      "outputs": [],
      "source": [
        "# Mostra o tensorboard no notebook, ocupando a altura indicada\n",
        "#tensorboard.notebook.display(height=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP1Xyu4wndhH"
      },
      "source": [
        "## 6 - Treinando no Jogo Pong (Atari)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MvEJ8CNvndhH"
      },
      "outputs": [],
      "source": [
        "# Veja outros em: https://gymnasium.farama.org/environments/atari/\n",
        "# Se mudar o jogo, lembre-se de alterar também o GOAL_REWARD abaixo!\n",
        "ATARI_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "\n",
        "# Recompensa alvo; no Pong, esta é a diferença de pontos do player para a \"cpu\", sendo +21.0 o máximo e -21.0 o mínimo\n",
        "# Tente com algum valor negativo (e.g. -15.0) para um treinamento mais rápido, ou algum valor positivo (+15.0) para ver o agent ganhar da \"cpu\"\n",
        "GOAL_REWARD = -10.0 #0.0\n",
        "\n",
        "# Parâmetros do DQN\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 20_000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1_500\n",
        "\n",
        "EPSILON_DECAY_PERIOD = 100_000\n",
        "EPSILON_FINAL = 0.02\n",
        "\n",
        "env2 = gym.make(ATARI_ENV_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OxNw6F_hywXV"
      },
      "outputs": [],
      "source": [
        "# Aplica os wrappers do DQN para ambientes Atari\n",
        "env2a = MaxAndSkipEnv(env2)\n",
        "env2b = FireResetEnv(env2a)\n",
        "env2c = ProcessFrame84(env2b)\n",
        "env2d = ImageToPyTorch(env2c)\n",
        "env2e = BufferWrapper(env2d, 4)\n",
        "env2f = ScaledFloatFrame(env2e)\n",
        "\n",
        "# Cria as redes neurais\n",
        "qnet2 = dqn_models.DQNNet(env2f.observation_space.shape, env2f.action_space.n)\n",
        "qtarget2 = dqn_models.DQNNet(env2f.observation_space.shape, env2f.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CGUxgf_iiOU"
      },
      "outputs": [],
      "source": [
        "# Mostrando a tela do ambiente original, e outras telas após passar por alguns wrappers\n",
        "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12,5))\n",
        "\n",
        "s, _ = env2.reset()\n",
        "ax1.imshow(s)\n",
        "ax1.set_title('Tela antes de iniciar')\n",
        "\n",
        "s, _ = env2b.reset()\n",
        "ax2.imshow(s)\n",
        "ax2.set_title('Tela da partida iniciada automaticamente')\n",
        "\n",
        "s, _ = env2d.reset()\n",
        "ax3.imshow(s[0], cmap='gray', vmin=0, vmax=255)  # exibe em escala de cinza\n",
        "ax3.set_title('Imagem processada');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Ynqnn8eBi_8-"
      },
      "outputs": [],
      "source": [
        "# Para carregar uma rede salva de arquivo, descomente o bloco abaixo\n",
        "# Permite testar ou treinar mais\n",
        "'''\n",
        "filename = \"/content/rl_facil/cap08/\" if IN_COLAB else \"\"\n",
        "filename = filename + \"PongNoFrameskip-v4-2023-11-21,18-35-12.dat\"\n",
        "qnet2.load_state_dict(torch.load(filename, map_location=lambda storage,loc: storage))\n",
        "#''';"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umDKxHFVndhK"
      },
      "outputs": [],
      "source": [
        "# Para treinar o agente, rode o código abaixo\n",
        "\n",
        "DQN_TRAIN(\n",
        "    env = env2f,\n",
        "    env_name = ATARI_ENV_NAME,\n",
        "    gamma = GAMMA,\n",
        "    qnet = qnet2,\n",
        "    qnet_lr = LEARNING_RATE,\n",
        "    target_qnet = qtarget2,\n",
        "    target_update_freq = SYNC_TARGET_FRAMES,\n",
        "    replay_size = REPLAY_SIZE,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    epsilon_f = EPSILON_FINAL,\n",
        "    epsilon_decay_period = EPSILON_DECAY_PERIOD,\n",
        "    GOAL_REWARD = GOAL_REWARD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzVZv90bvG1x"
      },
      "outputs": [],
      "source": [
        "# Roda alguns episódigos com o modelo e salva os vídeos em arquivos\n",
        "# Atenção: precisa rodar com os wrappers aplicados!\n",
        "env2 = gym.make(ATARI_ENV_NAME, render_mode=\"rgb_array\")\n",
        "env2a = MaxAndSkipEnv(env2)\n",
        "env2b = FireResetEnv(env2a)\n",
        "env2c = ProcessFrame84(env2b)\n",
        "env2d = ImageToPyTorch(env2c)\n",
        "env2e = BufferWrapper(env2d, 4)\n",
        "env2f = ScaledFloatFrame(env2e)\n",
        "\n",
        "record_video_qnet(env2f, qnet2, episodes=1, folder=\"./dqn-atari\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lYCL7Plv3RY"
      },
      "outputs": [],
      "source": [
        "display_videos_from_path('./dqn-atari')\n",
        "#display_videos_from_path_widgets(\"./dqn-atari\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bU6eb4miMody"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cap08-main.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "rl23",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
