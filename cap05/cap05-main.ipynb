{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeJ3wCaKe2Wl"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap05/cap05-main.ipynb)\n",
    "\n",
    "# Capítulo 5 - Q-Learning, SARSA e Expected-SARSA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAHITU7VhsM7"
   },
   "source": [
    "## 1. Configurações Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NS23BU8R1vq-"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # for saving videos\n",
    "    !apt-get install ffmpeg\n",
    "\n",
    "    !pip install gymnasium, moviepy\n",
    "\n",
    "    # clone repository\n",
    "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
    "    sys.path.append(\"/content/rl_facil\")\n",
    "\n",
    "    clear_output()\n",
    "else:\n",
    "    from os import path\n",
    "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euyQqkX2x8K4"
   },
   "source": [
    "### Imports Gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ya6gLbpmx8K4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Gzf7VhkiHxQ"
   },
   "outputs": [],
   "source": [
    "from util.experiments import repeated_exec\n",
    "from util.plot import plot_result, plot_multiple_results\n",
    "from util.notebook import display_videos_from_path\n",
    "from util.qtable_helper import evaluate_qtable, record_video_qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cj6MsnLTx8K6"
   },
   "outputs": [],
   "source": [
    "# define se os algoritmos vão imprimir dados do treinamento\n",
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlVmoMADx8K7"
   },
   "source": [
    "## 1. Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDHgaAaXFktL"
   },
   "source": [
    "Veremos aqui, vários algoritmos da família *Temporal-Difference Learning* de 1 passo. Todos eles se baseiam na em uma estimativa da função $Q(s,a)$. Porém, diferente dos Monte Carlo, eles apresendem a cada passo de interação com o episódio.\n",
    "\n",
    "Ao invés do retorno parcial real $G_t$, todos usam uma estimativa *bootstraped* baseada nas equações de Bellamn.\n",
    "\n",
    "Após uma experiência $(s,a,r,s')$, eles atualizam $Q$ assim:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\times (r + \\gamma V(s') - Q(s,a))\n",
    "$$\n",
    "\n",
    "O método **Q-Learning** tem como característica a suposição de que, a partir de s', será executada a ação \"gulosa\". Assim, ele assume que um $s'$ não-terminal terá seu valor dado pela ação de melhor valor (retorno futuro a partir dela):\n",
    "\n",
    "$$\n",
    "V(s') = max_{a'}(Q(s',a'))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0I0NlS6Jx8K7"
   },
   "outputs": [],
   "source": [
    "# Esta é a política. Neste caso, escolhe uma ação com base nos valores\n",
    "# da tabela Q, usando uma estratégia epsilon-greedy.\n",
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    num_actions = len(Q[state])\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(0, num_actions)\n",
    "    else:\n",
    "        return np.argmax(Q[state])   # em caso de empates, retorna sempre o menor índice --> mais eficiente, porém...\n",
    "        #return np.random.choice(np.where(Q[state] == Q[state].max())[0]) # aleatoriza em caso de empates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_mBlDYQx8K8"
   },
   "outputs": [],
   "source": [
    "# Algoritmo Q-learning\n",
    "# Atenção: os espaços de estados e de ações precisam ser discretos, dados por valores inteiros\n",
    "def run_qlearning(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1):\n",
    "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # inicializa a tabela Q toda com zeros\n",
    "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
    "    Q = np.zeros(shape = (env.observation_space.n, num_actions))\n",
    "\n",
    "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
    "    all_episode_rewards = []\n",
    "\n",
    "    # loop principal\n",
    "    for i in range(episodes):\n",
    "        done = False\n",
    "        sum_rewards, reward = 0, 0\n",
    "\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        # executa 1 episódio completo, fazendo atualizações na Q-table\n",
    "        while not done:\n",
    "            # escolhe a próxima ação -- usa epsilon-greedy\n",
    "            action = epsilon_greedy(Q, state, epsilon)\n",
    "\n",
    "            # realiza a ação, ou seja, dá um passo no ambiente\n",
    "            next_state, reward, terminated, trunc, _ = env.step(action)\n",
    "            done = terminated or trunc\n",
    "\n",
    "            if terminated:\n",
    "                # para estados terminais\n",
    "                V_next_state = 0\n",
    "            else:\n",
    "                # para estados não-terminais -- valor máximo (melhor ação)\n",
    "                V_next_state = np.max(Q[next_state])\n",
    "\n",
    "            # atualiza a Q-table\n",
    "            # delta = (estimativa usando a nova recompensa) - estimativa antiga\n",
    "            delta = (reward + gamma * V_next_state) - Q[state,action]\n",
    "            Q[state,action] = Q[state,action] + lr * delta\n",
    "\n",
    "            sum_rewards += reward\n",
    "            state = next_state\n",
    "\n",
    "        all_episode_rewards.append(sum_rewards)\n",
    "\n",
    "        # a cada 100 episódios, imprime informação sobre o progresso\n",
    "        if VERBOSE and ((i+1) % 100 == 0):\n",
    "            avg_reward = np.mean(all_episode_rewards[-100:])\n",
    "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
    "\n",
    "    return all_episode_rewards, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Pgs0um7x8K8"
   },
   "outputs": [],
   "source": [
    "#ENV_NAME, r_max = \"Taxi-v3\", 10\n",
    "ENV_NAME, r_max = \"CliffWalking-v0\", 0\n",
    "\n",
    "EPISODES = 8_000\n",
    "LR = 0.01\n",
    "GAMMA = 0.95\n",
    "EPSILON = 0.05\n",
    "\n",
    "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
    "\n",
    "# Roda o algoritmo Q-Learning\n",
    "rewards1, qtable1 = run_qlearning(env, EPISODES, LR, GAMMA, EPSILON)\n",
    "clear_output()\n",
    "\n",
    "print(\"Últimos resultados: media =\", np.mean(rewards1[-20:]), \", desvio padrao =\", np.std(rewards1[-20:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9eZBjutx8K8"
   },
   "outputs": [],
   "source": [
    "# Mostra um gráfico de episódios x retornos não descontados\n",
    "plot_result(rewards1, r_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnK7OuHPFktN"
   },
   "outputs": [],
   "source": [
    "# Avalia, usando a tabela de forma greedy\n",
    "evaluate_qtable(env, qtable1, num_episodes=10, epsilon=0.0, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVfR9GGBx8K9"
   },
   "outputs": [],
   "source": [
    "# Salva vídeos, usando a tabela de forma greedy\n",
    "record_video_qtable(ENV_NAME, qtable1, episodes=2, folder='videos/', prefix='qlearning', epsilon=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoofIvgKFktN"
   },
   "outputs": [],
   "source": [
    "# Exibe o vídeo gravado\n",
    "display_videos_from_path('videos/', prefix='qlearning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztvJdbKVh20Y"
   },
   "source": [
    "## 2. SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ULLPHx7x8K9"
   },
   "source": [
    "O **SARSA** é um algoritmo semelhante ao Q-Learning. Porém, ele usa um valor de $V(s')$ baseado na próxima ação ($a'$) realmente escolhida.\n",
    "\n",
    "Em outras palavras, o Q-Learning:\n",
    "- usa os dados $(s,a,r,s')$ para fazer uma atualização no $Q(s,a)$\n",
    "- assume que o valor do próximo estado é (guloso) baseado na melhor ação possível: $V(s') = max_{a'}{Q(s', a')}$\n",
    "\n",
    "Porém, o SARSA:\n",
    "- usa os dados $(s,a,r,s',a')$ para fazer uma atualização no $Q(s,a)$\n",
    "- assume que o valor do próximo estado é o resultado da verdadeira próxima ação $a'$: $V(s') = Q(s', a')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_ALlauax8K9"
   },
   "source": [
    "Crie o código do SARSA abaixo, com base no código do Q-Learning. Dicas:\n",
    "- para atualizar o $Q$, você precisa desta 4 variáveis coerentemente mantidas: `state, action, next_state, next_action`\n",
    "- escolha a \"próxima ação\" imediatamente após cada passo (e logo após o reset) e antes da atualização do $Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-LuUb_Kx8K9"
   },
   "outputs": [],
   "source": [
    "# Algoritmo SARSA\n",
    "# Atenção: os espaços de estados e de ações precisam ser discretos, dados por valores inteiros\n",
    "def run_sarsa(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1):\n",
    "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # inicializa a tabela Q com valores aleatórios de -1.0 a 0.0\n",
    "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
    "    Q = np.zeros(shape = (env.observation_space.n, num_actions))\n",
    "\n",
    "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
    "    all_episode_rewards = []\n",
    "\n",
    "    # loop principal\n",
    "    for i in range(episodes):\n",
    "        done = False\n",
    "        sum_rewards, reward = 0, 0\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        action = epsilon_greedy(Q, state, epsilon)\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, terminated, trunc, _ = env.step(action)\n",
    "            done = terminated or trunc\n",
    "        \n",
    "            next_action = epsilon_greedy(Q, next_state, epsilon)\n",
    "\n",
    "            if terminated:\n",
    "                # para estados terminais\n",
    "                V_next_state = 0\n",
    "            else:\n",
    "                # para estados não-terminais\n",
    "                V_next_state = Q[next_state, next_action]\n",
    "\n",
    "            # atualiza a Q-table\n",
    "            # delta = (estimativa usando a nova recompensa) - estimativa antiga\n",
    "            delta = (reward + gamma * V_next_state) - Q[state,action]\n",
    "            Q[state,action] = Q[state,action] + lr * delta\n",
    "\n",
    "            sum_rewards += reward\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "        all_episode_rewards.append(sum_rewards)\n",
    "\n",
    "        # a cada 100 episódios, imprime informação sobre o progresso\n",
    "        if VERBOSE and ((i+1) % 100 == 0):\n",
    "            avg_reward = np.mean(all_episode_rewards[-100:])\n",
    "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
    "\n",
    "    return all_episode_rewards, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3K9Nqgwx8K-"
   },
   "source": [
    "Se não conseguir implementar, descomente o código abaixo para prosseguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxpkB2SAFktO"
   },
   "outputs": [],
   "source": [
    "#from cap05.qlearning_sarsa import run_sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95q2OmlVx8K-"
   },
   "outputs": [],
   "source": [
    "#ENV_NAME, r_max = \"Taxi-v3\", 10\n",
    "ENV_NAME, r_max = \"CliffWalking-v0\", 0\n",
    "\n",
    "EPISODES = 8_000\n",
    "LR = 0.05\n",
    "GAMMA = 0.95\n",
    "EPSILON = 0.05\n",
    "\n",
    "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
    "\n",
    "# Roda o algoritmo SARSA\n",
    "rewards2, qtable2 = run_sarsa(env, EPISODES, LR, GAMMA, EPSILON)\n",
    "clear_output()\n",
    "\n",
    "print(\"Últimos resultados: media =\", np.mean(rewards2[-20:]), \", desvio padrao =\", np.std(rewards2[-20:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MemHbKN6x8K-"
   },
   "outputs": [],
   "source": [
    "plot_result(rewards2, r_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VV-dDyLyFktO"
   },
   "outputs": [],
   "source": [
    "evaluate_qtable(env, qtable2, num_episodes=10, epsilon=0.0, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlHqnyDdx8K-"
   },
   "outputs": [],
   "source": [
    "record_video_qtable(ENV_NAME, qtable2, episodes=2, folder='videos/', prefix='sarsa', epsilon=0.0)\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4JXLsR2FktP"
   },
   "outputs": [],
   "source": [
    "display_videos_from_path('videos/', prefix='sarsa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cncfa3ghx8K-"
   },
   "source": [
    "## 3. Expected-SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-bhDTXfx8K_"
   },
   "source": [
    "O Expected-SARSA tem semelhanças com os dois algoritmos vistos antes:\n",
    "- usa os dados $(s,a,r,s')$ para fazer uma atualização no $Q(s,a)$\n",
    "- assume que o valor do próximo estado é o valor médio, com base nas probabilidades dadas pela política:\n",
    "   $$V(s') = \\displaystyle\\sum_{a'} \\pi(a'|s') \\times Q(s',a')$$\n",
    "\n",
    "Para implementá-lo, precisamos das probabilidades da política realizar cada ação.\n",
    "\n",
    "A função abaixo retorna as probabilidades de cada ação na política *epsilon-greedy*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiXy1k97x8K_"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_probs(Q, state, num_actions, epsilon):\n",
    "    # lista com a probabilidade que todas as ações têm de ser escolhidas nas decisões exploratórias (não-gulosas)\n",
    "    probs = [ (epsilon / num_actions) ] * num_actions\n",
    "\n",
    "    # adiciona a probabilidade para a ação 'greedy'\n",
    "    # em caso de empate, só o menor índice é considerado\n",
    "    # (isso é coerente com a implementação da função epsilon_greedy())\n",
    "    greedy_action = np.argmax(Q[state])\n",
    "    probs[greedy_action] += (1.0 - epsilon)\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vez0f2dJx8K_"
   },
   "outputs": [],
   "source": [
    "# Algoritmo Expected-SARSA\n",
    "def run_expected_sarsa(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1):\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # inicializa a tabela Q toda com zeros\n",
    "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
    "    Q = np.zeros(shape = (env.observation_space.n, num_actions))\n",
    "\n",
    "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
    "    sum_rewards_per_ep = []\n",
    "\n",
    "    # loop principal\n",
    "    for i in range(episodes):\n",
    "        done = False\n",
    "        sum_rewards, reward = 0, 0\n",
    "\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        # executa 1 episódio completo, fazendo atualizações na Q-table\n",
    "        while not done:\n",
    "            # escolhe a próxima ação -- usa epsilon-greedy\n",
    "            action = epsilon_greedy(Q, state, epsilon)\n",
    "\n",
    "            # realiza a ação, ou seja, dá um passo no ambiente\n",
    "            next_state, reward, terminated, trunc, _ = env.step(action)\n",
    "            done = terminated or trunc\n",
    "\n",
    "            if terminated:\n",
    "                # para estados terminais\n",
    "                V_next_state = 0\n",
    "            else:\n",
    "                # para estados não-terminais -- valor esperado\n",
    "                p_next_actions = epsilon_greedy_probs(Q, next_state, num_actions, epsilon)\n",
    "                V_next_state = np.sum( np.asarray(p_next_actions) * Q[next_state] )\n",
    "\n",
    "            # atualiza a Q-table\n",
    "            # delta = (estimativa usando a nova recompensa) - estimativa antiga\n",
    "            delta = (reward + gamma * V_next_state) - Q[state,action]\n",
    "            Q[state,action] = Q[state,action] + lr * delta\n",
    "\n",
    "            sum_rewards += reward\n",
    "            state = next_state\n",
    "\n",
    "        # salva o retorno do episódio que encerrou\n",
    "        sum_rewards_per_ep.append(sum_rewards)\n",
    "\n",
    "        # a cada 100 episódios, imprime informação sobre o progresso\n",
    "        if VERBOSE and ((i+1) % 100 == 0):\n",
    "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
    "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
    "\n",
    "    return sum_rewards_per_ep, Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQoNjZ1Rx8K_"
   },
   "outputs": [],
   "source": [
    "#ENV_NAME, r_max = \"Taxi-v3\", 10\n",
    "ENV_NAME, r_max = \"CliffWalking-v0\", 0\n",
    "\n",
    "EPISODES = 8_000\n",
    "LR = 0.2\n",
    "GAMMA = 0.95\n",
    "EPSILON = 0.1\n",
    "\n",
    "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
    "\n",
    "rewards3, qtable3 = run_expected_sarsa(env, EPISODES, LR, GAMMA, EPSILON)\n",
    "clear_output()\n",
    "\n",
    "print(\"Últimos resultados: media =\", np.mean(rewards3[-20:]), \", desvio padrao =\", np.std(rewards3[-20:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUTaFHK_FktQ"
   },
   "outputs": [],
   "source": [
    "plot_result(rewards3, r_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icTmEms6FktQ"
   },
   "outputs": [],
   "source": [
    "evaluate_qtable(env, qtable3, num_episodes=10, epsilon=0.0, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VaV3UyAIFktQ"
   },
   "outputs": [],
   "source": [
    "record_video_qtable(ENV_NAME, qtable3, episodes=2, folder='videos/', prefix='expec-sarsa', epsilon=0.0)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TIU8OShFktQ"
   },
   "outputs": [],
   "source": [
    "display_videos_from_path('videos/', prefix='expec-sarsa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yF9fggfZzFVV"
   },
   "source": [
    "## 4. Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "352WmZAhx8K_"
   },
   "outputs": [],
   "source": [
    "# parâmetros para todos os algoritmos (não é o ideal)\n",
    "LR = 0.1\n",
    "GAMMA = 0.95\n",
    "EPSILON = 0.05\n",
    "\n",
    "# silencia os algoritmos\n",
    "VERBOSE = False\n",
    "\n",
    "# repetições de cada algoritmo\n",
    "RUNS = 5\n",
    "\n",
    "# para carregar automaticamente os resultados de simulações já executadas\n",
    "AUTO_LOAD = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmQyZXdex8LA"
   },
   "source": [
    "### Taxi-v3 (discreto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYSG9xiHxBKe"
   },
   "outputs": [],
   "source": [
    "environment = gym.make(\"Taxi-v3\")\n",
    "NUM_EPISODES = 5_000\n",
    "\n",
    "results_t = []\n",
    "\n",
    "results_t.append( repeated_exec(RUNS, f\"Q-Learning\", run_qlearning, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=AUTO_LOAD) )\n",
    "clear_output()\n",
    "results_t.append( repeated_exec(RUNS, f\"Sarsa\", run_sarsa, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=AUTO_LOAD) )\n",
    "clear_output()\n",
    "results_t.append( repeated_exec(RUNS, f\"Exp-Sarsa\", run_expected_sarsa, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=AUTO_LOAD) )\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmAc-tJKx8LA"
   },
   "outputs": [],
   "source": [
    "plot_multiple_results(results_t, cumulative=False, x_log_scale=False, window=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-I2_nawx8LA"
   },
   "source": [
    "### FrozenLake-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVZ3eXZyx8LA"
   },
   "outputs": [],
   "source": [
    "environment = gym.make(\"FrozenLake-v1\")\n",
    "NUM_EPISODES = 8_000\n",
    "\n",
    "results_f = []\n",
    "\n",
    "results_f.append( repeated_exec(RUNS, f\"Q-Learning\", run_qlearning, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=AUTO_LOAD) )\n",
    "clear_output()\n",
    "results_f.append( repeated_exec(RUNS, f\"Sarsa\", run_sarsa, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=AUTO_LOAD) )\n",
    "clear_output()\n",
    "results_f.append( repeated_exec(RUNS, f\"Exp-Sarsa\", run_expected_sarsa, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=AUTO_LOAD) )\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97AkZNAqx8LF"
   },
   "outputs": [],
   "source": [
    "plot_multiple_results(results_f, cumulative=False, x_log_scale=False, window=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhyQN_Wsx8LF"
   },
   "source": [
    "### Cliff Walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAVZPiSVx8LF"
   },
   "outputs": [],
   "source": [
    "environment = gym.make(\"CliffWalking-v0\", max_episode_steps=100)\n",
    "NUM_EPISODES = 1_000\n",
    "\n",
    "results_c = []\n",
    "\n",
    "results_c.append( repeated_exec(RUNS, f\"Q-Learning\", run_qlearning, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=AUTO_LOAD) )\n",
    "clear_output()\n",
    "results_c.append( repeated_exec(RUNS, f\"Sarsa\", run_sarsa, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=AUTO_LOAD) )\n",
    "clear_output()\n",
    "results_c.append( repeated_exec(RUNS, f\"Exp-Sarsa\", run_expected_sarsa, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=AUTO_LOAD) )\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmLJARc6x8LF"
   },
   "outputs": [],
   "source": [
    "plot_multiple_results(results_c, cumulative=False, x_log_scale=False, window=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDBq0lcvx8LG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "cap05-main.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "rl23y",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
