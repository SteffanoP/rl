{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Equações de Bellman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para V(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição\n",
    "\n",
    "$V(s) = E[G_t]$, considerando todos os retornos em que $S_t=s$ para $t$ qualquer.\n",
    "\n",
    "Mas, a seguir, veremos equações derivadas da definição, e chamadas de *equações de Bellman*.\n",
    "\n",
    "Esta primeira é apenas para destacar que, basicamente, a equação segue a ideia de cálculo da esperança $E[.]$:\n",
    "\n",
    "$V(s) = \\displaystyle\\sum_{a,s',r} p_{a,s',r} \\times (r + \\gamma V(s'))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas, se formos distinguir as contribuições de uma política estocástica e um ambiente estocástico, esta é a forma completa da equação:\n",
    "\n",
    "$V(s) = \\displaystyle\\sum_{a} \\pi(a|s) \\times \\displaystyle\\sum_{s', r} p(s',r|s,a) \\times (r + \\gamma V(s'))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas se o ambiente (MDP) e a política forem determinísticos, a equação fica bem mais simples, sem probabilidades:\n",
    "\n",
    "$V(s) = r + \\gamma V(s')$\n",
    "\n",
    "Onde:\n",
    "- $a$ é a ação escolhida pela política no estado $s$, ou seja, $\\pi(s)=a$\n",
    "- $r$ = recompensa recebida após fazer ação $a$ no estado $s$\n",
    "- $s'$ = próximo estado, após fazer a ação $a$ no estado $s$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para Q(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição\n",
    "\n",
    "$Q(s,a) = E[G_t]$, considerando todos os retornos em que $S_t=s$ e e $A_t=a$, para $t$ qualquer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a ação $a$ é definida diretamente no parâmetro, a *equação de Bellman* para $Q$ fica um pouco mais simples:\n",
    "\n",
    "$Q(s,a) = \\displaystyle\\sum_{s', r} p(s',r|s,a) \\times (r + \\gamma V(s'))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se a política for determinística, fica ainda mais simples:\n",
    "\n",
    "$Q(s,a) = r + \\gamma V(s')$\n",
    "\n",
    "Onde:\n",
    "- $r$ = recompensa recebida após fazer ação $a$ no estado $s$\n",
    "- $s'$ = próximo estado, após fazer a ação $a$ no estado $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TD-Learning (para o Q)\n",
    "\n",
    "Consider que $Q$ é um tipo de tabela (ou matriz) que guarda uma estimativa do valor teórico do $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os algoritmos que vimos, a cada passo, fazem uma nova estimativa do valor do estado atual, usando:\n",
    "- a recompensa recebida\n",
    "- algum valor do próximo estado calculado usando $Q$\n",
    "\n",
    "$Q_{target} = r + \\gamma V(s')$\n",
    "\n",
    "\n",
    "Onde:\n",
    "\n",
    "- *Q-Learning*: assume a política gulosa\n",
    "\n",
    "$V(s') = \\displaystyle \\max_{a'} Q(s',a')$\n",
    "\n",
    "- *SARSA*: usa as probabilidades da própria política\n",
    "\n",
    "$V(s') = \\displaystyle\\sum_{a'} \\pi(a'|s') Q(s',a')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atualiza a tabela $Q$ fazendo um tipo de média, onde o parâmetro $\\alpha$ (taxa de aprendizagem) indica o peso do novo valor:\n",
    "\n",
    "$Q(s,a) \\leftarrow (1-\\alpha) Q(s,a) + \\alpha Q_{target}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desenvolvendo e rearranjando, ficamos com esta equação:\n",
    "\n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha (Q_{target} - Q(s,a))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rlx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27dbc9ce4cc602e4f15257b7b0018d8dff5b9ce9a7d73bc4399cb5afb1e02c4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
