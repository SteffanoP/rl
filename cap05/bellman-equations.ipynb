{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Equações de Bellman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para V(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição\n",
    "\n",
    "$V(s) = E[G_t]$, considerando todos os retornos em que $S_t=s$ para $t$ qualquer.\n",
    "\n",
    "Mas, a seguir, veremos equações derivadas da definição, e chamadas de *Equações de Bellman*.\n",
    "\n",
    "Basicamente, as equações de Bellamn têm a forma de um cálculo da esperança $E[.]$, assim:\n",
    "\n",
    "$V(s) = \\displaystyle\\sum_{a,s',r} p_{a,s',r} \\times (r + \\gamma V(s'))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas, incluindo as contribuições de uma política estocástica e um ambiente estocástico, esta é a forma completa da equação:\n",
    "\n",
    "$V(s) = \\displaystyle\\sum_{a} \\pi(a|s) \\times \\displaystyle\\sum_{s', r} p(s',r|s,a) \\times (r + \\gamma V(s'))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas se o ambiente (MDP) e a política forem determinísticos, a equação fica bem mais simples, sem probabilidades:\n",
    "\n",
    "$V(s) = r + \\gamma V(s')$\n",
    "\n",
    "Onde:\n",
    "- $a$ é a ação escolhida pela política no estado $s$, ou seja, $\\pi(s)=a$\n",
    "- $r$ = recompensa recebida após fazer ação $a$ no estado $s$\n",
    "- $s'$ = próximo estado, após fazer a ação $a$ no estado $s$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para Q(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição\n",
    "\n",
    "$Q(s,a) = E[G_t]$, considerando todos os retornos em que $S_t=s$ e e $A_t=a$, para $t$ qualquer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a ação $a$ é definida diretamente no parâmetro, a *equação de Bellman* para $Q$ fica um pouco mais simples:\n",
    "\n",
    "$Q(s,a) = \\displaystyle\\sum_{s', r} p(s',r|s,a) \\times (r + \\gamma V(s'))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se a política for determinística, fica ainda mais simples:\n",
    "\n",
    "$Q(s,a) = r + \\gamma V(s')$\n",
    "\n",
    "Onde:\n",
    "- $r$ = recompensa recebida após fazer ação $a$ no estado $s$\n",
    "- $s'$ = próximo estado, após fazer a ação $a$ no estado $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TD-Learning de 1 passo (para o Q)\n",
    "\n",
    "Considere que $Q$ é um tipo de tabela (ou matriz) que guarda uma estimativa do valor teórico do $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os algoritmos que vimos, a cada passo, fazem uma nova estimativa do valor do estado atual, usando:\n",
    "- a recompensa recebida\n",
    "- algum valor do próximo estado calculado usando $Q$\n",
    "\n",
    "$Q_{target} = r + \\gamma V(s')$\n",
    "\n",
    "\n",
    "Onde:\n",
    "\n",
    "- *Q-Learning*: assume a política gulosa\n",
    "\n",
    "$V(s') = \\displaystyle \\max_{a'} Q(s',a')$\n",
    "\n",
    "- *Expected-SARSA*: usa as probabilidades da própria política\n",
    "\n",
    "$V(s') = \\displaystyle\\sum_{a'} \\pi(a'|s') Q(s',a')$\n",
    "\n",
    "- *SARSA*: usa exatamente a próxima ação $a'$ escolhida\n",
    "\n",
    "$V(s') = Q(s',a')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atualiza a tabela $Q$ fazendo um tipo de média, onde o parâmetro $\\alpha$ (taxa de aprendizagem) indica o peso do novo valor:\n",
    "\n",
    "$Q(s,a) \\leftarrow (1-\\alpha) Q(s,a) + \\alpha Q_{target}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desenvolvendo, podemos rearrumar assim esta equação:\n",
    "\n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha (Q_{target} - Q(s,a))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. n-Step TD-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os métodos Monte-Carlo, e TD-Learning de 1 passo (Q-Learning e Expected-SARSA) visam atualizar o $Q(s,a)$, formando uma nova estimativa $Q_{target}$ e usando-a para atualizar $Q(s,a)$.\n",
    "\n",
    "Porém, observe que esses métodos calculam $Q_{target}$ de formas distintas:\n",
    "- No Monte-Carlo, é o retorno real $G_i$ calculado até o fim de um episódio\n",
    "- No TD-Learning, é uma estimativa calculada com 1 recompensa assim: $(r + V(s'))$\n",
    "\n",
    "Observe que são casos extremos! Será que podemos fazer algo intermediário?\n",
    "\n",
    "Sim! Nos *métodos de de n passos* (n-step methods), nós vamos generalizar os métodos TD-Learning que vimos (Q-Learning e Expected-SARSA). Usaremos $n$ recompensas no cálculo do $Q_{target}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para n=2 passos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo, com *n=2*, para estimar $Q(s,a)$:\n",
    "- usamos essa experiência do episódio: $s, a, r_1, s_1, a_1, r_2, s_2, a_2$\n",
    "- calculamos assim: $Q_{target} = r_1 + \\gamma .r_2 + \\gamma^2 .V(s_2)$\n",
    "\n",
    "A forma de calcular $V(s_2)$ vai variar conforme o algoritmo, de maneira similar aos algoritmos de TD-Learning de 1 passo.\n",
    "\n",
    "A vantagem de usar mais passos é que o valor do $Q_{target}$ usa mais dados reais, e se aproxima do valor do retorno $G_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para n=3 passos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com n=3, faríamos assim:\n",
    "- usamos essa experiência do episódio: $s, a, r_1, s_1, a_1, r_2, s_2, a_2, r_3, s_3, a_3$\n",
    "- $Q_{target} = r_1 + \\gamma .r_2 + \\gamma^2 .r_3 + \\gamma^3 .V(s_3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para n qualquer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E assim sucessivamente, para valores maiores de $n$... O valor de $n$ vai ser um parâmetro do algoritmo.\n",
    "\n",
    "E qual a vantagem?\n",
    "\n",
    "A vantagem é podermos fazer estimativas melhores do $Q(s,a)$ usando mais experiência real. Em outras palavras, o valor do $Q_{target}$ usa mais recompensas e, assim, se aproxima do valor do retorno real $G_i$.\n",
    "\n",
    "No limite (com $n=\\infty$), o $Q_{target}$ se torna o próximo retorno $G_i$ e o método n-step torna-se o método Monte Carlo que vimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rlx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27dbc9ce4cc602e4f15257b7b0018d8dff5b9ce9a7d73bc4399cb5afb1e02c4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
