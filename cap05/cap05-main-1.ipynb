{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SeJ3wCaKe2Wl"
      },
      "source": [
        "# Capítulo 5 - Parte 1: Q-Learning, SARSA e Expected-SARSA\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TY2aAJM1x8K0"
      },
      "source": [
        "Você pode rodar este notebook localmente ou no Colab. Para abrir diretamente no Colab, basta clicar no link abaixo.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap05/cap05-main-1.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uAHITU7VhsM7"
      },
      "source": [
        "## 1. Configurações Iniciais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS23BU8R1vq-"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # for saving videos\n",
        "    !apt-get install ffmpeg freeglut3-dev xvfb\n",
        "\n",
        "    !pip install gym==0.23.1\n",
        "\n",
        "    # clone repository\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "\n",
        "    clear_output()\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4XHWUeyx8K3"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    # Set up fake display; otherwise rendering will fail\n",
        "    import os\n",
        "    os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "    os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "    from util.notebook import display_videos_from_path"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "euyQqkX2x8K4"
      },
      "source": [
        "### Imports Gerais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya6gLbpmx8K4"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Gzf7VhkiHxQ",
        "outputId": "8793add9-7219-41a4-ea02-23f26c32fed3"
      },
      "outputs": [],
      "source": [
        "from util.experiments import test_greedy_Q_policy, repeated_exec\n",
        "from util.plot import plot_result, plot_multiple_results\n",
        "from util.notebook import display_videos_from_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj6MsnLTx8K6"
      },
      "outputs": [],
      "source": [
        "# define se os algoritmos vão imprimir dados do treinamento\n",
        "VERBOSE = True"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wlVmoMADx8K7"
      },
      "source": [
        "## 1. Q-Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Veremos aqui, vários algoritmos da família *Temporal-Difference Learning* de 1 passo. Todos eles se baseiam na em uma estimativa da função $Q(s,a)$. Porém, diferente dos Monte Carlo, eles apresendem a cada passo de interação com o episódio.\n",
        "\n",
        "Ao invés do retorno parcial real $G_t$, todos usam uma estimativa *bootstraped* baseada nas equações de Bellamn. \n",
        "\n",
        "Após uma experiência $(s,a,r,s')$, eles atualizam $Q$ assim:\n",
        "\n",
        "$$\n",
        "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\times (r + \\gamma V(s') - Q(s,a)) \n",
        "$$\n",
        "\n",
        "O método **Q-Learning** tem como característica a suposição de que, a partir de s', será executada a ação \"gulosa\". Assim, ele assume que, para $s'$ não-terminal:\n",
        "\n",
        "$$\n",
        "V(s') = max_{a'}(Q(s',a'))\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0I0NlS6Jx8K7"
      },
      "outputs": [],
      "source": [
        "# Esta é a política. Neste caso, escolhe uma ação com base nos valores\n",
        "# da tabela Q, usando uma estratégia epsilon-greedy.\n",
        "def epsilon_greedy(Q, state, num_actions, epsilon):\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(0, num_actions)\n",
        "    else:\n",
        "        return np.argmax(Q[state])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_mBlDYQx8K8"
      },
      "outputs": [],
      "source": [
        "# Algoritmo Q-learning, online learning (TD-learning)\n",
        "# Atenção: os espaços de estados e de ações precisam ser discretos, dados por valores inteiros\n",
        "def run_qlearning(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1, render=False):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # inicializa a tabela Q com valores aleatórios de -1.0 a 0.0\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.random.uniform(low = -1.0, high = 0.0,\n",
        "                          size = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "\n",
        "        state = env.reset()\n",
        "\n",
        "        # executa um episódio completo, fazendo atualizações na Q-table\n",
        "        while not done:\n",
        "            # exibe/renderiza os passos no ambiente, durante 1 episódio a cada mil e também nos últimos 5 episódios\n",
        "            if render and (i >= (episodes - 5) or (i+1) % 1000 == 0):\n",
        "                env.render()\n",
        "\n",
        "            # escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = epsilon_greedy(Q, state, num_actions, epsilon)\n",
        "\n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                # para estados terminais\n",
        "                V_next_state = 0\n",
        "            else:\n",
        "                # para estados não-terminais -- valor máximo (melhor ação)\n",
        "                V_next_state = np.max(Q[next_state])\n",
        "\n",
        "            # atualiza a Q-table\n",
        "            # delta = (estimativa usando a nova recompensa) - estimativa antiga\n",
        "            delta = (reward + gamma * V_next_state) - Q[state,action]\n",
        "            Q[state,action] = Q[state,action] + lr * delta\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso\n",
        "        if VERBOSE and ((i+1) % 100 == 0):\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Pgs0um7x8K8",
        "outputId": "2e94226e-0d0f-4106-a2f7-386be3cc2563"
      },
      "outputs": [],
      "source": [
        "#ENV_NAME, r_max = \"Taxi-v3\", 10\n",
        "ENV_NAME, r_max = \"CliffWalking-v0\", 0\n",
        "\n",
        "EPISODES = 5000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.1\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "# Roda o algoritmo Q-Learning\n",
        "rewards1, qtable1 = run_qlearning(env, EPISODES, LR, GAMMA, EPSILON, render=False)\n",
        "clear_output()\n",
        "\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards1[-20:]), \", desvio padrao =\", np.std(rewards1[-20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "r9eZBjutx8K8",
        "outputId": "20c11044-1ffe-4bda-b867-c4a15045bd48"
      },
      "outputs": [],
      "source": [
        "# Mostra um gráfico de episódios x retornos não descontados\n",
        "plot_result(rewards1, r_max, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVfR9GGBx8K9"
      },
      "outputs": [],
      "source": [
        "test_greedy_Q_policy(env, qtable1, 1, render=True)\n",
        "#test_greedy_Q_policy(env, qtable1, 1, render=True, recorded_video_folder='./videos-qlearning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#display_videos_from_path('./videos-qlearning')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ztvJdbKVh20Y"
      },
      "source": [
        "## 2. SARSA"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3ULLPHx7x8K9"
      },
      "source": [
        "O **SARSA** é um algoritmo semelhante ao Q-Learning. Porém, ele usa um valor de $V(s')$ baseado na próxima ação ($a'$) realmente escolhida.\n",
        "\n",
        "Em outras palavras, o Q-Learning:\n",
        "- usa os dados $(s,a,r,s')$ para fazer uma atualização no $Q(s,a)$\n",
        "- assume que o valor do próximo estado é (guloso) baseado na melhor ação possível: $V(s') = max_{a'}{Q(s', a')}$\n",
        "\n",
        "Porém, o SARSA:\n",
        "- usa os dados $(s,a,r,s',a')$ para fazer uma atualização no $Q(s,a)$\n",
        "- assume que o valor do próximo estado é o resultado da verdadeira próxima ação $a'$: $V(s') = Q(s', a')$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O_ALlauax8K9"
      },
      "source": [
        "Crie o código do SARSA abaixo, com base no código do Q-Learning. Dicas:\n",
        "- para atualizar o $Q$, você precisa desta 4 variáveis coerentemente mantidas: `state, action, next_state, next_action`\n",
        "- escolha a \"próxima ação\" imediatamente após cada passo (e logo após o reset) e antes da atualização do $Q$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-LuUb_Kx8K9"
      },
      "outputs": [],
      "source": [
        "# Algoritmo SARSA, online learning (TD-learning)\n",
        "# Atenção: os espaços de estados e de ações precisam ser discretos, dados por valores inteiros\n",
        "def run_sarsa(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1, render=False):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # inicializa a tabela Q com valores aleatórios de -1.0 a 0.0\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.random.uniform(low = -1.0, high = 0.0,\n",
        "                          size = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "\n",
        "        # IMPLEMENTE A PARTIR DAQUI (removendo a exceção)\n",
        "\n",
        "        # remova\n",
        "        raise Exception(\"Not implemented\")\n",
        "\n",
        "        # ATÉ AQUI....\n",
        "\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso\n",
        "        if VERBOSE and ((i+1) % 100 == 0):\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j3K9Nqgwx8K-"
      },
      "source": [
        "Se não conseguir, descomente o código abaixo para prosseguir:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9jzhZnUx8K-"
      },
      "outputs": [],
      "source": [
        "#from cap05.nstep_sarsa import run_nstep_sarsa\n",
        "#run_sarsa = lambda env, episodes, lr, gamma, eps, render=False : run_nstep_sarsa(env, episodes, 1, lr, gamma, eps, render=render)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95q2OmlVx8K-",
        "outputId": "cc2c6bbf-8e84-4df9-f70f-ec7a2922794d"
      },
      "outputs": [],
      "source": [
        "#ENV_NAME, r_max = \"Taxi-v3\", 10\n",
        "ENV_NAME, r_max = \"CliffWalking-v0\", 0\n",
        "\n",
        "EPISODES = 5000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.1\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "# Roda o algoritmo SARSA\n",
        "rewards2, qtable2 = run_sarsa(env, EPISODES, LR, GAMMA, EPSILON, render=False)\n",
        "clear_output()\n",
        "\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards2[-20:]), \", desvio padrao =\", np.std(rewards2[-20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "MemHbKN6x8K-",
        "outputId": "4eb76019-653e-4661-8082-8b098f3382fd"
      },
      "outputs": [],
      "source": [
        "# Mostra um gráfico de episódios x retornos não descontados\n",
        "plot_result(rewards2, r_max, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlHqnyDdx8K-"
      },
      "outputs": [],
      "source": [
        "test_greedy_Q_policy(env, qtable2, 1, render=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Cncfa3ghx8K-"
      },
      "source": [
        "## 3. Expected-SARSA"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X-bhDTXfx8K_"
      },
      "source": [
        "O Expected-SARSA tem semelhanças com os dois algoritmos vistos antes:\n",
        "- usa os dados $(s,a,r,s')$ para fazer uma atualização no $Q(s,a)$\n",
        "- assume que o valor do próximo estado é o valor médio, com base nas probabilidades dadas pela política:\n",
        "   $$V(s') = \\displaystyle\\sum_{a'} \\pi(a'|s') \\times Q(s',a')$$\n",
        "\n",
        "Para implementá-lo, precisamos das probabilidades da política realizar cada ação.\n",
        "\n",
        "A função abaixo retorna as probabilidades de cada ação na política *epsilon-greedy*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiXy1k97x8K_"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_probs(Q, state, num_actions, epsilon):\n",
        "    # lista com a probabilidade que todas as ações têm de ser escolhidas nas decisões exploratórias (não-gulosas)\n",
        "    probs = [ (epsilon / num_actions) ] * num_actions\n",
        "\n",
        "    # adiciona a probabilidade para a ação 'greedy'\n",
        "    # em caso de empate, só o menor índice é considerado, mas isso é coerente com a implementação de epsilon_greedy()\n",
        "    greedy_action = np.argmax(Q[state])\n",
        "    probs[greedy_action] += (1.0 - epsilon)\n",
        "\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vez0f2dJx8K_"
      },
      "outputs": [],
      "source": [
        "# Algoritmo Expected-SARSA\n",
        "def run_expected_sarsa(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1, render=False):\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # inicializa a tabela Q com valores aleatórios de -1.0 a 0.0\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.random.uniform(low = -1.0, high = 0.0,\n",
        "                          size = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "\n",
        "        state = env.reset()\n",
        "\n",
        "        # executa 1 episódio completo, fazendo atualizações na Q-table\n",
        "        while not done:\n",
        "            # exibe/renderiza os passos no ambiente, durante 1 episódio a cada mil e também nos últimos 5 episódios\n",
        "            if render and (i >= (episodes - 5) or (i+1) % 1000 == 0):\n",
        "                env.render()\n",
        "\n",
        "            # escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = epsilon_greedy(Q, state, num_actions, epsilon)\n",
        "\n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                # para estados terminais\n",
        "                V_next_state = 0\n",
        "            else:\n",
        "                # para estados não-terminais -- valor esperado\n",
        "                p_next_actions = epsilon_greedy_probs(Q, next_state, num_actions, epsilon)\n",
        "                V_next_state = np.sum( np.asarray(p_next_actions) * Q[next_state] )\n",
        "\n",
        "            # atualiza a Q-table\n",
        "            # delta = (estimativa usando a nova recompensa) - estimativa antiga\n",
        "            Qnew = (reward + gamma * V_next_state) \n",
        "            Q[state,action] = Q[state,action] + lr * (Qnew - Q[state,action])\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        # salva o retorno do episódio que encerrou\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso\n",
        "        if VERBOSE and ((i+1) % 100 == 0):\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    return sum_rewards_per_ep, Q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "id": "nQoNjZ1Rx8K_",
        "outputId": "2bd240a5-c2c2-4d8e-feb0-299e961e9301"
      },
      "outputs": [],
      "source": [
        "#ENV_NAME, r_max = \"Taxi-v3\", 10\n",
        "ENV_NAME, r_max = \"CliffWalking-v0\", 0\n",
        "\n",
        "EPISODES = 7000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.1\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "rewards3, qtable3 = run_expected_sarsa(env, EPISODES, LR, GAMMA, EPSILON, render=False)\n",
        "clear_output()\n",
        "\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards3[-20:]), \", desvio padrao =\", np.std(rewards3[-20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostra um gráfico de episódios x retornos não descontados\n",
        "plot_result(rewards3, r_max, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_greedy_Q_policy(env, qtable3, 1, render=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yF9fggfZzFVV"
      },
      "source": [
        "## 4. Experimentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "352WmZAhx8K_"
      },
      "outputs": [],
      "source": [
        "# parâmetros para todos os algoritmos (não é o ideal)\n",
        "LR = 0.1\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.05\n",
        "\n",
        "# silencia os algoritmos\n",
        "VERBOSE = False\n",
        "\n",
        "# repetições de cada algoritmo\n",
        "RUNS = 5\n",
        "\n",
        "# para carregar automaticamente os resultados de simulações já executadas\n",
        "auto_load = True"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fmQyZXdex8LA"
      },
      "source": [
        "### Taxi-v3 (discreto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYSG9xiHxBKe"
      },
      "outputs": [],
      "source": [
        "environment = gym.make(\"Taxi-v3\")\n",
        "NUM_EPISODES = 5_000\n",
        "\n",
        "results_t = []\n",
        "\n",
        "results_t.append( repeated_exec(RUNS, f\"Q-Learning\", run_qlearning, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=auto_load) )\n",
        "clear_output()\n",
        "results_t.append( repeated_exec(RUNS, f\"Sarsa\", run_sarsa, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=auto_load) )\n",
        "clear_output()\n",
        "results_t.append( repeated_exec(RUNS, f\"Exp-Sarsa\", run_expected_sarsa, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=auto_load) )\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "LmAc-tJKx8LA",
        "outputId": "eacbb2ce-e86b-4a4a-a8b3-75d36479ab94"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results_t, cumulative=False, x_log_scale=False, window=50)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "L-I2_nawx8LA"
      },
      "source": [
        "### FrozenLake-v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVZ3eXZyx8LA"
      },
      "outputs": [],
      "source": [
        "environment = gym.make(\"FrozenLake-v1\")\n",
        "NUM_EPISODES = 8_000\n",
        "\n",
        "results_f = []\n",
        "\n",
        "results_f.append( repeated_exec(RUNS, f\"Q-Learning\", run_qlearning, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=auto_load) )\n",
        "clear_output()\n",
        "results_f.append( repeated_exec(RUNS, f\"Sarsa\", run_sarsa, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=auto_load) )\n",
        "clear_output()\n",
        "results_f.append( repeated_exec(RUNS, f\"Exp-Sarsa\", run_expected_sarsa, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=auto_load) )\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "97AkZNAqx8LF",
        "outputId": "11f20e4e-ad1e-4f00-dae5-1c9cb360fbe2"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results_f, cumulative=False, x_log_scale=False, window=100)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MhyQN_Wsx8LF"
      },
      "source": [
        "### Cliff Walking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAVZPiSVx8LF"
      },
      "outputs": [],
      "source": [
        "environment = gym.make(\"CliffWalking-v0\")\n",
        "NUM_EPISODES = 3_000\n",
        "\n",
        "results_c = []\n",
        "\n",
        "results_c.append( repeated_exec(RUNS, f\"Q-Learning\", run_qlearning, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=auto_load) )\n",
        "clear_output()\n",
        "results_c.append( repeated_exec(RUNS, f\"Sarsa\", run_sarsa, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=auto_load) )\n",
        "clear_output()\n",
        "results_c.append( repeated_exec(RUNS, f\"Exp-Sarsa\", run_expected_sarsa, environment, NUM_EPISODES, LR, GAMMA, EPSILON, auto_load=auto_load) )\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "dmLJARc6x8LF",
        "outputId": "f1938293-288a-4471-acc3-ad75cbf8ddc2"
      },
      "outputs": [],
      "source": [
        "plot_multiple_results(results_c, cumulative=False, x_log_scale=False, window=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDBq0lcvx8LG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "cap05-main-1.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('rlx')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "27dbc9ce4cc602e4f15257b7b0018d8dff5b9ce9a7d73bc4399cb5afb1e02c4a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
